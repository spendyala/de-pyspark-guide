{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDFs)\n",
    "\n",
    "User-Defined Functions (UDFs) allow you to define custom column-based functions in Python (or other languages) and apply them to your Spark DataFrames. While powerful, try to use built-in Spark SQL functions when possible, as they are generally more optimized than UDFs.\n",
    "\n",
    "Let's create a simple DataFrame to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 06:43:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Make sure SparkSession is initialized (it usually is in Jupyter)\n",
    "# If not, uncomment the next line\n",
    "spark = SparkSession.builder.appName(\"UDF Examples\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple String Manipulation UDF\n",
    "\n",
    "Let's define a UDF that converts the 'name' column to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Uppercase Name:\n",
      "+-------+---+----------+\n",
      "|   name|age|name_upper|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|     ALICE|\n",
      "|    Bob| 30|       BOB|\n",
      "|Charlie| 35|   CHARLIE|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Python function\n",
    "def to_upper_case(s):\n",
    "  if s is not None:\n",
    "    return s.upper()\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# 2. Register the Python function as a UDF\n",
    "#    Specify the return type of the UDF\n",
    "upper_case_udf = udf(to_upper_case, StringType())\n",
    "\n",
    "# 3. Apply the UDF to the DataFrame column\n",
    "df_upper = df.withColumn(\"name_upper\", upper_case_udf(col(\"name\")))\n",
    "\n",
    "print(\"DataFrame with Uppercase Name:\")\n",
    "df_upper.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: UDF with Multiple Columns and Different Return Type\n",
    "\n",
    "Now, let's create a UDF that takes the 'name' and 'age' columns and returns a descriptive string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Description:\n",
      "+-------+---+--------------------+\n",
      "|   name|age|         description|\n",
      "+-------+---+--------------------+\n",
      "|  Alice| 25|Alice is 25 years...|\n",
      "|    Bob| 30|Bob is 30 years old.|\n",
      "|Charlie| 35|Charlie is 35 yea...|\n",
      "+-------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Python function\n",
    "def describe_person(name, age):\n",
    "    return f\"{name} is {age} years old.\"\n",
    "\n",
    "# 2. Register the UDF\n",
    "describe_person_udf = udf(describe_person, StringType())\n",
    "\n",
    "# 3. Apply the UDF\n",
    "df_described = df.withColumn(\"description\", describe_person_udf(col(\"name\"), col(\"age\")))\n",
    "\n",
    "print(\"DataFrame with Description:\")\n",
    "df_described.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Using UDFs with `spark.sql` (SQL Expressions)\n",
    "\n",
    "You can also register UDFs for use directly within Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from SQL query using UDF:\n",
      "+----------+---+\n",
      "|upper_name|age|\n",
      "+----------+---+\n",
      "|       BOB| 30|\n",
      "|   CHARLIE| 35|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the first UDF for SQL use\n",
    "spark.udf.register(\"SQL_UPPER\", to_upper_case, StringType())\n",
    "\n",
    "# Create a temporary view to run SQL queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Use the registered UDF in a SQL query\n",
    "sql_result = spark.sql(\"SELECT SQL_UPPER(name) as upper_name, age FROM people WHERE age > 28\")\n",
    "\n",
    "print(\"Result from SQL query using UDF:\")\n",
    "sql_result.show()\n",
    "\n",
    "# Remember to stop the SparkSession if you're done (optional in interactive sessions)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Using a UDF with Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Category Column (UDF approach):\n",
      "+-------+---+------------+\n",
      "|   name|age|age_category|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25| Young Adult|\n",
      "|    Bob| 30|       Adult|\n",
      "|Charlie| 35|     Mid-30s|\n",
      "+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary\n",
    "age_category = {\n",
    "    25: \"Young Adult\",\n",
    "    30: \"Adult\",\n",
    "    35: \"Mid-30s\"\n",
    "}\n",
    "\n",
    "# Define UDF that uses the dictionary\n",
    "def get_age_category(age):\n",
    "    return age_category.get(age, \"Unknown\")\n",
    "\n",
    "# Register UDF\n",
    "age_category_udf = udf(get_age_category, StringType())\n",
    "\n",
    "# Apply UDF to create new column\n",
    "df_with_category = df.withColumn(\"age_category\", age_category_udf(col(\"age\")))\n",
    "\n",
    "# Show result\n",
    "print(\"DataFrame with Category Column (UDF approach):\")\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Map expression (more efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Category Column (map approach):\n",
      "+-------+---+------------+\n",
      "|   name|age|age_category|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25| Young Adult|\n",
      "|    Bob| 30|       Adult|\n",
      "|Charlie| 35|     Mid-30s|\n",
      "+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "# Create a map expression from the dictionary items\n",
    "# This converts the Python dictionary to a Spark map\n",
    "mapping_expr = create_map([lit(x) for x in chain(*[(k, v) for k, v in age_category.items()])])\n",
    "\n",
    "# Apply the mapping expression to create new column\n",
    "df_with_map = df.withColumn(\"age_category\", mapping_expr[df.age])\n",
    "\n",
    "# Show result\n",
    "print(\"DataFrame with Category Column (map approach):\")\n",
    "df_with_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: Creating a mapping DataFrame and joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Category Column (join approach):\n",
      "+---+-------+-----------+\n",
      "|age|   name|   category|\n",
      "+---+-------+-----------+\n",
      "| 25|  Alice|Young Adult|\n",
      "| 30|    Bob|      Adult|\n",
      "| 35|Charlie|    Mid-30s|\n",
      "+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary to DataFrame\n",
    "mapping_data = [(k, v) for k, v in age_category.items()]\n",
    "mapping_df = spark.createDataFrame(mapping_data, [\"age\", \"category\"])\n",
    "\n",
    "# Join with original DataFrame\n",
    "df_with_join = df.join(mapping_df, \"age\", \"left\")\n",
    "\n",
    "# Show result\n",
    "print(\"DataFrame with Category Column (join approach):\")\n",
    "df_with_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDF approach is the simplest but tends to be less performant for large datasets. The map expression is generally more efficient as it keeps processing inside Spark's execution engine. The join approach works well when you have a large mapping table that might be used in multiple places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
