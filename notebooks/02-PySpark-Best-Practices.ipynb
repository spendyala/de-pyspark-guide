{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Best Practices\n",
    "\n",
    "This notebook covers key best practices and optimization techniques for working with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SparkSession Configuration\n",
    "\n",
    "Properly configuring your SparkSession is the first step toward optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 06:48:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.memory', '2g'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.startTime', '1744958925326'), ('spark.app.id', 'local-1744958925709'), ('spark.default.parallelism', '8'), ('spark.sql.adaptive.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.driver.port', '38745'), ('spark.app.name', 'PySpark Best Practices'), ('spark.app.submitTime', '1744958925242'), ('spark.sql.adaptive.coalescePartitions.enabled', 'true'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.memory', '2g'), ('spark.driver.host', '3ad2e217349e'), ('spark.sql.shuffle.partitions', '200'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.serializer.objectStreamReset', '100'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a well-configured SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"PySpark Best Practices\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 200)\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.default.parallelism\", 8)\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Get the current configuration\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "\n",
    "- **spark.sql.shuffle.partitions**: Controls the number of partitions during shuffles (default: 200)\n",
    "- **spark.executor.memory**: Memory per executor\n",
    "- **spark.driver.memory**: Memory for driver process\n",
    "- **spark.default.parallelism**: Default number of partitions for RDDs\n",
    "- **spark.sql.adaptive.enabled**: Enables adaptive query execution\n",
    "\n",
    "ðŸ’¡ **Best Practice**: Tune these parameters based on your cluster size and workload characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+-------+---+-------+\n",
      "| id|   name|age| salary|\n",
      "+---+-------+---+-------+\n",
      "|  1|  Alice| 30|50000.0|\n",
      "|  2|    Bob| 32|60000.0|\n",
      "|  3|Charlie| 28|55000.0|\n",
      "+---+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 30, 50000.0),\n",
    "    (2, \"Bob\", 32, 60000.0),\n",
    "    (3, \"Charlie\", 28, 55000.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Data Loading\n",
    "\n",
    "1. **Always Define Schema Explicitly**\n",
    "   - Avoids costly schema inference\n",
    "   - Ensures correct data types\n",
    "   - Improves performance on large datasets\n",
    "\n",
    "2. **Use Appropriate File Formats**\n",
    "   - Parquet is usually the best choice (columnar, compressed, schema-preserved)\n",
    "   - ORC is good for Hive compatibility\n",
    "   - Avoid text/CSV for large datasets when possible\n",
    "\n",
    "3. **Partition Data Appropriately**\n",
    "   - Choose partition columns that distribute data evenly\n",
    "   - Avoid too many small partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#29,name#30,salary#31,age#32] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/example-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string,salary:double>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Example of reading/writing with best practices\n",
    "\n",
    "# Write data using Parquet format with partitioning\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"age\") \\\n",
    "    .parquet(\"/tmp/example-data\")\n",
    "\n",
    "# Read data with explicit schema\n",
    "df_read = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .parquet(\"/tmp/example-data\")\n",
    "\n",
    "df_read.explain()  # Show execution plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame Operations - Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+-----------------+---------------+\n",
      "| id|   name|age| salary|  adjusted_salary|salary_category|\n",
      "+---+-------+---+-------+-----------------+---------------+\n",
      "|  1|  Alice| 30|50000.0|55000.00000000001|          Entry|\n",
      "|  2|    Bob| 32|60000.0|          66000.0|            Mid|\n",
      "|  3|Charlie| 28|55000.0|60500.00000000001|            Mid|\n",
      "+---+-------+---+-------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, expr, avg, sum, max, min, count\n",
    "\n",
    "# Column operations - Good practice\n",
    "df_transformed = df \\\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        col(\"name\"),\n",
    "        col(\"age\"),\n",
    "        col(\"salary\"),  # Keep the original salary column\n",
    "        (col(\"salary\") * 1.1).alias(\"adjusted_salary\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"salary_category\",\n",
    "        when(col(\"salary\") < 55000, \"Entry\")\n",
    "        .when(col(\"salary\") < 65000, \"Mid\")\n",
    "        .otherwise(\"Senior\")\n",
    "    ) \\\n",
    "    .filter(col(\"age\") > 25)\n",
    "\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Transformations\n",
    "\n",
    "1. **Chain Operations Efficiently**\n",
    "   - Chain multiple transformations before actions\n",
    "   - Use method chaining for readability\n",
    "\n",
    "2. **Use Column Expressions**\n",
    "   - Prefer `col()` and expressions over UDFs when possible\n",
    "   - Use SQL functions from `pyspark.sql.functions`\n",
    "\n",
    "3. **Limit Shuffling Operations**\n",
    "   - Operations like `groupBy`, `join`, `repartition` cause shuffling\n",
    "   - Try to minimize these operations\n",
    "\n",
    "4. **Filter Early**\n",
    "   - Apply filters as early as possible to reduce data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Techniques - Caching and Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 06:48:49 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, age: int, salary: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Cache data that will be reused\n",
    "df_cached = df.cache()  # or df.persist()\n",
    "df_cached.count()  # Materialize the cache\n",
    "\n",
    "# More control with specific storage level\n",
    "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_persisted.count()  # Materialize the persistence\n",
    "\n",
    "# Clean up when done\n",
    "df_cached.unpersist()\n",
    "df_persisted.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Best Practices\n",
    "\n",
    "1. **When to Cache**\n",
    "   - Cache DataFrames used multiple times\n",
    "   - Cache after expensive transformations\n",
    "   - Cache after filtering down large datasets\n",
    "\n",
    "2. **Storage Levels**\n",
    "   - `MEMORY_ONLY`: Default, fastest but can cause OOM errors\n",
    "   - `MEMORY_AND_DISK`: Safer option, spills to disk if needed\n",
    "   - `DISK_ONLY`: When memory is limited\n",
    "\n",
    "3. **Clean Up Cache**\n",
    "   - Call `unpersist()` when done to free up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joins and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [id#0], [id#313], Inner, BuildRight, false\n",
      "   :- Scan ExistingRDD[id#0,name#1,age#2,salary#3]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=166]\n",
      "      +- Scan ExistingRDD[id#313,department#314]\n",
      "\n",
      "\n",
      "+---+-------+---+-------+---+-----------+\n",
      "| id|   name|age| salary| id| department|\n",
      "+---+-------+---+-------+---+-----------+\n",
      "|  1|  Alice| 30|50000.0|  1|Engineering|\n",
      "|  2|    Bob| 32|60000.0|  2|         HR|\n",
      "|  3|Charlie| 28|55000.0|  3|  Marketing|\n",
      "+---+-------+---+-------+---+-----------+\n",
      "\n",
      "+---+-----+----------+----------+\n",
      "|age|count|avg_salary|max_salary|\n",
      "+---+-----+----------+----------+\n",
      "| 30|    1|   50000.0|   50000.0|\n",
      "| 32|    1|   60000.0|   60000.0|\n",
      "| 28|    1|   55000.0|   55000.0|\n",
      "+---+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a department DataFrame\n",
    "dept_data = [(1, \"Engineering\"), (2, \"HR\"), (3, \"Marketing\")]\n",
    "dept_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "dept_df = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "\n",
    "# Broadcast join (efficient for small + large table joins)\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast the smaller DataFrame\n",
    "joined_df = df.join(broadcast(dept_df), df.id == dept_df.id)\n",
    "joined_df.explain()\n",
    "joined_df.show()\n",
    "\n",
    "# Efficient aggregations\n",
    "agg_df = df.groupBy(\"age\").agg(\n",
    "    count(\"id\").alias(\"count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ")\n",
    "\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Joins and Aggregations\n",
    "\n",
    "1. **Join Strategies**\n",
    "   - Use broadcast joins when one DataFrame is small (<10MB)\n",
    "   - Join on columns with high cardinality\n",
    "   - Prefer using `join()` with explicit conditions over SQL-style joins\n",
    "\n",
    "2. **Join Types**\n",
    "   - Inner joins are fastest\n",
    "   - Left/right outer joins preserve one side\n",
    "   - Full outer joins are most expensive\n",
    "\n",
    "3. **Efficient Aggregations**\n",
    "   - Combine multiple aggregations in a single call\n",
    "   - Filter before grouping when possible\n",
    "   - Consider `approx_count_distinct()` for approximate counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partitioning and Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Number of partitions after coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Repartitioning to control parallelism\n",
    "df_repartitioned = df.repartition(8)\n",
    "print(f\"Number of partitions: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by specific column (good for joins)\n",
    "df_repart_by_col = df.repartition(\"age\")\n",
    "\n",
    "# Coalesce to reduce partitions (no shuffle)\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "print(f\"Number of partitions after coalesce: {df_coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Write with bucketing (good for repeated joins)\n",
    "df.write \\\n",
    "    .bucketBy(4, \"id\") \\\n",
    "    .sortBy(\"id\") \\\n",
    "    .saveAsTable(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Best Practices\n",
    "\n",
    "1. **Choosing Number of Partitions**\n",
    "   - Rule of thumb: 2-3 Ã— number of CPU cores\n",
    "   - Too few: underutilization, potential OOM\n",
    "   - Too many: task scheduling overhead\n",
    "\n",
    "2. **When to Repartition**\n",
    "   - Before wide operations (joins, groupBy)\n",
    "   - When partition sizes are skewed\n",
    "   - When number of partitions is too low/high\n",
    "\n",
    "3. **Coalesce vs. Repartition**\n",
    "   - Use `coalesce()` to reduce partitions (no shuffle)\n",
    "   - Use `repartition()` to increase partitions (full shuffle)\n",
    "\n",
    "4. **Bucketing**\n",
    "   - Good for repeated joins on same column\n",
    "   - Pre-organizes data to avoid shuffles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. UDFs and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With UDF:\n",
      "== Physical Plan ==\n",
      "*(2) Project [id#0, name#1, age#2, salary#3, pythonUDF0#413 AS age_plus_one#401]\n",
      "+- BatchEvalPython [slow_add_one(age#2)#400], [pythonUDF0#413]\n",
      "   +- *(1) Scan ExistingRDD[id#0,name#1,age#2,salary#3]\n",
      "\n",
      "\n",
      "\n",
      "With built-in function:\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#0, name#1, age#2, salary#3, (age#2 + 1) AS age_plus_one#407]\n",
      "+- *(1) Scan ExistingRDD[id#0,name#1,age#2,salary#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Avoid this approach for simple operations\n",
    "@udf(returnType=IntegerType())\n",
    "def slow_add_one(x):\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "    return None\n",
    "\n",
    "# Prefer built-in functions (much faster)\n",
    "df_slow = df.withColumn(\"age_plus_one\", slow_add_one(col(\"age\")))\n",
    "df_fast = df.withColumn(\"age_plus_one\", col(\"age\") + 1)\n",
    "\n",
    "# Compare execution plans\n",
    "print(\"With UDF:\")\n",
    "df_slow.explain()\n",
    "print(\"\\nWith built-in function:\")\n",
    "df_fast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Best Practices\n",
    "\n",
    "1. **Avoid UDFs When Possible**\n",
    "   - Use built-in functions from `pyspark.sql.functions`\n",
    "   - Use SQL expressions with `expr()`\n",
    "   - UDFs require serialization/deserialization overhead\n",
    "\n",
    "2. **When to Use UDFs**\n",
    "   - Complex logic not available in built-in functions\n",
    "   - Operations requiring external libraries\n",
    "\n",
    "3. **Pandas UDFs**\n",
    "   - Use Pandas UDFs (vectorized UDFs) for better performance\n",
    "   - Can be 10-100x faster than regular UDFs\n",
    "   - Requires Arrow serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (19.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+------------+\n",
      "| id|   name|age| salary|age_plus_one|\n",
      "+---+-------+---+-------+------------+\n",
      "|  1|  Alice| 30|50000.0|          31|\n",
      "|  2|    Bob| 32|60000.0|          33|\n",
      "|  3|Charlie| 28|55000.0|          29|\n",
      "+---+-------+---+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas UDF example (much faster than regular UDF)\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def pandas_add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "df_pandas_udf = df.withColumn(\"age_plus_one\", pandas_add_one(col(\"age\")))\n",
    "df_pandas_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+----+----------+----------+-----------+-----------+-------------+\n",
      "| id|   name|age| salary|rank|dense_rank|row_number|next_salary|prev_salary|running_total|\n",
      "+---+-------+---+-------+----+----------+----------+-----------+-----------+-------------+\n",
      "|  3|Charlie| 28|55000.0|   1|         1|         1|       NULL|       NULL|      55000.0|\n",
      "|  1|  Alice| 30|50000.0|   1|         1|         1|       NULL|       NULL|      50000.0|\n",
      "|  2|    Bob| 32|60000.0|   1|         1|         1|       NULL|       NULL|      60000.0|\n",
      "+---+-------+---+-------+----+----------+----------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lead, lag, sum\n",
    "\n",
    "# Create window spec\n",
    "window_spec = Window.partitionBy(\"age\").orderBy(\"salary\")\n",
    "\n",
    "# Apply window functions\n",
    "df_window = df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "              .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "              .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "              .withColumn(\"next_salary\", lead(\"salary\", 1).over(window_spec)) \\\n",
    "              .withColumn(\"prev_salary\", lag(\"salary\", 1).over(window_spec))\n",
    "\n",
    "# Window for running totals\n",
    "sum_window = Window.partitionBy(\"age\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_window = df_window.withColumn(\"running_total\", sum(\"salary\").over(sum_window))\n",
    "\n",
    "df_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function Best Practices\n",
    "\n",
    "1. **Reuse Window Specifications**\n",
    "   - Define window specs once and reuse\n",
    "   - Improves readability and performance\n",
    "\n",
    "2. **Window Function Types**\n",
    "   - Ranking functions: `rank()`, `dense_rank()`, `row_number()`\n",
    "   - Analytic functions: `lead()`, `lag()`\n",
    "   - Aggregate functions: `sum()`, `avg()`, `min()`, `max()`\n",
    "\n",
    "3. **Bounded vs. Unbounded Windows**\n",
    "   - Bounded windows (e.g., `rowsBetween(-2, 2)`) are faster\n",
    "   - Unbounded windows need more resources\n",
    "\n",
    "4. **Partitioning Considerations**\n",
    "   - Partition by columns with reasonable cardinality\n",
    "   - Too many partitions can degrade performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(avg_salary#573) AND (avg_salary#573 > 50000.0))\n",
      "   +- HashAggregate(keys=[department#314], functions=[avg(salary#3)])\n",
      "      +- Exchange hashpartitioning(department#314, 200), ENSURE_REQUIREMENTS, [plan_id=464]\n",
      "         +- HashAggregate(keys=[department#314], functions=[partial_avg(salary#3)])\n",
      "            +- Project [salary#3, department#314]\n",
      "               +- SortMergeJoin [id#0], [id#313], Inner\n",
      "                  :- Sort [id#0 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=456]\n",
      "                  :     +- Project [id#0, salary#3]\n",
      "                  :        +- Scan ExistingRDD[id#0,name#1,age#2,salary#3]\n",
      "                  +- Sort [id#313 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(id#313, 200), ENSURE_REQUIREMENTS, [plan_id=457]\n",
      "                        +- Scan ExistingRDD[id#313,department#314]\n",
      "\n",
      "\n",
      "\n",
      "Detailed Physical Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (14)\n",
      "+- Filter (13)\n",
      "   +- HashAggregate (12)\n",
      "      +- Exchange (11)\n",
      "         +- HashAggregate (10)\n",
      "            +- Project (9)\n",
      "               +- SortMergeJoin Inner (8)\n",
      "                  :- Sort (4)\n",
      "                  :  +- Exchange (3)\n",
      "                  :     +- Project (2)\n",
      "                  :        +- Scan ExistingRDD (1)\n",
      "                  +- Sort (7)\n",
      "                     +- Exchange (6)\n",
      "                        +- Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [4]: [id#0, name#1, age#2, salary#3]\n",
      "Arguments: [id#0, name#1, age#2, salary#3], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Project\n",
      "Output [2]: [id#0, salary#3]\n",
      "Input [4]: [id#0, name#1, age#2, salary#3]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [id#0, salary#3]\n",
      "Arguments: hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=456]\n",
      "\n",
      "(4) Sort\n",
      "Input [2]: [id#0, salary#3]\n",
      "Arguments: [id#0 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD\n",
      "Output [2]: [id#313, department#314]\n",
      "Arguments: [id#313, department#314], MapPartitionsRDD[34] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Exchange\n",
      "Input [2]: [id#313, department#314]\n",
      "Arguments: hashpartitioning(id#313, 200), ENSURE_REQUIREMENTS, [plan_id=457]\n",
      "\n",
      "(7) Sort\n",
      "Input [2]: [id#313, department#314]\n",
      "Arguments: [id#313 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(8) SortMergeJoin\n",
      "Left keys [1]: [id#0]\n",
      "Right keys [1]: [id#313]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [salary#3, department#314]\n",
      "Input [4]: [id#0, salary#3, id#313, department#314]\n",
      "\n",
      "(10) HashAggregate\n",
      "Input [2]: [salary#3, department#314]\n",
      "Keys [1]: [department#314]\n",
      "Functions [1]: [partial_avg(salary#3)]\n",
      "Aggregate Attributes [2]: [sum#576, count#577L]\n",
      "Results [3]: [department#314, sum#578, count#579L]\n",
      "\n",
      "(11) Exchange\n",
      "Input [3]: [department#314, sum#578, count#579L]\n",
      "Arguments: hashpartitioning(department#314, 200), ENSURE_REQUIREMENTS, [plan_id=464]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [3]: [department#314, sum#578, count#579L]\n",
      "Keys [1]: [department#314]\n",
      "Functions [1]: [avg(salary#3)]\n",
      "Aggregate Attributes [1]: [avg(salary#3)#572]\n",
      "Results [2]: [department#314, avg(salary#3)#572 AS avg_salary#573]\n",
      "\n",
      "(13) Filter\n",
      "Input [2]: [department#314, avg_salary#573]\n",
      "Condition : (isnotnull(avg_salary#573) AND (avg_salary#573 > 50000.0))\n",
      "\n",
      "(14) AdaptiveSparkPlan\n",
      "Output [2]: [department#314, avg_salary#573]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Count: 2, Time: 0.37s\n"
     ]
    }
   ],
   "source": [
    "# Get execution plan\n",
    "df_complex = df.join(dept_df, df.id == dept_df.id) \\\n",
    "               .groupBy(\"department\") \\\n",
    "               .agg(avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "               .filter(col(\"avg_salary\") > 50000)\n",
    "\n",
    "# Logical and physical plans\n",
    "print(\"Logical Plan:\")\n",
    "df_complex.explain()\n",
    "\n",
    "print(\"\\nDetailed Physical Plan:\")\n",
    "df_complex.explain(\"formatted\")\n",
    "\n",
    "# Count with explanation\n",
    "from time import time\n",
    "start = time()\n",
    "count = df_complex.count()\n",
    "end = time()\n",
    "print(f\"Count: {count}, Time: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Monitoring Best Practices\n",
    "\n",
    "1. **Use explain() to Understand Plans**\n",
    "   - Check physical plan for expensive operations\n",
    "   - Look for broadcast joins, exchange (shuffle) operations\n",
    "\n",
    "2. **Monitor Spark UI**\n",
    "   - Available at http://localhost:4040 by default\n",
    "   - Check stage durations, executor utilization\n",
    "   - Identify skew in task durations\n",
    "\n",
    "3. **Performance Metrics to Watch**\n",
    "   - Shuffle read/write size\n",
    "   - Spill memory to disk\n",
    "   - Task durations\n",
    "   - Cache hit ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Common Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+-------------+\n",
      "| id|   name|age| salary|lookup_result|\n",
      "+---+-------+---+-------+-------------+\n",
      "|  1|  Alice| 30|50000.0|            A|\n",
      "|  2|    Bob| 32|60000.0|            B|\n",
      "|  3|Charlie| 28|55000.0|            C|\n",
      "+---+-------+---+-------+-------------+\n",
      "\n",
      "+----------+----------+\n",
      "|department|avg_salary|\n",
      "+----------+----------+\n",
      "|        HR|   60000.0|\n",
      "| Marketing|   55000.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Broadcast variables for lookup tables\n",
    "lookup_dict = {1: \"A\", 2: \"B\", 3: \"C\"}\n",
    "lookup_broadcast = spark.sparkContext.broadcast(lookup_dict)\n",
    "\n",
    "@udf(StringType())\n",
    "def lookup_value(key):\n",
    "    return lookup_broadcast.value.get(key, \"Unknown\")\n",
    "\n",
    "df.withColumn(\"lookup_result\", lookup_value(col(\"id\"))).show()\n",
    "\n",
    "# 2. Avoid collect() on large DataFrames\n",
    "# Good: Take just what you need\n",
    "sample_rows = df.limit(10).collect()\n",
    "\n",
    "# 3. Use SQL when it's more intuitive\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "dept_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT d.department, AVG(e.salary) as avg_salary\n",
    "    FROM employees e JOIN departments d ON e.id = d.id\n",
    "    GROUP BY d.department\n",
    "    HAVING AVG(e.salary) > 50000\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of PySpark Best Practices\n",
    "\n",
    "1. **Data Management**\n",
    "   - Use Parquet format for efficient storage\n",
    "   - Define schemas explicitly\n",
    "   - Partition data wisely\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Cache/persist reused DataFrames\n",
    "   - Use broadcast joins for small tables\n",
    "   - Minimize shuffling operations\n",
    "   - Filter early to reduce data volume\n",
    "\n",
    "3. **Code Practices**\n",
    "   - Prefer built-in functions over UDFs\n",
    "   - Use Pandas UDFs for better performance\n",
    "   - Chain transformations efficiently\n",
    "   - Monitor the Spark UI and physical plans\n",
    "\n",
    "4. **Resource Management**\n",
    "   - Tune partitioning based on cluster size\n",
    "   - Configure memory settings appropriately\n",
    "   - Clean up cached data when no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
