{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark File Format Guide: Reading and Writing Data\n",
    "\n",
    "This notebook demonstrates how to efficiently read and write data in various file formats using PySpark:\n",
    "\n",
    "1. CSV\n",
    "2. JSON\n",
    "3. Parquet\n",
    "4. Avro\n",
    "\n",
    "For each format, we'll cover:\n",
    "- Basic reading and writing operations\n",
    "- Common options and parameters\n",
    "- Performance considerations\n",
    "- Best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4d80b244-e9d8-4cbe-bf9d-2286bd9beae6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4d80b244-e9d8-4cbe-bf9d-2286bd9beae6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/04/18 06:54:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47654)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, rand, monotonically_increasing_id\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, BooleanType, DateType, TimestampType\n",
    ")\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"File Format Guide\") \\\n",
    "    .config(\"spark.sql.avro.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Data\n",
    "\n",
    "First, let's create a sample dataset to work with throughout this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "| id|        name|age|         city|  salary|is_manager| hire_date|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "|  1|    John Doe| 35|     New York| 72000.5|      true|2020-01-15|\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|     false|2019-06-22|\n",
      "|  3|Robert Brown| 42|      Chicago|92000.75|      true|2021-03-08|\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25|      true|2018-11-30|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|     false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple dataset with different data types\n",
    "data = [\n",
    "    (1, \"John Doe\", 35, \"New York\", 72000.50, True, \"2020-01-15\"),\n",
    "    (2, \"Jane Smith\", 28, \"San Francisco\", 86000.00, False, \"2019-06-22\"),\n",
    "    (3, \"Robert Brown\", 42, \"Chicago\", 92000.75, True, \"2021-03-08\"),\n",
    "    (4, \"Maria Garcia\", 31, \"Los Angeles\", 67500.25, True, \"2018-11-30\"),\n",
    "    (5, \"James Wilson\", 45, \"Seattle\", 115000.00, False, \"2022-02-12\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"is_manager\", BooleanType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"/tmp/spark_data\", exist_ok=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) is a widely-used format for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files written successfully.\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV write\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"/tmp/spark_data/basic.csv\")\n",
    "\n",
    "# CSV write with options\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
    "\n",
    "# CSV with partition (data is stored in subdirectories by city)\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/tmp/spark_data/partitioned.csv\")\n",
    "\n",
    "print(\"CSV files written successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV Read (note how column names are generic and types are strings):\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n",
      "+---+------------+---+-------------+--------+-----+----------+\n",
      "|_c0|         _c1|_c2|          _c3|     _c4|  _c5|       _c6|\n",
      "+---+------------+---+-------------+--------+-----+----------+\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25| true|2018-11-30|\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|false|2019-06-22|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "CSV Read with Header and Schema Inference:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "| id|        name|age|         city|  salary|is_manager| hire_date|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25|      true|2018-11-30|\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|     false|2019-06-22|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|     false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "CSV Read with Explicit Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "| id|        name|age|         city|  salary|is_manager| hire_date|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25|      true|2018-11-30|\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|     false|2019-06-22|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|     false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV read\n",
    "df_csv_basic = spark.read.csv(\"/tmp/spark_data/basic.csv\")\n",
    "\n",
    "print(\"Basic CSV Read (note how column names are generic and types are strings):\")\n",
    "df_csv_basic.printSchema()\n",
    "df_csv_basic.show(3)\n",
    "\n",
    "# Read with options and schema inference\n",
    "df_csv_with_header = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
    "\n",
    "print(\"\\nCSV Read with Header and Schema Inference:\")\n",
    "df_csv_with_header.printSchema()\n",
    "df_csv_with_header.show(3)\n",
    "\n",
    "# Read with explicit schema\n",
    "df_csv_with_schema = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
    "\n",
    "print(\"\\nCSV Read with Explicit Schema:\")\n",
    "df_csv_with_schema.printSchema()\n",
    "df_csv_with_schema.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Best Practices\n",
    "\n",
    "1. **Always specify a schema for production workloads** - Schema inference is convenient but slow and may guess the wrong types.\n",
    "2. **Use header=true when possible** - Makes your data self-documenting.\n",
    "3. **Set appropriate nullValue option** - Define how NULL values are represented in your CSV.\n",
    "4. **Be explicit with date formats** - Set dateFormat to ensure correct parsing.\n",
    "5. **For large datasets, consider**:\n",
    "   - Setting compression (compression='gzip')\n",
    "   - Proper partitioning (partitionBy)\n",
    "   - Specifying escape characters for special data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is excellent for semi-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files written successfully.\n"
     ]
    }
   ],
   "source": [
    "# Basic JSON write\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/tmp/spark_data/basic.json\")\n",
    "\n",
    "# JSON write with options\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .json(\"/tmp/spark_data/compressed.json\")\n",
    "\n",
    "# JSON with pretty printing (one record per line, properly formatted)\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"pretty\", \"true\") \\\n",
    "    .json(\"/tmp/spark_data/pretty.json\")\n",
    "\n",
    "print(\"JSON files written successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic JSON Read (with schema inference):\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+-------------+----------+---+----------+------------+--------+\n",
      "|age|         city| hire_date| id|is_manager|        name|  salary|\n",
      "+---+-------------+----------+---+----------+------------+--------+\n",
      "| 28|San Francisco|2019-06-22|  2|     false|  Jane Smith| 86000.0|\n",
      "| 31|  Los Angeles|2018-11-30|  4|      true|Maria Garcia|67500.25|\n",
      "| 45|      Seattle|2022-02-12|  5|     false|James Wilson|115000.0|\n",
      "+---+-------------+----------+---+----------+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "JSON Read with Explicit Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "| id|        name|age|         city|  salary|is_manager| hire_date|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|     false|2019-06-22|\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25|      true|2018-11-30|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|     false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "Multi-line JSON Read:\n",
      "+---+-------------+----------+---+----------+----------+-------+\n",
      "|age|         city| hire_date| id|is_manager|      name| salary|\n",
      "+---+-------------+----------+---+----------+----------+-------+\n",
      "| 35|     New York|2020-01-15|  1|      true|  John Doe|72000.5|\n",
      "| 28|San Francisco|2019-06-22|  2|     false|Jane Smith|86000.0|\n",
      "+---+-------------+----------+---+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic JSON read\n",
    "df_json_basic = spark.read.json(\"/tmp/spark_data/basic.json\")\n",
    "\n",
    "print(\"Basic JSON Read (with schema inference):\")\n",
    "df_json_basic.printSchema()\n",
    "df_json_basic.show(3)\n",
    "\n",
    "# Read with explicit schema\n",
    "df_json_with_schema = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .json(\"/tmp/spark_data/basic.json\")\n",
    "\n",
    "print(\"\\nJSON Read with Explicit Schema:\")\n",
    "df_json_with_schema.printSchema()\n",
    "df_json_with_schema.show(3)\n",
    "\n",
    "# Reading multi-line JSON (where each JSON object may span multiple lines)\n",
    "multi_line_json = \"\"\"[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"John Doe\",\n",
    "        \"age\": 35,\n",
    "        \"city\": \"New York\",\n",
    "        \"salary\": 72000.50,\n",
    "        \"is_manager\": true,\n",
    "        \"hire_date\": \"2020-01-15\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Jane Smith\",\n",
    "        \"age\": 28,\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"salary\": 86000.00,\n",
    "        \"is_manager\": false,\n",
    "        \"hire_date\": \"2019-06-22\"\n",
    "    }\n",
    "]\"\"\"\n",
    "\n",
    "# Write multi-line JSON to a file\n",
    "with open(\"/tmp/spark_data/multiline.json\", \"w\") as f:\n",
    "    f.write(multi_line_json)\n",
    "\n",
    "# Read multi-line JSON\n",
    "df_multiline_json = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"/tmp/spark_data/multiline.json\")\n",
    "\n",
    "print(\"\\nMulti-line JSON Read:\")\n",
    "df_multiline_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Best Practices\n",
    "\n",
    "1. **Use multiline=true when needed** - For JSON files where objects span multiple lines.\n",
    "2. **Prefer one record per line** - For performance and parallelism.\n",
    "3. **Use explicit schemas in production** - For consistent type handling.\n",
    "4. **Consider compression** - JSON is verbose, so compression helps with storage.\n",
    "5. **Be careful with complex nested structures** - These can be processed but impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parquet Files\n",
    "\n",
    "Parquet is a columnar format optimized for analytics workloads, offering efficient storage and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files written successfully.\n"
     ]
    }
   ],
   "source": [
    "# Basic Parquet write\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/spark_data/basic.parquet\")\n",
    "\n",
    "# Parquet with compression options\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"/tmp/spark_data/compressed.parquet\")\n",
    "\n",
    "# Parquet with partitioning\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\", \"is_manager\") \\\n",
    "    .parquet(\"/tmp/spark_data/partitioned.parquet\")\n",
    "\n",
    "print(\"Parquet files written successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Parquet Read (schemas are preserved):\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "| id|        name|age|         city|  salary|is_manager| hire_date|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "|  4|Maria Garcia| 31|  Los Angeles|67500.25|      true|2018-11-30|\n",
      "|  2|  Jane Smith| 28|San Francisco| 86000.0|     false|2019-06-22|\n",
      "|  5|James Wilson| 45|      Seattle|115000.0|     false|2022-02-12|\n",
      "+---+------------+---+-------------+--------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "Parquet Read with Column Projection:\n",
      "+---+------------+-------------+\n",
      "| id|        name|         city|\n",
      "+---+------------+-------------+\n",
      "|  4|Maria Garcia|  Los Angeles|\n",
      "|  2|  Jane Smith|San Francisco|\n",
      "|  5|James Wilson|      Seattle|\n",
      "+---+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "Parquet Read with Partition Filtering:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#818,name#819,age#820,salary#821,hire_date#822,city#823,is_manager#824] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/spark_data/partitioned.parquet], PartitionFilters: [isnotnull(city#823), (city#823 = New York)], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,salary:double,hire_date:string>\n",
      "\n",
      "\n",
      "+---+--------+---+-------+----------+--------+----------+\n",
      "| id|    name|age| salary| hire_date|    city|is_manager|\n",
      "+---+--------+---+-------+----------+--------+----------+\n",
      "|  1|John Doe| 35|72000.5|2020-01-15|New York|      true|\n",
      "+---+--------+---+-------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Parquet read\n",
    "df_parquet_basic = spark.read.parquet(\"/tmp/spark_data/basic.parquet\")\n",
    "\n",
    "print(\"Basic Parquet Read (schemas are preserved):\")\n",
    "df_parquet_basic.printSchema()\n",
    "df_parquet_basic.show(3)\n",
    "\n",
    "# Read with column projection (reading only specific columns)\n",
    "df_parquet_select = spark.read.parquet(\"/tmp/spark_data/basic.parquet\").select(\"id\", \"name\", \"city\")\n",
    "\n",
    "print(\"\\nParquet Read with Column Projection:\")\n",
    "df_parquet_select.show(3)\n",
    "\n",
    "# Read with partition discovery and filtering\n",
    "df_parquet_filtered = spark.read.parquet(\"/tmp/spark_data/partitioned.parquet\") \\\n",
    "    .filter(col(\"city\") == \"New York\")\n",
    "\n",
    "print(\"\\nParquet Read with Partition Filtering:\")\n",
    "df_parquet_filtered.explain()  # Look for PushedFilters and PartitionFilters in the plan\n",
    "df_parquet_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Best Practices\n",
    "\n",
    "1. **Use Parquet for analytics workloads** - It's optimized for query performance.\n",
    "2. **Choose appropriate partitioning** - Partition on columns used for filtering.\n",
    "3. **Enable predicate pushdown** - Filtering happens at file read time, not after loading data.\n",
    "4. **Choose Snappy compression** - Good balance of compression ratio and speed.\n",
    "5. **Consider file size** - Aim for parquet files in the 256MB-1GB range for best performance.\n",
    "6. **Design for column projection** - Parquet shines when you only need to read a subset of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Let's create a larger dataset and compare the formats for both writing and reading performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=============================================>       (165 + 12) / 192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created performance test dataset with 80 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create a larger dataset\n",
    "large_df = df\n",
    "for i in range(4):  # Will give us about 80 rows (5 * 2^4)\n",
    "    large_df = large_df.union(large_df)\n",
    "\n",
    "# Add some randomness\n",
    "large_df = large_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "large_df = large_df.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
    "\n",
    "print(f\"Created performance test dataset with {large_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure write performance\n",
    "def measure_write_performance(df, format_name, options={}):\n",
    "    path = f\"/tmp/spark_data/perf_{format_name}\"\n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "    \n",
    "    # Add options\n",
    "    for k, v in options.items():\n",
    "        writer = writer.option(k, v)\n",
    "    \n",
    "    # Measure write time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if format_name == \"avro\":\n",
    "        writer.format(\"avro\").save(path)\n",
    "    else:\n",
    "        getattr(writer, format_name)(path)\n",
    "    \n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    return path, write_time\n",
    "\n",
    "# Function to measure read performance\n",
    "def measure_read_performance(format_name, path, options={}):\n",
    "    reader = spark.read\n",
    "    \n",
    "    # Add options\n",
    "    for k, v in options.items():\n",
    "        reader = reader.option(k, v)\n",
    "    \n",
    "    # Measure read time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if format_name == \"avro\":\n",
    "        df_read = reader.format(\"avro\").load(path)\n",
    "    else:\n",
    "        df_read = getattr(reader, format_name)(path)\n",
    "    \n",
    "    count = df_read.count()  # Force execution\n",
    "    read_time = time.time() - start_time\n",
    "    \n",
    "    return read_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison:\n",
      "Format\tWrite Time (s)\tRead Time (s)\n",
      "------\t--------------\t-------------\n",
      "CSV\t1.75\t\t0.32\n",
      "JSON\t1.53\t\t0.18\n",
      "Parquet\t1.64\t\t0.09\n"
     ]
    }
   ],
   "source": [
    "# Run performance comparison\n",
    "results = []\n",
    "\n",
    "# Test CSV\n",
    "csv_path, csv_write_time = measure_write_performance(\n",
    "    large_df, \"csv\", {\"header\": \"true\"}\n",
    ")\n",
    "csv_read_time = measure_read_performance(\n",
    "    \"csv\", csv_path, {\"header\": \"true\", \"inferSchema\": \"true\"}\n",
    ")\n",
    "results.append((\"CSV\", csv_write_time, csv_read_time))\n",
    "\n",
    "# Test JSON\n",
    "json_path, json_write_time = measure_write_performance(large_df, \"json\")\n",
    "json_read_time = measure_read_performance(\"json\", json_path)\n",
    "results.append((\"JSON\", json_write_time, json_read_time))\n",
    "\n",
    "# Test Parquet\n",
    "parquet_path, parquet_write_time = measure_write_performance(\n",
    "    large_df, \"parquet\", {\"compression\": \"snappy\"}\n",
    ")\n",
    "parquet_read_time = measure_read_performance(\"parquet\", parquet_path)\n",
    "results.append((\"Parquet\", parquet_write_time, parquet_read_time))\n",
    "\n",
    "# Display results\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"Format\\tWrite Time (s)\\tRead Time (s)\")\n",
    "print(\"------\\t--------------\\t-------------\")\n",
    "for format_name, write_time, read_time in results:\n",
    "    print(f\"{format_name}\\t{write_time:.2f}\\t\\t{read_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. File Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size Comparison:\n",
      "Format\tSize\n",
      "------\t----\n",
      "CSV\t672K\n",
      "JSON\t668K\n",
      "Parquet\t672K\n",
      "Avro\tNot tested\n"
     ]
    }
   ],
   "source": [
    "def get_directory_size(path):\n",
    "    # A helper function to calculate directory size\n",
    "    # This is a simplified version and might not work in all environments\n",
    "    import subprocess\n",
    "    try:\n",
    "        output = subprocess.check_output(['du', '-sh', path]).decode('utf-8')\n",
    "        size = output.split()[0]\n",
    "        return size\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Measure directory sizes\n",
    "print(\"File Size Comparison:\")\n",
    "print(\"Format\\tSize\")\n",
    "print(\"------\\t----\")\n",
    "print(f\"CSV\\t{get_directory_size(csv_path)}\")\n",
    "print(f\"JSON\\t{get_directory_size(json_path)}\")\n",
    "print(f\"Parquet\\t{get_directory_size(parquet_path)}\")\n",
    "try:\n",
    "    print(f\"Avro\\t{get_directory_size(avro_path)}\")\n",
    "except NameError:\n",
    "    print(\"Avro\\tNot tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Format Selection Guide\n",
    "\n",
    "### When to use each format:\n",
    "\n",
    "**CSV**\n",
    "- ✅ Human-readable, universal compatibility\n",
    "- ✅ Easy to edit manually or with spreadsheet software\n",
    "- ❌ Poor performance for large data\n",
    "- ❌ Inefficient storage (no compression by default)\n",
    "- ❌ No schema preservation\n",
    "- **Best for**: Small datasets, data exchange with non-Spark systems, human-editable files\n",
    "\n",
    "**JSON**\n",
    "- ✅ Supports nested structures\n",
    "- ✅ Good compatibility with web services\n",
    "- ✅ Human-readable\n",
    "- ❌ Verbose format, larger files\n",
    "- ❌ Slower than binary formats\n",
    "- **Best for**: API integrations, semi-structured data, moderate-sized datasets\n",
    "\n",
    "**Parquet**\n",
    "- ✅ Best query performance\n",
    "- ✅ Column pruning and predicate pushdown\n",
    "- ✅ Efficient compression\n",
    "- ✅ Schema preservation\n",
    "- ❌ Not human-readable\n",
    "- ❌ Less universal than CSV/JSON\n",
    "- **Best for**: Analytics workloads, large datasets, frequent querying, column-oriented access patterns\n",
    "\n",
    "**Avro**\n",
    "- ✅ Schema evolution support\n",
    "- ✅ Good for record/row-based access\n",
    "- ✅ Rich data type support\n",
    "- ✅ Compact binary format\n",
    "- ❌ Not as efficient as Parquet for analytics\n",
    "- ❌ Not human-readable\n",
    "- **Best for**: Data with evolving schemas, streaming data integration, row-oriented access patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of Key Options and Parameters\n",
    "\n",
    "### Common Parameters for All Formats\n",
    "- `mode`: `overwrite`, `append`, `ignore`, `error` (default)\n",
    "- `partitionBy`: Saves data in partitioned directory structure\n",
    "\n",
    "### CSV Options\n",
    "- `header`: `true` to include column names\n",
    "- `delimiter`: Character separator (default `,`)\n",
    "- `quote`: Character for quoting (default `\"`)\n",
    "- `escape`: Character for escaping (default `\\`)\n",
    "- `nullValue`: String representation for null values\n",
    "- `inferSchema`: Automatically detect column types\n",
    "- `dateFormat`: Format string for date parsing\n",
    "\n",
    "### JSON Options\n",
    "- `multiLine`: `true` for multi-line JSON records\n",
    "- `dateFormat`: Format string for date parsing\n",
    "- `compression`: Compression codec (e.g., `gzip`, `snappy`)\n",
    "- `primitivesAsString`: Convert primitives to strings\n",
    "- `allowUnquotedFieldNames`: Allow unquoted field names\n",
    "\n",
    "### Parquet Options\n",
    "- `compression`: Compression codec (e.g., `snappy`, `gzip`, `none`)\n",
    "- `mergeSchema`: Reconcile schemas when reading multiple files\n",
    "- `partitionOverwriteMode`: `static` or `dynamic` partition overwrite\n",
    "\n",
    "### Avro Options\n",
    "- `avroSchema`: User-provided schema\n",
    "- `compression`: Compression codec (e.g., `snappy`, `deflate`)\n",
    "- `recordName`: Record name in the Avro schema\n",
    "- `recordNamespace`: Namespace in the Avro schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
