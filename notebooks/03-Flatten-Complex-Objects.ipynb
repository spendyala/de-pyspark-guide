{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening Complex Objects in PySpark with UDFs\n",
    "\n",
    "This notebook demonstrates how to use User-Defined Functions (UDFs) to flatten complex nested data structures in PySpark. We'll cover three common scenarios:\n",
    "\n",
    "1. Flattening arrays into strings\n",
    "2. Extracting data from nested structures\n",
    "3. Flattening map/dictionary structures\n",
    "\n",
    "Let's first initialize our SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 06:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59644)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, explode, array_join\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, MapType, IntegerType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Flatten Object Examples\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flattening Arrays\n",
    "\n",
    "Arrays are common data structures in Spark DataFrames. Sometimes you need to convert an array into a single string (e.g., for reporting, exporting to CSV, etc.).\n",
    "\n",
    "Let's create a DataFrame with an array column and flatten it using a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Array:\n",
      "+---+-----------------------+\n",
      "|id |fruits                 |\n",
      "+---+-----------------------+\n",
      "|1  |[apple, banana, cherry]|\n",
      "|2  |[orange, grape]        |\n",
      "|3  |[]                     |\n",
      "|4  |NULL                   |\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with array column\n",
    "array_data = [\n",
    "    (1, [\"apple\", \"banana\", \"cherry\"]),\n",
    "    (2, [\"orange\", \"grape\"]),\n",
    "    (3, []),\n",
    "    (4, None)  # Handle null case\n",
    "]\n",
    "array_df = spark.createDataFrame(array_data, [\"id\", \"fruits\"])\n",
    "\n",
    "print(\"Original DataFrame with Array:\")\n",
    "array_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Approach to Flatten Array\n",
    "\n",
    "Let's define a UDF that converts an array to a comma-separated string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Flattened Array:\n",
      "+---+-----------------------+---------------------+\n",
      "|id |fruits                 |flattened_fruits     |\n",
      "+---+-----------------------+---------------------+\n",
      "|1  |[apple, banana, cherry]|apple, banana, cherry|\n",
      "|2  |[orange, grape]        |orange, grape        |\n",
      "|3  |[]                     |                     |\n",
      "|4  |NULL                   |NULL                 |\n",
      "+---+-----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define UDF to flatten array to comma-separated string\n",
    "@udf(StringType())\n",
    "def flatten_array(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    return \", \".join(arr)\n",
    "\n",
    "# Apply the UDF\n",
    "flattened_array_df = array_df.withColumn(\"flattened_fruits\", flatten_array(col(\"fruits\")))\n",
    "\n",
    "print(\"DataFrame with Flattened Array:\")\n",
    "flattened_array_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Function Alternative\n",
    "\n",
    "While UDFs work well, Spark provides a built-in function `array_join()` that can be more efficient for this particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Built-in array_join Function:\n",
      "+---+-----------------------+-------------------------+\n",
      "|id |fruits                 |flattened_fruits_built_in|\n",
      "+---+-----------------------+-------------------------+\n",
      "|1  |[apple, banana, cherry]|apple, banana, cherry    |\n",
      "|2  |[orange, grape]        |orange, grape            |\n",
      "|3  |[]                     |                         |\n",
      "|4  |NULL                   |NULL                     |\n",
      "+---+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using built-in function (more efficient)\n",
    "array_df_built_in = array_df.withColumn(\n",
    "    \"flattened_fruits_built_in\", \n",
    "    array_join(col(\"fruits\"), \", \")\n",
    ")\n",
    "\n",
    "print(\"Using Built-in array_join Function:\")\n",
    "array_df_built_in.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Flattening Nested Structures\n",
    "\n",
    "Nested structures (structs) are common in semi-structured data like JSON. Let's see how to extract and flatten data from deeply nested fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Nested Structure:\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- person: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- zip: string (nullable = true)\n",
      "\n",
      "+---+-------------------------------+\n",
      "|id |person                         |\n",
      "+---+-------------------------------+\n",
      "|1  |{John, {New York, 10001}}      |\n",
      "|2  |{Alice, {San Francisco, 94105}}|\n",
      "|3  |{Bob, NULL}                    |\n",
      "|4  |NULL                           |\n",
      "+---+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema with nested structure\n",
    "nested_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"person\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"zip\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create data with nested structure\n",
    "nested_data = [\n",
    "    (1, {\"name\": \"John\", \"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (2, {\"name\": \"Alice\", \"address\": {\"city\": \"San Francisco\", \"zip\": \"94105\"}}),\n",
    "    (3, {\"name\": \"Bob\", \"address\": None}),\n",
    "    (4, None)  # Handle completely null record\n",
    "]\n",
    "nested_df = spark.createDataFrame(nested_data, nested_schema)\n",
    "\n",
    "print(\"Original DataFrame with Nested Structure:\")\n",
    "nested_df.printSchema()\n",
    "nested_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Approach for Nested Structures\n",
    "\n",
    "Let's create a UDF that combines fields from a nested structure into a single string, with appropriate null handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Flattened Nested Structure:\n",
      "+---+-----------------------------------+\n",
      "|id |person_summary                     |\n",
      "+---+-----------------------------------+\n",
      "|1  |John lives in New York, 10001      |\n",
      "|2  |Alice lives in San Francisco, 94105|\n",
      "|3  |Bob - No address                   |\n",
      "|4  |No person data                     |\n",
      "+---+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define UDF to flatten the nested structure\n",
    "@udf(StringType())\n",
    "def flatten_address(person):\n",
    "    if person is None:\n",
    "        return \"No person data\"\n",
    "    if person[\"address\"] is None:\n",
    "        return f\"{person['name']} - No address\"\n",
    "    \n",
    "    address = person[\"address\"]\n",
    "    return f\"{person['name']} lives in {address['city']}, {address['zip']}\"\n",
    "\n",
    "# Apply the UDF\n",
    "flattened_nested_df = nested_df.withColumn(\n",
    "    \"person_summary\", \n",
    "    flatten_address(col(\"person\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame with Flattened Nested Structure:\")\n",
    "flattened_nested_df.select(\"id\", \"person_summary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Column Reference Alternative\n",
    "\n",
    "For accessing specific nested fields directly, Spark allows dot notation which can be more efficient than UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Direct Column References:\n",
      "+---+-----+-------------+--------+\n",
      "|id |name |city         |zip_code|\n",
      "+---+-----+-------------+--------+\n",
      "|1  |John |New York     |10001   |\n",
      "|2  |Alice|San Francisco|94105   |\n",
      "|3  |Bob  |NULL         |NULL    |\n",
      "|4  |NULL |NULL         |NULL    |\n",
      "+---+-----+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting fields with direct column references\n",
    "direct_access_df = nested_df.select(\n",
    "    \"id\",\n",
    "    col(\"person.name\").alias(\"name\"),\n",
    "    col(\"person.address.city\").alias(\"city\"),\n",
    "    col(\"person.address.zip\").alias(\"zip_code\")\n",
    ")\n",
    "\n",
    "print(\"Using Direct Column References:\")\n",
    "direct_access_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flattening Maps/Dictionaries\n",
    "\n",
    "Maps (key-value pairs) are often used for sparse data or attribute collections. Let's see how to extract and format map data using UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Map:\n",
      "+---+----------------------------------------+\n",
      "|id |metrics                                 |\n",
      "+---+----------------------------------------+\n",
      "|1  |{weight -> 75, age -> 30, height -> 180}|\n",
      "|2  |{weight -> 65, height -> 165}           |\n",
      "|3  |{}                                      |\n",
      "|4  |NULL                                    |\n",
      "+---+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with a map column\n",
    "map_data = [\n",
    "    (1, {\"height\": 180, \"weight\": 75, \"age\": 30}),\n",
    "    (2, {\"height\": 165, \"weight\": 65}),  # Missing age key\n",
    "    (3, {}),  # Empty map\n",
    "    (4, None)  # Null map\n",
    "]\n",
    "map_df = spark.createDataFrame(map_data, [\"id\", \"metrics\"])\n",
    "\n",
    "print(\"Original DataFrame with Map:\")\n",
    "map_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Approach for Maps\n",
    "\n",
    "Let's create a UDF that extracts specific values from the map and formats them with default handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Flattened Map:\n",
      "+---+----------------------------------------+------------------------------------------+\n",
      "|id |metrics                                 |metrics_summary                           |\n",
      "+---+----------------------------------------+------------------------------------------+\n",
      "|1  |{weight -> 75, age -> 30, height -> 180}|Height: 180cm, Weight: 75kg, Age: 30y     |\n",
      "|2  |{weight -> 65, height -> 165}           |Height: 165cm, Weight: 65kg, Age: unknowny|\n",
      "|3  |{}                                      |Empty metrics                             |\n",
      "|4  |NULL                                    |No metrics data                           |\n",
      "+---+----------------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define UDF to extract specific values with default handling\n",
    "@udf(StringType())\n",
    "def flatten_metrics(metrics):\n",
    "    if metrics is None:\n",
    "        return \"No metrics data\"\n",
    "    if len(metrics) == 0:\n",
    "        return \"Empty metrics\"\n",
    "    \n",
    "    # Extract values with defaults\n",
    "    height = metrics.get(\"height\", \"unknown\")\n",
    "    weight = metrics.get(\"weight\", \"unknown\")\n",
    "    age = metrics.get(\"age\", \"unknown\")\n",
    "    \n",
    "    return f\"Height: {height}cm, Weight: {weight}kg, Age: {age}y\"\n",
    "\n",
    "# Apply the UDF\n",
    "flattened_map_df = map_df.withColumn(\"metrics_summary\", flatten_metrics(col(\"metrics\")))\n",
    "\n",
    "print(\"DataFrame with Flattened Map:\")\n",
    "flattened_map_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode Alternative\n",
    "\n",
    "If you need to extract all keys and values, you can use the `explode` function to convert the map to rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using explode to Convert Map to Rows:\n",
      "+---+-----------+------------+\n",
      "|id |metric_name|metric_value|\n",
      "+---+-----------+------------+\n",
      "|1  |weight     |75          |\n",
      "|1  |age        |30          |\n",
      "|1  |height     |180         |\n",
      "|2  |weight     |65          |\n",
      "|2  |height     |165         |\n",
      "+---+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, map_keys, map_values, col, size\n",
    "\n",
    "# Filter out nulls and empty maps for the explode\n",
    "map_df_filtered = map_df.filter(col(\"metrics\").isNotNull() & (size(map_keys(col(\"metrics\"))) > 0))\n",
    "\n",
    "# Explode the map into rows\n",
    "exploded_map_df = map_df_filtered.select(\n",
    "    \"id\",\n",
    "    explode(col(\"metrics\")).alias(\"metric_name\", \"metric_value\")\n",
    ")\n",
    "\n",
    "print(\"Using explode to Convert Map to Rows:\")\n",
    "exploded_map_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complex Example: Combining Multiple Complex Objects\n",
    "\n",
    "In real-world scenarios, you often need to handle multiple complex types together. Let's create a more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex DataFrame:\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- skills: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- properties: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------+\n",
      "|id |user                                                                 |\n",
      "+---+---------------------------------------------------------------------+\n",
      "|1  |{John, [Python, SQL, Spark], {level -> Senior, dept -> Data Science}}|\n",
      "|2  |{Mary, [Java, Scala], {location -> Remote, dept -> Engineering}}     |\n",
      "|3  |{Bob, [], {}}                                                        |\n",
      "+---+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a complex schema\n",
    "complex_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"user\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"skills\", ArrayType(StringType()), True),\n",
    "        StructField(\"properties\", MapType(StringType(), StringType()), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create complex data\n",
    "complex_data = [\n",
    "    (1, {\n",
    "        \"name\": \"John\", \n",
    "        \"skills\": [\"Python\", \"SQL\", \"Spark\"], \n",
    "        \"properties\": {\"dept\": \"Data Science\", \"level\": \"Senior\"}\n",
    "    }),\n",
    "    (2, {\n",
    "        \"name\": \"Mary\", \n",
    "        \"skills\": [\"Java\", \"Scala\"], \n",
    "        \"properties\": {\"dept\": \"Engineering\", \"location\": \"Remote\"}\n",
    "    }),\n",
    "    (3, {\n",
    "        \"name\": \"Bob\", \n",
    "        \"skills\": [], \n",
    "        \"properties\": {}\n",
    "    })\n",
    "]\n",
    "complex_df = spark.createDataFrame(complex_data, complex_schema)\n",
    "\n",
    "print(\"Complex DataFrame:\")\n",
    "complex_df.printSchema()\n",
    "complex_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive UDF for Complex Flattening\n",
    "\n",
    "Let's create a UDF that processes the entire complex structure to create a comprehensive profile string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Comprehensive User Profile:\n",
      "+---+---------------------------------------------------------------------------------------+\n",
      "|id |user_profile                                                                           |\n",
      "+---+---------------------------------------------------------------------------------------+\n",
      "|1  |User: John | Skills: Python, SQL, Spark | Properties: level: Senior, dept: Data Science|\n",
      "|2  |User: Mary | Skills: Java, Scala | Properties: location: Remote, dept: Engineering     |\n",
      "|3  |User: Bob | Skills: No skills listed | Properties: No properties listed                |\n",
      "+---+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define comprehensive flattening UDF\n",
    "@udf(StringType())\n",
    "def create_user_profile(user):\n",
    "    if user is None:\n",
    "        return \"No user data\"\n",
    "    \n",
    "    # Extract basic info\n",
    "    name = user[\"name\"]\n",
    "    \n",
    "    # Process skills (array)\n",
    "    skills = user[\"skills\"]\n",
    "    if skills and len(skills) > 0:\n",
    "        skills_str = \", \".join(skills)\n",
    "    else:\n",
    "        skills_str = \"No skills listed\"\n",
    "    \n",
    "    # Process properties (map)\n",
    "    props = user[\"properties\"]\n",
    "    if props and len(props) > 0:\n",
    "        # Create a formatted string from the map\n",
    "        props_str = \", \".join([f\"{k}: {v}\" for k, v in props.items()])\n",
    "    else:\n",
    "        props_str = \"No properties listed\"\n",
    "    \n",
    "    # Combine everything into a profile\n",
    "    return f\"User: {name} | Skills: {skills_str} | Properties: {props_str}\"\n",
    "\n",
    "# Apply the UDF\n",
    "profile_df = complex_df.withColumn(\"user_profile\", create_user_profile(col(\"user\")))\n",
    "\n",
    "print(\"DataFrame with Comprehensive User Profile:\")\n",
    "profile_df.select(\"id\", \"user_profile\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Best Practices for Flattening Complex Objects\n",
    "\n",
    "1. **Choose the Right Approach:**\n",
    "   - Use built-in functions when possible (like `array_join`, dot notation, `explode`)\n",
    "   - Use UDFs when you need custom logic or handling multiple nested levels together\n",
    "\n",
    "2. **Always Handle Nulls and Edge Cases:**\n",
    "   - Check for `None` values at each level of nesting\n",
    "   - Provide meaningful defaults or error messages\n",
    "   - Handle empty collections (empty arrays, maps)\n",
    "\n",
    "3. **Consider Performance:**\n",
    "   - UDFs have serialization overhead - avoid for simple operations\n",
    "   - For large-scale data, consider restructuring data model if possible\n",
    "   - For better performance with UDFs, consider Pandas UDFs (vectorized UDFs)\n",
    "\n",
    "4. **Documentation:**\n",
    "   - Document complex UDFs clearly with examples\n",
    "   - Specify return types explicitly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
