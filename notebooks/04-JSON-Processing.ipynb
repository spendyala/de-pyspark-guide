{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing JSON Data in PySpark\n",
    "\n",
    "This notebook demonstrates how to read, parse, and extract fields from JSON data using PySpark. We'll cover:\n",
    "\n",
    "1. Reading JSON from various sources\n",
    "2. Extracting fields from JSON structures\n",
    "3. Working with nested JSON objects\n",
    "4. Handling JSON arrays\n",
    "5. Schema inference and explicit schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 06:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60890)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, to_json, json_tuple, get_json_object, size\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"JSON Processing\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample JSON Data\n",
    "\n",
    "Let's start by creating some sample JSON data to work with. We'll create JSON strings directly in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple JSON DataFrame:\n",
      "+---+------------------------------------------------+\n",
      "|id |json_data                                       |\n",
      "+---+------------------------------------------------+\n",
      "|1  |{\"name\":\"John\", \"age\":30, \"city\":\"New York\"}    |\n",
      "|2  |{\"name\":\"Alice\", \"age\":25, \"city\":\"Los Angeles\"}|\n",
      "|3  |{\"name\":\"Bob\", \"age\":35, \"city\":\"Chicago\"}      |\n",
      "+---+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple JSON data\n",
    "simple_json_data = [\n",
    "    (1, '{\"name\":\"John\", \"age\":30, \"city\":\"New York\"}'),\n",
    "    (2, '{\"name\":\"Alice\", \"age\":25, \"city\":\"Los Angeles\"}'),\n",
    "    (3, '{\"name\":\"Bob\", \"age\":35, \"city\":\"Chicago\"}')\n",
    "]\n",
    "\n",
    "simple_json_df = spark.createDataFrame(simple_json_data, [\"id\", \"json_data\"])\n",
    "\n",
    "print(\"Simple JSON DataFrame:\")\n",
    "simple_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Fields from JSON Strings\n",
    "\n",
    "### Method 1: Using `json_tuple`\n",
    "\n",
    "The `json_tuple` function allows extracting multiple fields from a JSON string in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON using json_tuple:\n",
      "+---+-----+---+-----------+\n",
      "| id| name|age|       city|\n",
      "+---+-----+---+-----------+\n",
      "|  1| John| 30|   New York|\n",
      "|  2|Alice| 25|Los Angeles|\n",
      "|  3|  Bob| 35|    Chicago|\n",
      "+---+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract fields using json_tuple\n",
    "parsed_json_df1 = simple_json_df.select(\n",
    "    \"id\",\n",
    "    json_tuple(col(\"json_data\"), \"name\", \"age\", \"city\").alias(\"name\", \"age\", \"city\")\n",
    ")\n",
    "\n",
    "print(\"Parsed JSON using json_tuple:\")\n",
    "parsed_json_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using `get_json_object`\n",
    "\n",
    "The `get_json_object` function extracts a single field at a time using a JSONPath expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON using get_json_object:\n",
      "+---+-----+---+-----------+\n",
      "| id| name|age|       city|\n",
      "+---+-----+---+-----------+\n",
      "|  1| John| 30|   New York|\n",
      "|  2|Alice| 25|Los Angeles|\n",
      "|  3|  Bob| 35|    Chicago|\n",
      "+---+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract fields using get_json_object\n",
    "parsed_json_df2 = simple_json_df.select(\n",
    "    \"id\",\n",
    "    get_json_object(col(\"json_data\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.age\").alias(\"age\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.city\").alias(\"city\")\n",
    ")\n",
    "\n",
    "print(\"Parsed JSON using get_json_object:\")\n",
    "parsed_json_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Using `from_json` with Schema\n",
    "\n",
    "For more control and type safety, use `from_json` with an explicit schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON using from_json with schema:\n",
      "+---+-----+---+-----------+\n",
      "| id| name|age|       city|\n",
      "+---+-----+---+-----------+\n",
      "|  1| John| 30|   New York|\n",
      "|  2|Alice| 25|Los Angeles|\n",
      "|  3|  Bob| 35|    Chicago|\n",
      "+---+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for the JSON\n",
    "simple_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse with from_json\n",
    "parsed_json_df3 = simple_json_df.select(\n",
    "    \"id\",\n",
    "    from_json(col(\"json_data\"), simple_schema).alias(\"parsed_data\")\n",
    ")\n",
    "\n",
    "# Extract the struct fields\n",
    "parsed_json_df3 = parsed_json_df3.select(\n",
    "    \"id\",\n",
    "    \"parsed_data.name\",\n",
    "    \"parsed_data.age\",\n",
    "    \"parsed_data.city\"\n",
    ")\n",
    "\n",
    "print(\"Parsed JSON using from_json with schema:\")\n",
    "parsed_json_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Nested JSON\n",
    "\n",
    "Now let's handle more complex, nested JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested JSON DataFrame:\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |json_data                                                                                                                       |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |{\"name\":\"John\", \"contact\":{\"email\":\"john@example.com\", \"phone\":\"555-1234\"}, \"address\":{\"city\":\"New York\", \"zip\":\"10001\"}}       |\n",
      "|2  |{\"name\":\"Alice\", \"contact\":{\"email\":\"alice@example.com\", \"phone\":\"555-5678\"}, \"address\":{\"city\":\"San Francisco\", \"zip\":\"94105\"}}|\n",
      "|3  |{\"name\":\"Bob\", \"contact\":{\"email\":\"bob@example.com\"}, \"address\":{\"city\":\"Chicago\", \"zip\":\"60601\"}}                              |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nested JSON data\n",
    "nested_json_data = [\n",
    "    (1, '{\"name\":\"John\", \"contact\":{\"email\":\"john@example.com\", \"phone\":\"555-1234\"}, \"address\":{\"city\":\"New York\", \"zip\":\"10001\"}}'),\n",
    "    (2, '{\"name\":\"Alice\", \"contact\":{\"email\":\"alice@example.com\", \"phone\":\"555-5678\"}, \"address\":{\"city\":\"San Francisco\", \"zip\":\"94105\"}}'),\n",
    "    (3, '{\"name\":\"Bob\", \"contact\":{\"email\":\"bob@example.com\"}, \"address\":{\"city\":\"Chicago\", \"zip\":\"60601\"}}')\n",
    "]\n",
    "\n",
    "nested_json_df = spark.createDataFrame(nested_json_data, [\"id\", \"json_data\"])\n",
    "\n",
    "print(\"Nested JSON DataFrame:\")\n",
    "nested_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `get_json_object` for Nested Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed nested JSON using get_json_object:\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "| id| name|            email|   phone|         city|  zip|\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "|  1| John| john@example.com|555-1234|     New York|10001|\n",
      "|  2|Alice|alice@example.com|555-5678|San Francisco|94105|\n",
      "|  3|  Bob|  bob@example.com|    NULL|      Chicago|60601|\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract nested fields using get_json_object\n",
    "parsed_nested_df1 = nested_json_df.select(\n",
    "    \"id\",\n",
    "    get_json_object(col(\"json_data\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.contact.email\").alias(\"email\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.contact.phone\").alias(\"phone\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.address.city\").alias(\"city\"),\n",
    "    get_json_object(col(\"json_data\"), \"$.address.zip\").alias(\"zip\")\n",
    ")\n",
    "\n",
    "print(\"Parsed nested JSON using get_json_object:\")\n",
    "parsed_nested_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `from_json` with Nested Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened nested JSON using from_json with schema:\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "| id| name|            email|   phone|         city|  zip|\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "|  1| John| john@example.com|555-1234|     New York|10001|\n",
      "|  2|Alice|alice@example.com|555-5678|San Francisco|94105|\n",
      "|  3|  Bob|  bob@example.com|    NULL|      Chicago|60601|\n",
      "+---+-----+-----------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define nested schema\n",
    "nested_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"contact\", StructType([\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"zip\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Parse nested JSON with schema\n",
    "parsed_nested_df2 = nested_json_df.select(\n",
    "    \"id\",\n",
    "    from_json(col(\"json_data\"), nested_schema).alias(\"data\")\n",
    ")\n",
    "\n",
    "# Flatten the nested structure\n",
    "flattened_df = parsed_nested_df2.select(\n",
    "    \"id\",\n",
    "    \"data.name\",\n",
    "    \"data.contact.email\",\n",
    "    \"data.contact.phone\",\n",
    "    \"data.address.city\",\n",
    "    \"data.address.zip\"\n",
    ")\n",
    "\n",
    "print(\"Flattened nested JSON using from_json with schema:\")\n",
    "flattened_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling JSON Arrays\n",
    "\n",
    "JSON data often contains arrays. Let's see how to process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON with Arrays:\n",
      "+---+---------------------------------------------------+\n",
      "|id |json_data                                          |\n",
      "+---+---------------------------------------------------+\n",
      "|1  |{\"name\":\"John\", \"skills\":[\"Java\", \"Python\", \"SQL\"]}|\n",
      "|2  |{\"name\":\"Alice\", \"skills\":[\"C++\", \"JavaScript\"]}   |\n",
      "|3  |{\"name\":\"Bob\", \"skills\":[]}                        |\n",
      "+---+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON with arrays\n",
    "array_json_data = [\n",
    "    (1, '{\"name\":\"John\", \"skills\":[\"Java\", \"Python\", \"SQL\"]}'),\n",
    "    (2, '{\"name\":\"Alice\", \"skills\":[\"C++\", \"JavaScript\"]}'),\n",
    "    (3, '{\"name\":\"Bob\", \"skills\":[]}')\n",
    "]\n",
    "\n",
    "array_json_df = spark.createDataFrame(array_json_data, [\"id\", \"json_data\"])\n",
    "\n",
    "print(\"JSON with Arrays:\")\n",
    "array_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Arrays with Schema and Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON with arrays:\n",
      "+---+-----+-------------------+\n",
      "|id |name |skills             |\n",
      "+---+-----+-------------------+\n",
      "|1  |John |[Java, Python, SQL]|\n",
      "|2  |Alice|[C++, JavaScript]  |\n",
      "|3  |Bob  |[]                 |\n",
      "+---+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema with array\n",
    "array_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Parse JSON with array\n",
    "parsed_array_df = array_json_df.select(\n",
    "    \"id\",\n",
    "    from_json(col(\"json_data\"), array_schema).alias(\"data\")\n",
    ")\n",
    "\n",
    "# Extract fields\n",
    "parsed_array_df = parsed_array_df.select(\n",
    "    \"id\",\n",
    "    \"data.name\",\n",
    "    \"data.skills\"\n",
    ")\n",
    "\n",
    "print(\"Parsed JSON with arrays:\")\n",
    "parsed_array_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding Arrays\n",
    "\n",
    "We can use `explode` to convert array elements into separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded skills array:\n",
      "+---+-----+----------+\n",
      "| id| name|     skill|\n",
      "+---+-----+----------+\n",
      "|  1| John|      Java|\n",
      "|  1| John|    Python|\n",
      "|  1| John|       SQL|\n",
      "|  2|Alice|       C++|\n",
      "|  2|Alice|JavaScript|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out empty arrays to avoid explode issues\n",
    "non_empty_skills_df = parsed_array_df.filter(col(\"skills\").isNotNull() & (size(col(\"skills\")) > 0))\n",
    "\n",
    "# Explode the skills array\n",
    "exploded_skills_df = non_empty_skills_df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    explode(\"skills\").alias(\"skill\")\n",
    ")\n",
    "\n",
    "print(\"Exploded skills array:\")\n",
    "exploded_skills_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complex JSON with Arrays of Objects\n",
    "\n",
    "Let's handle even more complex JSON with arrays of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex JSON with arrays of objects:\n",
      "+---+--------------------------------------------------------------------------------------------+\n",
      "|id |json_data                                                                                   |\n",
      "+---+--------------------------------------------------------------------------------------------+\n",
      "|1  |{\"name\":\"John\", \"courses\":[{\"name\":\"Python\", \"score\":95}, {\"name\":\"SQL\", \"score\":87}]}      |\n",
      "|2  |{\"name\":\"Alice\", \"courses\":[{\"name\":\"Java\", \"score\":90}, {\"name\":\"JavaScript\", \"score\":85}]}|\n",
      "|3  |{\"name\":\"Bob\", \"courses\":[]}                                                                |\n",
      "+---+--------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON with array of objects\n",
    "complex_json_data = [\n",
    "    (1, '{\"name\":\"John\", \"courses\":[{\"name\":\"Python\", \"score\":95}, {\"name\":\"SQL\", \"score\":87}]}'),\n",
    "    (2, '{\"name\":\"Alice\", \"courses\":[{\"name\":\"Java\", \"score\":90}, {\"name\":\"JavaScript\", \"score\":85}]}'),\n",
    "    (3, '{\"name\":\"Bob\", \"courses\":[]}')\n",
    "]\n",
    "\n",
    "complex_json_df = spark.createDataFrame(complex_json_data, [\"id\", \"json_data\"])\n",
    "\n",
    "print(\"Complex JSON with arrays of objects:\")\n",
    "complex_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Exploding Arrays of Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed complex JSON:\n",
      "+---+-----+------------------------------+\n",
      "|id |name |courses                       |\n",
      "+---+-----+------------------------------+\n",
      "|1  |John |[{Python, 95}, {SQL, 87}]     |\n",
      "|2  |Alice|[{Java, 90}, {JavaScript, 85}]|\n",
      "|3  |Bob  |[]                            |\n",
      "+---+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# Define complex schema\n",
    "course_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "complex_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"courses\", ArrayType(course_schema), True)\n",
    "])\n",
    "\n",
    "# Parse complex JSON\n",
    "parsed_complex_df = complex_json_df.select(\n",
    "    \"id\",\n",
    "    from_json(col(\"json_data\"), complex_schema).alias(\"data\")\n",
    ")\n",
    "\n",
    "parsed_complex_df = parsed_complex_df.select(\n",
    "    \"id\",\n",
    "    \"data.name\",\n",
    "    \"data.courses\"\n",
    ")\n",
    "\n",
    "print(\"Parsed complex JSON:\")\n",
    "parsed_complex_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final exploded courses:\n",
      "+---+-----+-----------+-----+\n",
      "| id| name|course_name|score|\n",
      "+---+-----+-----------+-----+\n",
      "|  1| John|     Python|   95|\n",
      "|  1| John|        SQL|   87|\n",
      "|  2|Alice|       Java|   90|\n",
      "|  2|Alice| JavaScript|   85|\n",
      "+---+-----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out empty arrays\n",
    "non_empty_courses_df = parsed_complex_df.filter(col(\"courses\").isNotNull() & (size(col(\"courses\")) > 0))\n",
    "\n",
    "# Explode courses array\n",
    "exploded_courses_df = non_empty_courses_df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    explode(\"courses\").alias(\"course\")\n",
    ")\n",
    "\n",
    "# Extract fields from the exploded struct\n",
    "final_courses_df = exploded_courses_df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    col(\"course.name\").alias(\"course_name\"),\n",
    "    \"course.score\"\n",
    ")\n",
    "\n",
    "print(\"Final exploded courses:\")\n",
    "final_courses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reading JSON Files\n",
    "\n",
    "In real applications, you often need to read JSON from files. Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON read from file with inferred schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- json_data: string (nullable = true)\n",
      "\n",
      "+---+--------------------+\n",
      "| id|           json_data|\n",
      "+---+--------------------+\n",
      "|  2|{\"name\":\"Alice\", ...|\n",
      "|  1|{\"name\":\"John\", \"...|\n",
      "|  3|{\"name\":\"Bob\", \"a...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's write some sample data to a JSON file\n",
    "simple_json_df.write.mode(\"overwrite\").json(\"/tmp/sample.json\")\n",
    "\n",
    "# Reading JSON files\n",
    "# With schema inference\n",
    "json_file_df = spark.read.json(\"/tmp/sample.json\")\n",
    "print(\"JSON read from file with inferred schema:\")\n",
    "json_file_df.printSchema()\n",
    "json_file_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON read from file with explicit schema:\n",
      "+---+--------------------+\n",
      "| id|           json_data|\n",
      "+---+--------------------+\n",
      "|  2|{\"name\":\"Alice\", ...|\n",
      "|  1|{\"name\":\"John\", \"...|\n",
      "|  3|{\"name\":\"Bob\", \"a...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading with explicit schema\n",
    "file_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"json_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reading with options\n",
    "json_file_df2 = spark.read.option(\"multiLine\", \"true\").schema(file_schema).json(\"/tmp/sample.json\")\n",
    "print(\"JSON read from file with explicit schema:\")\n",
    "json_file_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Converting Dataframe to JSON\n",
    "\n",
    "We can also convert DataFrames back to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame converted to JSON strings:\n",
      "+---+------------------------------------------------+\n",
      "|id |person_json                                     |\n",
      "+---+------------------------------------------------+\n",
      "|1  |{\"name\":\"John\",\"age\":30,\"city\":\"New York\"}      |\n",
      "|2  |{\"name\":\"Alice\",\"age\":25,\"city\":\"San Francisco\"}|\n",
      "|3  |{\"name\":\"Bob\",\"age\":35,\"city\":\"Chicago\"}        |\n",
      "+---+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to JSON string\n",
    "from pyspark.sql.functions import struct, to_json\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data_for_json = [\n",
    "    (1, \"John\", 30, \"New York\"),\n",
    "    (2, \"Alice\", 25, \"San Francisco\"),\n",
    "    (3, \"Bob\", 35, \"Chicago\")\n",
    "]\n",
    "df_for_json = spark.createDataFrame(data_for_json, [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "# Convert to JSON string\n",
    "json_output_df = df_for_json.select(\n",
    "    \"id\",\n",
    "    to_json(struct(\"name\", \"age\", \"city\")).alias(\"person_json\")\n",
    ")\n",
    "\n",
    "print(\"DataFrame converted to JSON strings:\")\n",
    "json_output_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Schema Inference for JSON\n",
    "\n",
    "PySpark can infer the schema from JSON data, which is useful for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred schema from JSON:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- json_data: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 06:50:44 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-27bf6407-3d0f-4dfe-a761-0b3b189d9754/userFiles-3f634710-1cab-42d0-a0f4-1aedee30baef. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-27bf6407-3d0f-4dfe-a761-0b3b189d9754/userFiles-3f634710-1cab-42d0-a0f4-1aedee30baef\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Using schema inference with samplingRatio\n",
    "inferred_df = spark.read.option(\"samplingRatio\", \"0.8\").json(\"/tmp/sample.json\")\n",
    "\n",
    "print(\"Inferred schema from JSON:\")\n",
    "inferred_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Best Practices for JSON Processing\n",
    "\n",
    "1. **For simple JSON extraction:**\n",
    "   - Use `json_tuple` for extracting multiple fields at once\n",
    "   - Use `get_json_object` for extracting specific fields with JSONPath\n",
    "\n",
    "2. **For complex JSON structures:**\n",
    "   - Define explicit schemas with `StructType` and use `from_json`\n",
    "   - Handle nested structures with dot notation\n",
    "   - Use `explode` for arrays\n",
    "\n",
    "3. **Performance considerations:**\n",
    "   - Schema inference is convenient but can be slow on large datasets\n",
    "   - Define explicit schemas for production workloads\n",
    "   - Use appropriate data types to avoid type conversions\n",
    "\n",
    "4. **File handling:**\n",
    "   - Use `multiLine` option for multi-line JSON files\n",
    "   - Consider partitioning for large JSON datasets\n",
    "   - Use Parquet instead of JSON for better performance in analytical workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
