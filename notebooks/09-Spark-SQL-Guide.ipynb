{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL: Comprehensive Guide with Best Practices\n",
    "\n",
    "This notebook covers the essentials of Spark SQL, including:\n",
    "\n",
    "1. **SQL Basics**: Creating tables/views and running queries\n",
    "2. **User-Defined Functions (UDFs)** in SQL context\n",
    "3. **Advanced SQL Features**: Window functions, complex types, and SQL optimization\n",
    "4. **Performance Best Practices**: Tips for efficient Spark SQL usage\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (19.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 08:54:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Spark Session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, udf, lit, when, avg, sum, max, min, count, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType, BooleanType\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL Tutorial\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Datasets\n",
    "\n",
    "Before diving into SQL, let's create some sample datasets to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+---------+\n",
      "|emp_id|first_name|last_name| department|salary|years_exp|\n",
      "+------+----------+---------+-----------+------+---------+\n",
      "|     1|      John|      Doe|Engineering| 80000|        5|\n",
      "|     2|      Jane|    Smith|Engineering| 95000|        7|\n",
      "|     3|     Alice|  Johnson|      Sales| 75000|        3|\n",
      "|     4|       Bob|    Brown|      Sales| 68000|        2|\n",
      "|     5|   Charlie|   Miller|  Marketing| 72000|        4|\n",
      "|     6|      Dave|   Wilson|Engineering|105000|        9|\n",
      "|     7|       Eve|    Davis|         HR| 65000|        5|\n",
      "|     8|     Frank|    Jones|  Marketing| 78000|        6|\n",
      "|     9|     Grace|   Taylor|Engineering| 92000|        6|\n",
      "|    10|     Helen|    Moore|      Sales| 81000|        4|\n",
      "+------+----------+---------+-----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for employees\n",
    "employee_data = [\n",
    "    (1, \"John\", \"Doe\", \"Engineering\", 80000, 5),\n",
    "    (2, \"Jane\", \"Smith\", \"Engineering\", 95000, 7),\n",
    "    (3, \"Alice\", \"Johnson\", \"Sales\", 75000, 3),\n",
    "    (4, \"Bob\", \"Brown\", \"Sales\", 68000, 2),\n",
    "    (5, \"Charlie\", \"Miller\", \"Marketing\", 72000, 4),\n",
    "    (6, \"Dave\", \"Wilson\", \"Engineering\", 105000, 9),\n",
    "    (7, \"Eve\", \"Davis\", \"HR\", 65000, 5),\n",
    "    (8, \"Frank\", \"Jones\", \"Marketing\", 78000, 6),\n",
    "    (9, \"Grace\", \"Taylor\", \"Engineering\", 92000, 6),\n",
    "    (10, \"Helen\", \"Moore\", \"Sales\", 81000, 4)\n",
    "]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"years_exp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "employees_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------+--------------+\n",
      "|  dept_name|     location|       manager|employee_count|\n",
      "+-----------+-------------+--------------+--------------+\n",
      "|Engineering|San Francisco|    John Smith|            35|\n",
      "|      Sales|     New York|  Mary Johnson|            28|\n",
      "|  Marketing|      Chicago|   James Brown|            22|\n",
      "|         HR|       Boston|Patricia Davis|            15|\n",
      "|    Finance|     San Jose| Robert Wilson|            18|\n",
      "+-----------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for departments\n",
    "department_data = [\n",
    "    (\"Engineering\", \"San Francisco\", \"John Smith\", 35),\n",
    "    (\"Sales\", \"New York\", \"Mary Johnson\", 28),\n",
    "    (\"Marketing\", \"Chicago\", \"James Brown\", 22),\n",
    "    (\"HR\", \"Boston\", \"Patricia Davis\", 15),\n",
    "    (\"Finance\", \"San Jose\", \"Robert Wilson\", 18)\n",
    "]\n",
    "\n",
    "department_schema = StructType([\n",
    "    StructField(\"dept_name\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"manager\", StringType(), True),\n",
    "    StructField(\"employee_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "departments_df = spark.createDataFrame(department_data, department_schema)\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------+--------------------------------------------------------+\n",
      "|project_id|name              |team_members |details                                                 |\n",
      "+----------+------------------+-------------+--------------------------------------------------------+\n",
      "|1         |Mobile App        |[1, 2, 6, 9] |{priority -> high, status -> active, budget -> 250000}  |\n",
      "|2         |Website Redesign  |[3, 5, 8]    |{priority -> medium, status -> active, budget -> 175000}|\n",
      "|3         |Database Migration|[2, 6]       |{priority -> high, status -> planning, budget -> 300000}|\n",
      "|4         |API Integration   |[1, 4, 7]    |{priority -> low, status -> completed, budget -> 120000}|\n",
      "|5         |Data Analytics    |[5, 8, 9, 10]|{priority -> medium, status -> active, budget -> 200000}|\n",
      "+----------+------------------+-------------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a more complex dataset with arrays and maps\n",
    "projects_data = [\n",
    "    (1, \"Mobile App\", [1, 2, 6, 9], {\"budget\": 250000, \"status\": \"active\", \"priority\": \"high\"}),\n",
    "    (2, \"Website Redesign\", [3, 5, 8], {\"budget\": 175000, \"status\": \"active\", \"priority\": \"medium\"}),\n",
    "    (3, \"Database Migration\", [2, 6], {\"budget\": 300000, \"status\": \"planning\", \"priority\": \"high\"}),\n",
    "    (4, \"API Integration\", [1, 4, 7], {\"budget\": 120000, \"status\": \"completed\", \"priority\": \"low\"}),\n",
    "    (5, \"Data Analytics\", [5, 8, 9, 10], {\"budget\": 200000, \"status\": \"active\", \"priority\": \"medium\"})\n",
    "]\n",
    "\n",
    "projects_schema = StructType([\n",
    "    StructField(\"project_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"team_members\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"details\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "projects_df = spark.createDataFrame(projects_data, projects_schema)\n",
    "projects_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark SQL Basics\n",
    "\n",
    "Now that we have our data, let's explore different ways to work with Spark SQL. Spark SQL provides a SQL interface to interact with structured data in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating Temporary Views\n",
    "\n",
    "To query data using SQL, we first need to create temporary views from our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:\n",
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|         |departments|       true|\n",
      "|         |  employees|       true|\n",
      "|         |   projects|       true|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary views from DataFrames\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "projects_df.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "# List all tables in the current session\n",
    "print(\"Available tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Basic SQL Queries\n",
    "\n",
    "Let's start with some basic SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+\n",
      "|emp_id|first_name|last_name| department|salary|\n",
      "+------+----------+---------+-----------+------+\n",
      "|     6|      Dave|   Wilson|Engineering|105000|\n",
      "|     2|      Jane|    Smith|Engineering| 95000|\n",
      "|     9|     Grace|   Taylor|Engineering| 92000|\n",
      "|    10|     Helen|    Moore|      Sales| 81000|\n",
      "+------+----------+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple SELECT query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name, \n",
    "    last_name, \n",
    "    department, \n",
    "    salary\n",
    "FROM \n",
    "    employees\n",
    "WHERE \n",
    "    salary > 80000\n",
    "ORDER BY \n",
    "    salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+----------+----------+------------+\n",
      "| department|employee_count|       avg_salary|max_salary|min_salary|total_salary|\n",
      "+-----------+--------------+-----------------+----------+----------+------------+\n",
      "|Engineering|             4|          93000.0|    105000|     80000|      372000|\n",
      "|  Marketing|             2|          75000.0|     78000|     72000|      150000|\n",
      "|      Sales|             3|74666.66666666667|     81000|     68000|      224000|\n",
      "|         HR|             1|          65000.0|     65000|     65000|       65000|\n",
      "+-----------+--------------+-----------------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    department, \n",
    "    COUNT(*) as employee_count, \n",
    "    AVG(salary) as avg_salary, \n",
    "    MAX(salary) as max_salary,\n",
    "    MIN(salary) as min_salary,\n",
    "    SUM(salary) as total_salary\n",
    "FROM \n",
    "    employees\n",
    "GROUP BY \n",
    "    department\n",
    "ORDER BY \n",
    "    avg_salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+-------------+--------------+\n",
      "|emp_id|first_name|last_name| department|     location|  dept_manager|\n",
      "+------+----------+---------+-----------+-------------+--------------+\n",
      "|     1|      John|      Doe|Engineering|San Francisco|    John Smith|\n",
      "|     2|      Jane|    Smith|Engineering|San Francisco|    John Smith|\n",
      "|     3|     Alice|  Johnson|      Sales|     New York|  Mary Johnson|\n",
      "|     4|       Bob|    Brown|      Sales|     New York|  Mary Johnson|\n",
      "|     5|   Charlie|   Miller|  Marketing|      Chicago|   James Brown|\n",
      "|     6|      Dave|   Wilson|Engineering|San Francisco|    John Smith|\n",
      "|     7|       Eve|    Davis|         HR|       Boston|Patricia Davis|\n",
      "|     8|     Frank|    Jones|  Marketing|      Chicago|   James Brown|\n",
      "|     9|     Grace|   Taylor|Engineering|San Francisco|    John Smith|\n",
      "|    10|     Helen|    Moore|      Sales|     New York|  Mary Johnson|\n",
      "+------+----------+---------+-----------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOIN query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    e.emp_id, \n",
    "    e.first_name, \n",
    "    e.last_name, \n",
    "    e.department, \n",
    "    d.location,\n",
    "    d.manager as dept_manager\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "ORDER BY \n",
    "    e.emp_id\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Working with Complex Types\n",
    "\n",
    "Spark SQL can handle complex data types like arrays and maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-------------+--------------+\n",
      "|project_id|name              |team_size|member_ids   |has_employee_1|\n",
      "+----------+------------------+---------+-------------+--------------+\n",
      "|5         |Data Analytics    |4        |[5, 8, 9, 10]|false         |\n",
      "|1         |Mobile App        |4        |[1, 2, 6, 9] |true          |\n",
      "|2         |Website Redesign  |3        |[3, 5, 8]    |false         |\n",
      "|4         |API Integration   |3        |[1, 4, 7]    |true          |\n",
      "|3         |Database Migration|2        |[2, 6]       |false         |\n",
      "+----------+------------------+---------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with array operations\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    project_id, \n",
    "    name, \n",
    "    size(team_members) as team_size,\n",
    "    team_members as member_ids,\n",
    "    array_contains(team_members, 1) as has_employee_1\n",
    "FROM \n",
    "    projects\n",
    "ORDER BY \n",
    "    team_size DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------+------+--------+\n",
      "|project_id|            name|budget|status|priority|\n",
      "+----------+----------------+------+------+--------+\n",
      "|         1|      Mobile App|250000|active|    high|\n",
      "|         5|  Data Analytics|200000|active|  medium|\n",
      "|         2|Website Redesign|175000|active|  medium|\n",
      "+----------+----------------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with map operations\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    project_id, \n",
    "    name, \n",
    "    details['budget'] as budget,\n",
    "    details['status'] as status,\n",
    "    details['priority'] as priority\n",
    "FROM \n",
    "    projects\n",
    "WHERE \n",
    "    details['status'] = 'active'\n",
    "ORDER BY \n",
    "    details['budget'] DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Subqueries and Common Table Expressions (CTEs)\n",
    "\n",
    "Spark SQL supports advanced SQL features like subqueries and CTEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+\n",
      "|emp_id|first_name|last_name| department|salary|\n",
      "+------+----------+---------+-----------+------+\n",
      "|     6|      Dave|   Wilson|Engineering|105000|\n",
      "|     2|      Jane|    Smith|Engineering| 95000|\n",
      "|     9|     Grace|   Taylor|Engineering| 92000|\n",
      "+------+----------+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery example\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name, \n",
    "    last_name, \n",
    "    department, \n",
    "    salary\n",
    "FROM \n",
    "    employees\n",
    "WHERE \n",
    "    salary > (\n",
    "        SELECT AVG(salary) \n",
    "        FROM employees\n",
    "    )\n",
    "ORDER BY \n",
    "    salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+------+---------------+--------------+\n",
      "|first_name|last_name| department|salary|avg_dept_salary|max_experience|\n",
      "+----------+---------+-----------+------+---------------+--------------+\n",
      "|      Dave|   Wilson|Engineering|105000|        93000.0|             9|\n",
      "|      Jane|    Smith|Engineering| 95000|        93000.0|             9|\n",
      "|     Grace|   Taylor|Engineering| 92000|        93000.0|             9|\n",
      "|      John|      Doe|Engineering| 80000|        93000.0|             9|\n",
      "+----------+---------+-----------+------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CTE (Common Table Expression) example\n",
    "query = \"\"\"\n",
    "WITH dept_stats AS (\n",
    "    SELECT \n",
    "        department, \n",
    "        AVG(salary) as avg_dept_salary,\n",
    "        MAX(years_exp) as max_experience\n",
    "    FROM \n",
    "        employees\n",
    "    GROUP BY \n",
    "        department\n",
    "),\n",
    "high_paid_departments AS (\n",
    "    SELECT \n",
    "        department\n",
    "    FROM \n",
    "        dept_stats\n",
    "    WHERE \n",
    "        avg_dept_salary > 80000\n",
    ")\n",
    "SELECT \n",
    "    e.first_name,\n",
    "    e.last_name,\n",
    "    e.department,\n",
    "    e.salary,\n",
    "    s.avg_dept_salary,\n",
    "    s.max_experience\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    dept_stats s\n",
    "ON \n",
    "    e.department = s.department\n",
    "WHERE \n",
    "    e.department IN (SELECT department FROM high_paid_departments)\n",
    "ORDER BY \n",
    "    e.salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Window Functions\n",
    "\n",
    "Window functions perform calculations across related rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 08:54:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/18 08:54:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/18 08:54:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/18 08:54:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+----------------+-------------------+-------------------+------------------+\n",
      "|emp_id|first_name|last_name| department|salary|dept_salary_rank|overall_salary_rank| diff_from_dept_avg| pct_of_dept_total|\n",
      "+------+----------+---------+-----------+------+----------------+-------------------+-------------------+------------------+\n",
      "|     6|      Dave|   Wilson|Engineering|105000|               1|                  1|            12000.0|28.225806451612907|\n",
      "|     2|      Jane|    Smith|Engineering| 95000|               2|                  2|             2000.0|25.537634408602152|\n",
      "|     9|     Grace|   Taylor|Engineering| 92000|               3|                  3|            -1000.0|24.731182795698924|\n",
      "|     1|      John|      Doe|Engineering| 80000|               4|                  5|           -13000.0| 21.50537634408602|\n",
      "|     7|       Eve|    Davis|         HR| 65000|               1|                 10|                0.0|             100.0|\n",
      "|     8|     Frank|    Jones|  Marketing| 78000|               1|                  6|             3000.0|              52.0|\n",
      "|     5|   Charlie|   Miller|  Marketing| 72000|               2|                  8|            -3000.0|              48.0|\n",
      "|    10|     Helen|    Moore|      Sales| 81000|               1|                  4| 6333.3333333333285|36.160714285714285|\n",
      "|     3|     Alice|  Johnson|      Sales| 75000|               2|                  7|  333.3333333333285|33.482142857142854|\n",
      "|     4|       Bob|    Brown|      Sales| 68000|               3|                  9|-6666.6666666666715|30.357142857142854|\n",
      "+------+----------+---------+-----------+------+----------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window function example\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    department,\n",
    "    salary,\n",
    "    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank,\n",
    "    DENSE_RANK() OVER (ORDER BY salary DESC) as overall_salary_rank,\n",
    "    salary - AVG(salary) OVER (PARTITION BY department) as diff_from_dept_avg,\n",
    "    salary / SUM(salary) OVER (PARTITION BY department) * 100 as pct_of_dept_total\n",
    "FROM \n",
    "    employees\n",
    "ORDER BY \n",
    "    department, \n",
    "    dept_salary_rank\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User-Defined Functions (UDFs) in Spark SQL\n",
    "\n",
    "User-Defined Functions allow you to extend SQL with custom logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating and Using SQL UDFs\n",
    "\n",
    "Let's create some UDFs and use them in SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------+-------+\n",
      "|emp_id|first_name|last_name|salary|years_exp|  bonus|\n",
      "+------+----------+---------+------+---------+-------+\n",
      "|    10|     Helen|    Moore| 81000|        4| 8100.0|\n",
      "|     1|      John|      Doe| 80000|        5| 8000.0|\n",
      "|     3|     Alice|  Johnson| 75000|        3| 7500.0|\n",
      "|     5|   Charlie|   Miller| 72000|        4| 7200.0|\n",
      "|     4|       Bob|    Brown| 68000|        2| 6800.0|\n",
      "|     7|       Eve|    Davis| 65000|        5| 6500.0|\n",
      "|     6|      Dave|   Wilson|105000|        9|15750.0|\n",
      "|     2|      Jane|    Smith| 95000|        7|14250.0|\n",
      "|     9|     Grace|   Taylor| 92000|        6|13800.0|\n",
      "|     8|     Frank|    Jones| 78000|        6|11700.0|\n",
      "+------+----------+---------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "def calculate_bonus(salary, years, rate):\n",
    "    # Convert everything to float to avoid type issues\n",
    "    # This is the simplest solution though it may lose some precision\n",
    "    salary_float = float(salary)\n",
    "    years_float = float(years)\n",
    "    rate_float = float(rate)\n",
    "    \n",
    "    if years_float > 5:\n",
    "        return salary_float * rate_float * 1.5\n",
    "    else:\n",
    "        return salary_float * rate_float\n",
    "\n",
    "# Register the UDF\n",
    "spark.udf.register(\"calculate_bonus\", calculate_bonus)\n",
    "\n",
    "# Now use the UDF in a SQL query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name,\n",
    "    last_name,\n",
    "    salary,\n",
    "    years_exp,\n",
    "    calculate_bonus(salary, years_exp, 0.1) AS bonus\n",
    "FROM \n",
    "    employees\n",
    "ORDER BY \n",
    "    bonus DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Register a UDF using SQL syntax\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# CREATE TEMPORARY FUNCTION calculate_bonus\n",
    "# AS (salary, years, rate) -> \n",
    "#     CASE\n",
    "#       WHEN years > 5 THEN salary * rate * 1.5\n",
    "#       ELSE salary * rate\n",
    "#     END\n",
    "# \"\"\")\n",
    "\n",
    "# # Use the UDF in a SQL query\n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     emp_id, \n",
    "#     first_name, \n",
    "#     last_name, \n",
    "#     salary, \n",
    "#     years_exp,\n",
    "#     calculate_bonus(salary, years_exp, 0.1) as bonus\n",
    "# FROM \n",
    "#     employees\n",
    "# ORDER BY \n",
    "#     bonus DESC\n",
    "# \"\"\"\n",
    "\n",
    "# spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+--------------+----------------------+\n",
      "|emp_id|first_name|last_name|name          |formal_name           |\n",
      "+------+----------+---------+--------------+----------------------+\n",
      "|1     |John      |Doe      |John Doe      |Mr./Ms. John Doe      |\n",
      "|2     |Jane      |Smith    |Jane Smith    |Mr./Ms. Jane Smith    |\n",
      "|3     |Alice     |Johnson  |Alice Johnson |Mr./Ms. Alice Johnson |\n",
      "|4     |Bob       |Brown    |Bob Brown     |Mr./Ms. Bob Brown     |\n",
      "|5     |Charlie   |Miller   |Charlie Miller|Mr./Ms. Charlie Miller|\n",
      "|6     |Dave      |Wilson   |Dave Wilson   |Mr./Ms. Dave Wilson   |\n",
      "|7     |Eve       |Davis    |Eve Davis     |Mr./Ms. Eve Davis     |\n",
      "|8     |Frank     |Jones    |Frank Jones   |Mr./Ms. Frank Jones   |\n",
      "|9     |Grace     |Taylor   |Grace Taylor  |Mr./Ms. Grace Taylor  |\n",
      "|10    |Helen     |Moore    |Helen Moore   |Mr./Ms. Helen Moore   |\n",
      "+------+----------+---------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Register a Python function as a UDF\n",
    "# Define Python function\n",
    "def full_name(first_name, last_name, add_title=False):\n",
    "    if add_title:\n",
    "        return f\"Mr./Ms. {first_name} {last_name}\"\n",
    "    else:\n",
    "        return f\"{first_name} {last_name}\"\n",
    "\n",
    "# Register as UDF\n",
    "spark.udf.register(\"full_name\", full_name, StringType())\n",
    "\n",
    "# Use in SQL\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name, \n",
    "    last_name, \n",
    "    full_name(first_name, last_name) as name,\n",
    "    full_name(first_name, last_name, true) as formal_name\n",
    "FROM \n",
    "    employees\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Complex UDFs with Struct Returns\n",
    "\n",
    "UDFs can return complex data types like structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+-------------------------------------------------+----------+-----------+\n",
      "|emp_id|first_name|last_name|department |compensation                                     |total_comp|performance|\n",
      "+------+----------+---------+-----------+-------------------------------------------------+----------+-----------+\n",
      "|6     |Dave      |Wilson   |Engineering|{105000, 14700.000000000002, 119700.0, Excellent}|119700.0  |Excellent  |\n",
      "|2     |Jane      |Smith    |Engineering|{95000, 11400.0, 106400.0, Excellent}            |106400.0  |Excellent  |\n",
      "|9     |Grace     |Taylor   |Engineering|{92000, 10120.0, 102120.0, Excellent}            |102120.0  |Excellent  |\n",
      "|10    |Helen     |Moore    |Sales      |{81000, 7290.0, 88290.0, Good}                   |88290.0   |Good       |\n",
      "|1     |John      |Doe      |Engineering|{80000, 8000.0, 88000.0, Good}                   |88000.0   |Good       |\n",
      "|8     |Frank     |Jones    |Marketing  |{78000, 8580.0, 86580.0, Good}                   |86580.0   |Good       |\n",
      "|3     |Alice     |Johnson  |Sales      |{75000, 6000.0, 81000.0, Good}                   |81000.0   |Good       |\n",
      "|5     |Charlie   |Miller   |Marketing  |{72000, 6480.0, 78480.0, Average}                |78480.0   |Average    |\n",
      "|4     |Bob       |Brown    |Sales      |{68000, 4760.0, 72760.0, Average}                |72760.0   |Average    |\n",
      "|7     |Eve       |Davis    |HR         |{65000, 6500.0, 71500.0, Average}                |71500.0   |Average    |\n",
      "+------+----------+---------+-----------+-------------------------------------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define return schema for our UDF\n",
    "compensation_schema = StructType([\n",
    "    StructField(\"base\", IntegerType(), True),\n",
    "    StructField(\"bonus\", DoubleType(), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"rating\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define Python function\n",
    "def calculate_compensation(salary, years):\n",
    "    # Calculate bonus\n",
    "    bonus_rate = 0.05 + (years * 0.01)  # 5% + 1% per year\n",
    "    bonus = salary * bonus_rate\n",
    "    total = salary + bonus\n",
    "    \n",
    "    # Determine rating\n",
    "    if total > 100000:\n",
    "        rating = \"Excellent\"\n",
    "    elif total > 80000:\n",
    "        rating = \"Good\"\n",
    "    else:\n",
    "        rating = \"Average\"\n",
    "        \n",
    "    return (int(salary), float(bonus), float(total), rating)\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"calculate_compensation\", calculate_compensation, compensation_schema)\n",
    "\n",
    "# Use in SQL\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name, \n",
    "    last_name, \n",
    "    department,\n",
    "    calculate_compensation(salary, years_exp) as compensation,\n",
    "    calculate_compensation(salary, years_exp).total as total_comp,\n",
    "    calculate_compensation(salary, years_exp).rating as performance\n",
    "FROM \n",
    "    employees\n",
    "ORDER BY \n",
    "    total_comp DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pandas UDFs (Vectorized UDFs)\n",
    "\n",
    "Pandas UDFs provide much better performance than regular UDFs by leveraging vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+-------+---------+\n",
      "|emp_id|first_name|last_name| department|salary|    tax|after_tax|\n",
      "+------+----------+---------+-----------+------+-------+---------+\n",
      "|     6|      Dave|   Wilson|Engineering|105000|16500.0|  88500.0|\n",
      "|     2|      Jane|    Smith|Engineering| 95000|14000.0|  81000.0|\n",
      "|     9|     Grace|   Taylor|Engineering| 92000|13400.0|  78600.0|\n",
      "|    10|     Helen|    Moore|      Sales| 81000|11200.0|  69800.0|\n",
      "|     1|      John|      Doe|Engineering| 80000|11000.0|  69000.0|\n",
      "|     8|     Frank|    Jones|  Marketing| 78000|10600.0|  67400.0|\n",
      "|     3|     Alice|  Johnson|      Sales| 75000|10000.0|  65000.0|\n",
      "|     5|   Charlie|   Miller|  Marketing| 72000| 9400.0|  62600.0|\n",
      "|     4|       Bob|    Brown|      Sales| 68000| 8600.0|  59400.0|\n",
      "|     7|       Eve|    Davis|         HR| 65000| 8000.0|  57000.0|\n",
      "+------+----------+---------+-----------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Register a pandas UDF\n",
    "@pandas_udf(DoubleType())\n",
    "def calculate_tax(salary_series: pd.Series) -> pd.Series:\n",
    "    # Apply tax brackets\n",
    "    # - 10% up to 50K\n",
    "    # - 20% from 50K to 100K\n",
    "    # - 30% above 100K\n",
    "    def tax_for_salary(salary):\n",
    "        if salary <= 50000:\n",
    "            return salary * 0.10\n",
    "        elif salary <= 100000:\n",
    "            return 5000 + (salary - 50000) * 0.20\n",
    "        else:\n",
    "            return 5000 + 10000 + (salary - 100000) * 0.30\n",
    "    \n",
    "    return salary_series.apply(tax_for_salary)\n",
    "\n",
    "# Register for SQL use\n",
    "spark.udf.register(\"calculate_tax\", calculate_tax)\n",
    "\n",
    "# Use in SQL\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    emp_id, \n",
    "    first_name, \n",
    "    last_name, \n",
    "    department,\n",
    "    salary,\n",
    "    calculate_tax(salary) as tax,\n",
    "    salary - calculate_tax(salary) as after_tax\n",
    "FROM \n",
    "    employees\n",
    "ORDER BY \n",
    "    tax DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Features and Best Practices\n",
    "\n",
    "This section covers advanced features and best practices for working with Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating and Using Tables\n",
    "\n",
    "Beyond temporary views, you can create more persistent tables in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/18 08:54:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|global_employees|      false|\n",
      "|         |     departments|       true|\n",
      "|         |       employees|       true|\n",
      "|         |        projects|       true|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save data as a Spark table (in the metastore)\n",
    "employees_df.write.saveAsTable(\"global_employees\")\n",
    "\n",
    "# List all tables\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|col_name  |data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|emp_id    |int      |NULL   |\n",
      "|first_name|string   |NULL   |\n",
      "|last_name |string   |NULL   |\n",
      "|department|string   |NULL   |\n",
      "|salary    |int      |NULL   |\n",
      "|years_exp |int      |NULL   |\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe table schema\n",
    "spark.sql(\"DESCRIBE TABLE global_employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Optimizing SQL Queries\n",
    "\n",
    "Let's look at how to optimize SQL queries in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical and Physical Plans:\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['avg_salary DESC NULLS LAST], true\n",
      "+- 'UnresolvedHaving ('AVG('e.salary) > 75000)\n",
      "   +- 'Aggregate ['e.department, 'd.location], ['e.department, 'AVG('e.salary) AS avg_salary#729, 'd.location]\n",
      "      +- 'Filter ('e.salary > 70000)\n",
      "         +- 'Join Inner, ('e.department = 'd.dept_name)\n",
      "            :- 'SubqueryAlias e\n",
      "            :  +- 'UnresolvedRelation [employees], [], false\n",
      "            +- 'SubqueryAlias d\n",
      "               +- 'UnresolvedRelation [departments], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, avg_salary: double, location: string\n",
      "Sort [avg_salary#729 DESC NULLS LAST], true\n",
      "+- Filter (avg_salary#729 > cast(75000 as double))\n",
      "   +- Aggregate [department#3, location#38], [department#3, avg(salary#4) AS avg_salary#729, location#38]\n",
      "      +- Filter (salary#4 > 70000)\n",
      "         +- Join Inner, (department#3 = dept_name#37)\n",
      "            :- SubqueryAlias e\n",
      "            :  +- SubqueryAlias employees\n",
      "            :     +- View (`employees`, [emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5])\n",
      "            :        +- LogicalRDD [emp_id#0, first_name#1, last_name#2, department#3, salary#4, years_exp#5], false\n",
      "            +- SubqueryAlias d\n",
      "               +- SubqueryAlias departments\n",
      "                  +- View (`departments`, [dept_name#37,location#38,manager#39,employee_count#40])\n",
      "                     +- LogicalRDD [dept_name#37, location#38, manager#39, employee_count#40], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [avg_salary#729 DESC NULLS LAST], true\n",
      "+- Filter (isnotnull(avg_salary#729) AND (avg_salary#729 > 75000.0))\n",
      "   +- Aggregate [department#3, location#38], [department#3, avg(salary#4) AS avg_salary#729, location#38]\n",
      "      +- Project [department#3, salary#4, location#38]\n",
      "         +- Join Inner, (department#3 = dept_name#37)\n",
      "            :- Project [department#3, salary#4]\n",
      "            :  +- Filter (isnotnull(salary#4) AND (salary#4 > 70000))\n",
      "            :     +- LogicalRDD [emp_id#0, first_name#1, last_name#2, department#3, salary#4, years_exp#5], false\n",
      "            +- Project [dept_name#37, location#38]\n",
      "               +- LogicalRDD [dept_name#37, location#38, manager#39, employee_count#40], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_salary#729 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_salary#729 DESC NULLS LAST, 4), ENSURE_REQUIREMENTS, [plan_id=1167]\n",
      "      +- Filter (isnotnull(avg_salary#729) AND (avg_salary#729 > 75000.0))\n",
      "         +- HashAggregate(keys=[department#3, location#38], functions=[avg(salary#4)], output=[department#3, avg_salary#729, location#38])\n",
      "            +- HashAggregate(keys=[department#3, location#38], functions=[partial_avg(salary#4)], output=[department#3, location#38, sum#737, count#738L])\n",
      "               +- Project [department#3, salary#4, location#38]\n",
      "                  +- SortMergeJoin [department#3], [dept_name#37], Inner\n",
      "                     :- Sort [department#3 ASC NULLS FIRST], false, 0\n",
      "                     :  +- Exchange hashpartitioning(department#3, 4), ENSURE_REQUIREMENTS, [plan_id=1157]\n",
      "                     :     +- Project [department#3, salary#4]\n",
      "                     :        +- Filter (isnotnull(salary#4) AND (salary#4 > 70000))\n",
      "                     :           +- Scan ExistingRDD[emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5]\n",
      "                     +- Sort [dept_name#37 ASC NULLS FIRST], false, 0\n",
      "                        +- Exchange hashpartitioning(dept_name#37, 4), ENSURE_REQUIREMENTS, [plan_id=1158]\n",
      "                           +- Project [dept_name#37, location#38]\n",
      "                              +- Scan ExistingRDD[dept_name#37,location#38,manager#39,employee_count#40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine query plans to understand optimization\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    e.department, \n",
    "    AVG(e.salary) as avg_salary,\n",
    "    d.location\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "WHERE \n",
    "    e.salary > 70000\n",
    "GROUP BY \n",
    "    e.department, d.location\n",
    "HAVING \n",
    "    AVG(e.salary) > 75000\n",
    "ORDER BY \n",
    "    avg_salary DESC\n",
    "\"\"\"\n",
    "\n",
    "# Get query plan\n",
    "print(\"Logical and Physical Plans:\")\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Plan (should include BroadcastHashJoin):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [emp_id#0, first_name#1, last_name#2, department#3, location#38]\n",
      "   +- BroadcastHashJoin [department#3], [dept_name#37], Inner, BuildRight, false\n",
      "      :- Project [emp_id#0, first_name#1, last_name#2, department#3]\n",
      "      :  +- Scan ExistingRDD[emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1193]\n",
      "         +- Project [dept_name#37, location#38]\n",
      "            +- Scan ExistingRDD[dept_name#37,location#38,manager#39,employee_count#40]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Broadcast join for small tables\n",
    "# Enable automatic broadcast joins for small tables\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # 10MB\n",
    "\n",
    "# Force broadcast with a hint\n",
    "query = \"\"\"\n",
    "SELECT /*+ BROADCAST(d) */ \n",
    "    e.emp_id, \n",
    "    e.first_name, \n",
    "    e.last_name, \n",
    "    e.department, \n",
    "    d.location\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "\"\"\"\n",
    "\n",
    "# Check if broadcast was applied\n",
    "print(\"Physical Plan (should include BroadcastHashJoin):\")\n",
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Comparison: DataFrame vs SQL\n",
    "\n",
    "Let's compare the performance of equivalent operations using DataFrame API vs SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a larger dataset for performance testing\n",
    "large_df = spark.range(0, 1000000) \\\n",
    "    .withColumn(\"random_value\", (col(\"id\") * 12.345) % 100) \\\n",
    "    .withColumn(\"group\", (col(\"id\") % 10).cast(\"integer\")) \\\n",
    "    .withColumn(\"subgroup\", (col(\"id\") % 100).cast(\"integer\"))\n",
    "\n",
    "large_df.createOrReplaceTempView(\"large_table\")\n",
    "\n",
    "# Warm up the JVM\n",
    "large_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: SQL: 0.317s, DataFrame: 0.117s\n",
      "Run 2: SQL: 0.100s, DataFrame: 0.105s\n",
      "Run 3: SQL: 0.096s, DataFrame: 0.096s\n",
      "\n",
      "Average: SQL: 0.171s, DataFrame: 0.106s\n"
     ]
    }
   ],
   "source": [
    "# Test with SQL\n",
    "def test_sql_performance():\n",
    "    start_time = time.time()\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            group, \n",
    "            subgroup, \n",
    "            COUNT(*) as count, \n",
    "            AVG(random_value) as avg_val,\n",
    "            MAX(random_value) as max_val\n",
    "        FROM \n",
    "            large_table\n",
    "        WHERE \n",
    "            random_value > 50\n",
    "        GROUP BY \n",
    "            group, subgroup\n",
    "        HAVING \n",
    "            COUNT(*) > 5\n",
    "        ORDER BY \n",
    "            group, subgroup\n",
    "    \"\"\")\n",
    "    result.collect()  # Force execution\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Test with DataFrame API\n",
    "def test_df_performance():\n",
    "    start_time = time.time()\n",
    "    result = large_df \\\n",
    "        .filter(col(\"random_value\") > 50) \\\n",
    "        .groupBy(\"group\", \"subgroup\") \\\n",
    "        .agg( \\\n",
    "            count(\"*\").alias(\"count\"), \\\n",
    "            avg(\"random_value\").alias(\"avg_val\"), \\\n",
    "            max(\"random_value\").alias(\"max_val\") \\\n",
    "        ) \\\n",
    "        .filter(col(\"count\") > 5) \\\n",
    "        .orderBy(\"group\", \"subgroup\")\n",
    "    result.collect()  # Force execution\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Run multiple times for more accurate comparison\n",
    "sql_times = []\n",
    "df_times = []\n",
    "\n",
    "for i in range(3):\n",
    "    sql_time = test_sql_performance()\n",
    "    df_time = test_df_performance()\n",
    "    sql_times.append(sql_time)\n",
    "    df_times.append(df_time)\n",
    "    print(f\"Run {i+1}: SQL: {sql_time:.3f}s, DataFrame: {df_time:.3f}s\")\n",
    "\n",
    "import builtins  # Import Python's built-in functions\n",
    "print(f\"\\nAverage: SQL: {builtins.sum(sql_times)/len(sql_times):.3f}s, DataFrame: {builtins.sum(df_times)/len(df_times):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 SQL Best Practices\n",
    "\n",
    "Here are some best practices for using Spark SQL effectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Filter Early\n",
    "\n",
    "Apply filters as early as possible to reduce data size before expensive operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good query plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [first_name#1, last_name#2, location#38]\n",
      "   +- SortMergeJoin [department#3], [dept_name#37], Inner\n",
      "      :- Sort [department#3 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(department#3, 4), ENSURE_REQUIREMENTS, [plan_id=1872]\n",
      "      :     +- Project [first_name#1, last_name#2, department#3]\n",
      "      :        +- Filter (isnotnull(salary#4) AND (salary#4 > 80000))\n",
      "      :           +- Scan ExistingRDD[emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5]\n",
      "      +- Sort [dept_name#37 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(dept_name#37, 4), ENSURE_REQUIREMENTS, [plan_id=1873]\n",
      "            +- Project [dept_name#37, location#38]\n",
      "               +- Scan ExistingRDD[dept_name#37,location#38,manager#39,employee_count#40]\n",
      "\n",
      "\n",
      "\n",
      "Bad query plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [first_name#1, last_name#2, location#38]\n",
      "   +- SortMergeJoin [department#3], [dept_name#37], Inner\n",
      "      :- Sort [department#3 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(department#3, 4), ENSURE_REQUIREMENTS, [plan_id=1907]\n",
      "      :     +- Project [first_name#1, last_name#2, department#3]\n",
      "      :        +- Filter (isnotnull(salary#4) AND (salary#4 > 80000))\n",
      "      :           +- Scan ExistingRDD[emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5]\n",
      "      +- Sort [dept_name#37 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(dept_name#37, 4), ENSURE_REQUIREMENTS, [plan_id=1908]\n",
      "            +- Project [dept_name#37, location#38]\n",
      "               +- Scan ExistingRDD[dept_name#37,location#38,manager#39,employee_count#40]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Good: Filter before join\n",
    "query_good = \"\"\"\n",
    "SELECT e.first_name, e.last_name, d.location\n",
    "FROM \n",
    "    (SELECT * FROM employees WHERE salary > 80000) e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "\"\"\"\n",
    "\n",
    "# Bad: Filter after join\n",
    "query_bad = \"\"\"\n",
    "SELECT e.first_name, e.last_name, d.location\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "WHERE \n",
    "    e.salary > 80000\n",
    "\"\"\"\n",
    "\n",
    "# Compare execution plans\n",
    "print(\"Good query plan:\")\n",
    "spark.sql(query_good).explain()\n",
    "\n",
    "print(\"\\nBad query plan:\")\n",
    "spark.sql(query_bad).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use Appropriate Join Strategies\n",
    "\n",
    "- Use broadcast joins for small tables\n",
    "- Be mindful of join types (inner, left, right, full)\n",
    "- Avoid cartesian products (cross joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [emp_id#0, first_name#1, last_name#2, location#38]\n",
      "   +- BroadcastHashJoin [department#3], [dept_name#37], Inner, BuildRight, false\n",
      "      :- Project [emp_id#0, first_name#1, last_name#2, department#3]\n",
      "      :  +- Scan ExistingRDD[emp_id#0,first_name#1,last_name#2,department#3,salary#4,years_exp#5]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1937]\n",
      "         +- Project [dept_name#37, location#38]\n",
      "            +- Scan ExistingRDD[dept_name#37,location#38,manager#39,employee_count#40]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use broadcast hint for small tables\n",
    "query = \"\"\"\n",
    "SELECT /*+ BROADCAST(d) */ \n",
    "    e.emp_id, e.first_name, e.last_name, d.location\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Favor Standard SQL Functions Over UDFs When Possible\n",
    "\n",
    "Built-in functions are optimized and faster than custom UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+-----------+\n",
      "|first_name|last_name|     full_name|salary_tier|\n",
      "+----------+---------+--------------+-----------+\n",
      "|      John|      Doe|      John Doe|     Medium|\n",
      "|      Jane|    Smith|    Jane Smith|       High|\n",
      "|     Alice|  Johnson| Alice Johnson|     Medium|\n",
      "|       Bob|    Brown|     Bob Brown|        Low|\n",
      "|   Charlie|   Miller|Charlie Miller|     Medium|\n",
      "|      Dave|   Wilson|   Dave Wilson|       High|\n",
      "|       Eve|    Davis|     Eve Davis|        Low|\n",
      "|     Frank|    Jones|   Frank Jones|     Medium|\n",
      "|     Grace|   Taylor|  Grace Taylor|       High|\n",
      "|     Helen|    Moore|   Helen Moore|     Medium|\n",
      "+----------+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using built-in functions\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    first_name, \n",
    "    last_name,\n",
    "    CONCAT(first_name, ' ', last_name) AS full_name,\n",
    "    CASE \n",
    "        WHEN salary > 90000 THEN 'High'\n",
    "        WHEN salary > 70000 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END AS salary_tier\n",
    "FROM \n",
    "    employees\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use Persistent Tables for Frequently Accessed Data\n",
    "\n",
    "Save frequently used DataFrames as tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+-----------------+----------+----------+------------+--------------+\n",
      "| department|     location|emp_count|       avg_salary|max_salary|min_salary|total_salary|avg_experience|\n",
      "+-----------+-------------+---------+-----------------+----------+----------+------------+--------------+\n",
      "|Engineering|San Francisco|        4|          93000.0|    105000|     80000|      372000|          6.75|\n",
      "|      Sales|     New York|        3|74666.66666666667|     81000|     68000|      224000|           3.0|\n",
      "|  Marketing|      Chicago|        2|          75000.0|     78000|     72000|      150000|           5.0|\n",
      "|         HR|       Boston|        1|          65000.0|     65000|     65000|       65000|           5.0|\n",
      "+-----------+-------------+---------+-----------------+----------+----------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Use DataFrame API\n",
    "dept_stats_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    e.department, \n",
    "    d.location,\n",
    "    COUNT(*) AS emp_count,\n",
    "    AVG(salary) AS avg_salary,\n",
    "    MAX(salary) AS max_salary,\n",
    "    MIN(salary) AS min_salary,\n",
    "    SUM(salary) AS total_salary,\n",
    "    AVG(years_exp) AS avg_experience\n",
    "FROM \n",
    "    employees e\n",
    "JOIN \n",
    "    departments d\n",
    "ON \n",
    "    e.department = d.dept_name\n",
    "GROUP BY \n",
    "    e.department, d.location\n",
    "\"\"\")\n",
    "\n",
    "# Save as table (for persistence)\n",
    "dept_stats_df.write.mode(\"overwrite\").saveAsTable(\"dept_statistics\")\n",
    "\n",
    "# Now use the table\n",
    "spark.sql(\"SELECT * FROM dept_statistics\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Leverage Spark SQL Configurations\n",
    "\n",
    "Tune Spark SQL with appropriate configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current SQL Configurations:\n",
      "spark.sql.shuffle.partitions: 4\n",
      "spark.sql.autoBroadcastJoinThreshold: 10485760\n",
      "spark.sql.adaptive.enabled: true\n",
      "spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "spark.sql.optimizer.maxIterations: 100\n",
      "\n",
      "Setting optimized values...\n",
      "\n",
      "Verify new configurations:\n",
      "spark.sql.shuffle.partitions: 8\n",
      "spark.sql.autoBroadcastJoinThreshold: 10485760\n",
      "spark.sql.adaptive.enabled: true\n",
      "spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "spark.sql.optimizer.maxIterations: 100\n"
     ]
    }
   ],
   "source": [
    "# Show current SQL configurations\n",
    "print(\"Current SQL Configurations:\")\n",
    "configs = [\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.sql.optimizer.maxIterations\"\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    try:\n",
    "        value = spark.conf.get(config)\n",
    "        print(f\"{config}: {value}\")\n",
    "    except:\n",
    "        print(f\"{config}: Not set\")\n",
    "\n",
    "# Set some optimized values\n",
    "print(\"\\nSetting optimized values...\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "# Verify changes\n",
    "print(\"\\nVerify new configurations:\")\n",
    "for config in configs:\n",
    "    try:\n",
    "        value = spark.conf.get(config)\n",
    "        print(f\"{config}: {value}\")\n",
    "    except:\n",
    "        print(f\"{config}: Not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we've covered a wide range of Spark SQL features and best practices:\n",
    "\n",
    "1. **Basic SQL Operations**: Queries, joins, aggregations, and complex data types\n",
    "2. **Advanced SQL Features**: Subqueries, CTEs, and window functions\n",
    "3. **User-Defined Functions**: Different types of UDFs and when to use them\n",
    "4. **Performance Optimization**: Tips and tricks for efficient Spark SQL usage\n",
    "\n",
    "Remember these key takeaways:\n",
    "\n",
    "- Use Spark SQL when working with structured data and when you need SQL-like operations\n",
    "- Prefer built-in functions over UDFs when possible for better performance\n",
    "- Apply filters early and use appropriate join strategies\n",
    "- Leverage Spark's SQL optimizer by understanding query plans\n",
    "- Choose the right persistence strategy for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# Clean up resources\n",
    "spark.sql(\"DROP TABLE IF EXISTS global_employees\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS dept_statistics\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
