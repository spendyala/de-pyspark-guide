{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Plan Analysis and Query Optimization\n",
    "\n",
    "This notebook provides a comprehensive guide to analyzing Spark's logical and physical execution plans and using those insights to optimize your Spark SQL queries and DataFrame operations.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Spark Execution Plans\n",
    "2. Setting Up Our Environment\n",
    "3. Key Optimization Areas in Spark Plans\n",
    "   - Scan Operations and Table Statistics\n",
    "   - Filter Pushdown\n",
    "   - Join Strategies\n",
    "   - Partition Pruning\n",
    "   - Shuffle Operations\n",
    "   - Data Skew\n",
    "   - Whole-Stage Codegen\n",
    "   - Caching\n",
    "4. End-to-End Optimization Example\n",
    "5. Best Practices\n",
    "\n",
    "Let's get started by understanding what Spark execution plans are and why they matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Spark Execution Plans\n",
    "\n",
    "Spark uses a query optimizer called Catalyst to transform your high-level DataFrame operations or SQL queries into an efficient execution plan. This process involves several phases:\n",
    "\n",
    "1. **Unresolved Logical Plan**: Initial representation of your query\n",
    "2. **Resolved Logical Plan**: Column and table references are resolved\n",
    "3. **Optimized Logical Plan**: Catalyst applies rule-based optimizations\n",
    "4. **Physical Plan**: Converts logical plan to actual execution strategy\n",
    "5. **Selected Physical Plan**: The most efficient execution plan is chosen\n",
    "6. **Executed Plan**: The final plan after adaptations during runtime\n",
    "\n",
    "Understanding these plans is key to optimizing Spark performance. The `explain()` method is our primary tool for examining these plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Environment\n",
    "\n",
    "Let's start by creating a Spark session and some sample data for our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 09:25:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Session initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34790)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum, max, min, lit, concat, broadcast, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Plan Analysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 09:25:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/18 09:25:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/18 09:25:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/18 09:25:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/18 09:25:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/04/18 09:25:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/18 09:25:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/18 09:25:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/18 09:25:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/18 09:25:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample datasets with 1000 employees and 5 departments\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"dept_id\", IntegerType(), False),\n",
    "    StructField(\"salary\", DoubleType(), False),\n",
    "    StructField(\"hire_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Department schema\n",
    "department_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"department_name\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"budget\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Generate sample data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate departments\n",
    "dept_data = [\n",
    "    (1, \"Engineering\", \"San Francisco\", 1000000.0),\n",
    "    (2, \"Sales\", \"New York\", 800000.0),\n",
    "    (3, \"Marketing\", \"Chicago\", 600000.0),\n",
    "    (4, \"HR\", \"Seattle\", 400000.0),\n",
    "    (5, \"Finance\", \"Boston\", 750000.0)\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(dept_data, department_schema)\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Generate employees\n",
    "employee_data = []\n",
    "names = [\"John\", \"Emma\", \"Michael\", \"Sophia\", \"James\", \"Olivia\", \"William\", \"Ava\", \"Alexander\", \"Mia\"]\n",
    "surnames = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\", \"Garcia\", \"Rodriguez\", \"Wilson\"]\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "delta = (end_date - start_date).days\n",
    "\n",
    "for i in range(1, 1001):  # Generate 1000 employees\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    dept_id = random.randint(1, 5)\n",
    "    salary = round(random.uniform(50000, 150000), 2)\n",
    "    random_days = random.randint(0, delta)\n",
    "    hire_date = start_date + timedelta(days=random_days)\n",
    "    employee_data.append((i, name, dept_id, salary, hire_date))\n",
    "\n",
    "employees_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Also create a partitioned version of our employees table\n",
    "employees_df.write.mode(\"overwrite\").partitionBy(\"dept_id\").saveAsTable(\"partitioned_employees\")\n",
    "\n",
    "print(f\"Created sample datasets with {employees_df.count()} employees and {departments_df.count()} departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Optimization Areas in Spark Plans\n",
    "\n",
    "Now let's examine the key areas we need to look at in Spark plans for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Scan Operations and Table Statistics\n",
    "\n",
    "Scan operations determine how Spark reads data from sources. Inefficient scans can severely impact performance.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- Full table scans vs. filtered scans\n",
    "- Reading too many columns\n",
    "- Missing statistics\n",
    "- File format efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full table scan with all columns:\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Analyzing a simple scan operation\n",
    "query1 = employees_df.select(\"*\")\n",
    "print(\"Full table scan with all columns:\")\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column pruning in action:\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#8, name#9, salary#11]\n",
      "+- *(1) Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: More efficient scan with column pruning\n",
    "query2 = employees_df.select(\"id\", \"name\", \"salary\")\n",
    "print(\"Column pruning in action:\")\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Statistics\n",
    "\n",
    "Statistics help the Spark optimizer make better decisions about join strategies, broadcast size limits, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create permanent tables instead of temp views\n",
    "# # if you want to create permanent tables\n",
    "# employees_df.write.saveAsTable(\"employees_table\")\n",
    "# departments_df.write.saveAsTable(\"departments_table\")\n",
    "\n",
    "# # Now you can analyze them\n",
    "# spark.sql(\"ANALYZE TABLE employees_table COMPUTE STATISTICS\")\n",
    "# spark.sql(\"ANALYZE TABLE departments_table COMPUTE STATISTICS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Statistics for partitioned_employees:\n",
      "+----------+----------------------+-------+\n",
      "|col_name  |data_type             |comment|\n",
      "+----------+----------------------+-------+\n",
      "|Statistics|99624 bytes, 1000 rows|       |\n",
      "+----------+----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can analyze the partitioned table instead, which is permanent\n",
    "spark.sql(\"ANALYZE TABLE partitioned_employees COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE partitioned_employees COMPUTE STATISTICS FOR COLUMNS id, name, salary\")\n",
    "\n",
    "# Check statistics\n",
    "print(\"Table Statistics for partitioned_employees:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED partitioned_employees\").filter(col(\"col_name\").like(\"%Statistics%\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Scan Optimization:\n",
    "\n",
    "1. **Select only necessary columns**: Reduces I/O and memory usage\n",
    "2. **Use appropriate file formats**: Parquet/ORC > CSV/JSON for analytical workloads\n",
    "3. **Compute and maintain statistics**: For better query planning\n",
    "4. **Use partitioning**: For large tables to enable partition pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter Pushdown\n",
    "\n",
    "Filter pushdown is the ability to push filter conditions down closer to the data source, reducing the amount of data that needs to be loaded.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `PushedFilters` in the scan operation\n",
    "- Filters applied before or after reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter on in-memory DataFrame:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (salary#11 > 100000.0)\n",
      "+- *(1) Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Checking if filters are pushed down\n",
    "query3 = employees_df.filter(col(\"salary\") > 100000)\n",
    "print(\"Filter on in-memory DataFrame:\")\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter on Parquet-backed DataFrame:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(salary#10176) AND (salary#10176 > 100000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.partitioned_employees[id#10174,name#10175,salary#10176,hire_date#10177,dept_id#10178] Batched: true, DataFilters: [isnotnull(salary#10176), (salary#10176 > 100000.0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/partitioned_employees], PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThan(salary,100000.0)], ReadSchema: struct<id:int,name:string,salary:double,hire_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Filter pushdown to Parquet files\n",
    "parquet_employees = spark.read.table(\"partitioned_employees\")\n",
    "query4 = parquet_employees.filter(col(\"salary\") > 100000)\n",
    "print(\"Filter on Parquet-backed DataFrame:\")\n",
    "query4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Filter Optimization:\n",
    "\n",
    "1. **Apply filters as early as possible**: Put filters before joins or aggregations\n",
    "2. **Use file formats that support predicate pushdown**: Parquet, ORC\n",
    "3. **Filter on partitioned columns**: For partition pruning\n",
    "4. **Use compatible filter expressions**: Some complex expressions can't be pushed down\n",
    "\n",
    "Let's compare the performance impact of filter pushdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter then Join: 0.3623 seconds, 699 records\n",
      "Join then Filter: 0.3026 seconds, 699 records\n",
      "Performance difference: 0.84x\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison: filter pushdown vs. no pushdown\n",
    "def time_execution(df):\n",
    "    start = time.time()\n",
    "    count = df.count()  # Force execution\n",
    "    end = time.time()\n",
    "    return end - start, count\n",
    "\n",
    "# With pushdown (filter then join)\n",
    "start = time.time()\n",
    "result1 = employees_df.filter(col(\"salary\") > 80000) \\\n",
    "    .join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time1, count1 = time_execution(result1)\n",
    "\n",
    "# Without pushdown (join then filter)\n",
    "start = time.time()\n",
    "result2 = employees_df \\\n",
    "    .join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .filter(col(\"salary\") > 80000) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time2, count2 = time_execution(result2)\n",
    "\n",
    "print(f\"Filter then Join: {time1:.4f} seconds, {count1} records\")\n",
    "print(f\"Join then Filter: {time2:.4f} seconds, {count2} records\")\n",
    "print(f\"Performance difference: {(time2/time1):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Join Strategies\n",
    "\n",
    "Spark supports several join strategies, and choosing the right one can significantly impact performance.\n",
    "\n",
    "#### Common Join Types in Spark:\n",
    "\n",
    "1. **Broadcast Hash Join**: Small table is broadcasted to all executors\n",
    "2. **Shuffle Hash Join**: Both tables are shuffled by join key\n",
    "3. **Sort Merge Join**: Both tables are sorted and then merged\n",
    "4. **Broadcast Nested Loop Join**: Used for cross joins and some non-equi joins\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- The join strategy being used\n",
    "- Broadcast hints being applied\n",
    "- Join order optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic Join Strategy Selection:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [dept_id#10], [id#0], Inner\n",
      "   :- Sort [dept_id#10 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(dept_id#10, 10), ENSURE_REQUIREMENTS, [plan_id=647]\n",
      "   :     +- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "   +- Sort [id#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#0, 10), ENSURE_REQUIREMENTS, [plan_id=648]\n",
      "         +- Scan ExistingRDD[id#0,department_name#1,location#2,budget#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Letting Spark choose the join strategy\n",
    "auto_join = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Automatic Join Strategy Selection:\")\n",
    "auto_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced Broadcast Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [dept_id#10], [id#0], Inner, BuildRight, false\n",
      "   :- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=664]\n",
      "      +- Scan ExistingRDD[id#0,department_name#1,location#2,budget#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Forcing a broadcast join\n",
    "broadcast_join = employees_df.join(broadcast(departments_df), employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Forced Broadcast Join:\")\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Impact of Join Strategies\n",
    "\n",
    "Let's measure the performance of different join strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast Join: 0.2784 seconds\n",
      "Default Strategy: 0.2820 seconds\n",
      "Performance difference: 1.01x\n"
     ]
    }
   ],
   "source": [
    "# Broadcast join performance\n",
    "start = time.time()\n",
    "broadcast_result = employees_df.join(broadcast(departments_df), employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time_broadcast, count_broadcast = time_execution(broadcast_result)\n",
    "\n",
    "# Default join strategy performance (likely sort-merge for this data size)\n",
    "start = time.time()\n",
    "default_result = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time_default, count_default = time_execution(default_result)\n",
    "\n",
    "print(f\"Broadcast Join: {time_broadcast:.4f} seconds\")\n",
    "print(f\"Default Strategy: {time_default:.4f} seconds\")\n",
    "print(f\"Performance difference: {(time_default/time_broadcast):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Strategy Selection Guidelines:\n",
    "\n",
    "1. **Broadcast Hash Join**: For small tables that can fit in memory (< 10MB by default)\n",
    "   - Controlled by `spark.sql.autoBroadcastJoinThreshold`\n",
    "   - Good for dimension tables joining with fact tables\n",
    "  \n",
    "2. **Sort Merge Join**: For large tables with well-distributed keys\n",
    "   - Becomes default when tables are too large to broadcast\n",
    "   - Good for large-to-large table joins\n",
    "  \n",
    "3. **Shuffle Hash Join**: For medium-sized tables with skewed data\n",
    "   - Controlled by `spark.sql.join.preferSortMergeJoin`\n",
    "  \n",
    "4. **Broadcast Nested Loop Join**: Last resort, typically for non-equality joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original broadcast threshold: 10485760b\n",
      "\n",
      "Join strategy with broadcasting disabled:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [dept_id#10], [id#0], Inner\n",
      "   :- Sort [dept_id#10 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(dept_id#10, 10), ENSURE_REQUIREMENTS, [plan_id=976]\n",
      "   :     +- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "   +- Sort [id#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#0, 10), ENSURE_REQUIREMENTS, [plan_id=977]\n",
      "         +- Scan ExistingRDD[id#0,department_name#1,location#2,budget#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing the broadcast threshold\n",
    "original_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Original broadcast threshold: {original_threshold}\")\n",
    "\n",
    "# Set a very low threshold to prevent broadcasting\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "no_broadcast_join = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"\\nJoin strategy with broadcasting disabled:\")\n",
    "no_broadcast_join.explain()\n",
    "\n",
    "# Reset to original value\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", original_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Partition Pruning\n",
    "\n",
    "Partition pruning is the ability of Spark to skip reading partitions that aren't relevant to the query, which can dramatically improve performance for large datasets.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `PartitionFilters` in the scan operation\n",
    "- Reduction in the number of files/partitions read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned table structure:\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|id                          |int                                                           |NULL   |\n",
      "|name                        |string                                                        |NULL   |\n",
      "|salary                      |double                                                        |NULL   |\n",
      "|hire_date                   |date                                                          |NULL   |\n",
      "|dept_id                     |int                                                           |NULL   |\n",
      "|# Partition Information     |                                                              |       |\n",
      "|# col_name                  |data_type                                                     |comment|\n",
      "|dept_id                     |int                                                           |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |partitioned_employees                                         |       |\n",
      "|Created Time                |Fri Apr 18 09:25:18 UTC 2025                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Statistics                  |99624 bytes, 1000 rows                                        |       |\n",
      "|Location                    |file:/opt/spark/work-dir/spark-warehouse/partitioned_employees|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our partitioned table\n",
    "print(\"Partitioned table structure:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED partitioned_employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query without partition pruning:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(salary#10176) AND (salary#10176 > 100000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.partitioned_employees[id#10174,name#10175,salary#10176,hire_date#10177,dept_id#10178] Batched: true, DataFilters: [isnotnull(salary#10176), (salary#10176 > 100000.0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/partitioned_employees], PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThan(salary,100000.0)], ReadSchema: struct<id:int,name:string,salary:double,hire_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Query without partition pruning\n",
    "query_no_pruning = spark.table(\"partitioned_employees\").filter(col(\"salary\") > 100000)\n",
    "print(\"Query without partition pruning:\")\n",
    "query_no_pruning.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query with partition pruning:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.partitioned_employees[id#10174,name#10175,salary#10176,hire_date#10177,dept_id#10178] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/partitioned_employees/dept_id=2], PartitionFilters: [isnotnull(dept_id#10178), (dept_id#10178 = 2)], PushedFilters: [], ReadSchema: struct<id:int,name:string,salary:double,hire_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Query with partition pruning\n",
    "query_with_pruning = spark.table(\"partitioned_employees\").filter(col(\"dept_id\") == 2)\n",
    "print(\"Query with partition pruning:\")\n",
    "query_with_pruning.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query with partition pruning and additional filters:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(salary#10176) AND (salary#10176 > 100000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.partitioned_employees[id#10174,name#10175,salary#10176,hire_date#10177,dept_id#10178] Batched: true, DataFilters: [isnotnull(salary#10176), (salary#10176 > 100000.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/partitioned_employees/dept_id=2], PartitionFilters: [isnotnull(dept_id#10178), (dept_id#10178 = 2)], PushedFilters: [IsNotNull(salary), GreaterThan(salary,100000.0)], ReadSchema: struct<id:int,name:string,salary:double,hire_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Query with both partition pruning and additional filters\n",
    "query_combined = spark.table(\"partitioned_employees\") \\\n",
    "    .filter((col(\"dept_id\") == 2) & (col(\"salary\") > 100000))\n",
    "print(\"Query with partition pruning and additional filters:\")\n",
    "query_combined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Impact of Partition Pruning\n",
    "\n",
    "Let's measure the performance difference between queries with and without partition pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Partition Pruning: 0.1598 seconds, 489 records\n",
      "With Partition Pruning: 0.0615 seconds, 95 records\n",
      "Performance improvement: 2.60x faster with pruning\n"
     ]
    }
   ],
   "source": [
    "# Without partition pruning\n",
    "start = time.time()\n",
    "result_no_pruning = spark.table(\"partitioned_employees\") \\\n",
    "    .filter(col(\"salary\") > 100000) \\\n",
    "    .select(\"id\", \"name\", \"salary\", \"dept_id\")\n",
    "time_no_pruning, count_no_pruning = time_execution(result_no_pruning)\n",
    "\n",
    "# With partition pruning\n",
    "start = time.time()\n",
    "result_with_pruning = spark.table(\"partitioned_employees\") \\\n",
    "    .filter((col(\"dept_id\") == 2) & (col(\"salary\") > 100000)) \\\n",
    "    .select(\"id\", \"name\", \"salary\", \"dept_id\")\n",
    "time_with_pruning, count_with_pruning = time_execution(result_with_pruning)\n",
    "\n",
    "print(f\"Without Partition Pruning: {time_no_pruning:.4f} seconds, {count_no_pruning} records\")\n",
    "print(f\"With Partition Pruning: {time_with_pruning:.4f} seconds, {count_with_pruning} records\")\n",
    "\n",
    "if time_no_pruning > time_with_pruning:\n",
    "    print(f\"Performance improvement: {(time_no_pruning/time_with_pruning):.2f}x faster with pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Partition Optimization:\n",
    "\n",
    "1. **Choose appropriate partition columns**: \n",
    "   - High cardinality but not too high (e.g., date, region, category)\n",
    "   - Commonly used in filters\n",
    "  \n",
    "2. **Avoid over-partitioning**: \n",
    "   - Too many small partitions create small files and overhead\n",
    "   - Aim for partition sizes between 128MB and 1GB\n",
    "  \n",
    "3. **Include partition columns in queries**: \n",
    "   - Ensure queries filter on partition columns when possible\n",
    "  \n",
    "4. **Consider bucketing for join performance**: \n",
    "   - Complement partitioning with bucketing for frequently joined columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Shuffle Operations\n",
    "\n",
    "Shuffles redistribute data across partitions and are often the most expensive operations in Spark. They involve disk I/O, serialization, network transfer, and deserialization.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `Exchange` operations in the physical plan\n",
    "- The type of exchange (e.g., HashPartitioning, RangePartitioning)\n",
    "- The number of shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupBy operation causing shuffle:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_id#10], functions=[count(1), avg(salary#11)])\n",
      "   +- Exchange hashpartitioning(dept_id#10, 10), ENSURE_REQUIREMENTS, [plan_id=1137]\n",
      "      +- HashAggregate(keys=[dept_id#10], functions=[partial_count(1), partial_avg(salary#11)])\n",
      "         +- Project [dept_id#10, salary#11]\n",
      "            +- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Operations that trigger shuffles\n",
    "shuffled_df = employees_df.groupBy(\"dept_id\").agg(count(\"*\").alias(\"emp_count\"), avg(\"salary\").alias(\"avg_salary\"))\n",
    "print(\"GroupBy operation causing shuffle:\")\n",
    "shuffled_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join operation causing shuffle:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [dept_id#10], [id#0], Inner\n",
      "   :- Sort [dept_id#10 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(dept_id#10, 10), ENSURE_REQUIREMENTS, [plan_id=1152]\n",
      "   :     +- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "   +- Sort [id#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#0, 10), ENSURE_REQUIREMENTS, [plan_id=1153]\n",
      "         +- Scan ExistingRDD[id#0,department_name#1,location#2,budget#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Join operation causing shuffle\n",
    "joined_df = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Join operation causing shuffle:\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of the Number of Shuffle Partitions\n",
    "\n",
    "Let's measure the impact of different shuffle partition settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle partitions: 2, Execution time: 0.2265 seconds\n",
      "Shuffle partitions: 10, Execution time: 0.1719 seconds\n",
      "Shuffle partitions: 50, Execution time: 0.1725 seconds\n",
      "Shuffle partitions: 200, Execution time: 0.1646 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test with different shuffle partition counts\n",
    "partition_counts = [2, 10, 50, 200]\n",
    "test_results = []\n",
    "\n",
    "for partitions in partition_counts:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", partitions)\n",
    "    \n",
    "    # Run a query that involves shuffling\n",
    "    start = time.time()\n",
    "    result = employees_df.groupBy(\"dept_id\").agg(\n",
    "        count(\"*\").alias(\"emp_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\")\n",
    "    )\n",
    "    execution_time, _ = time_execution(result)\n",
    "    \n",
    "    test_results.append((partitions, execution_time))\n",
    "    print(f\"Shuffle partitions: {partitions}, Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "# Reset to initial value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Shuffle Optimization:\n",
    "\n",
    "1. **Tune shuffle partitions**: \n",
    "   - Default is 200, which is often too high for small datasets\n",
    "   - Rule of thumb: 2-3 * number of cores for small-medium datasets\n",
    "   - For larger clusters, start with cluster cores * 3-4\n",
    "  \n",
    "2. **Use appropriate partitioning**: \n",
    "   - Pre-partition data by join keys to reduce shuffling\n",
    "   - Consider repartitioning before expensive operations\n",
    "  \n",
    "3. **Minimize the number of stages**: \n",
    "   - Chain transformations that don't require shuffles\n",
    "  \n",
    "4. **Use broadcast joins**: \n",
    "   - When possible, to avoid shuffling larger tables\n",
    "  \n",
    "5. **Consider enabling Adaptive Query Execution**:\n",
    "   - Allows Spark to dynamically coalesce shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution plan with Adaptive Execution enabled:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_id#10], functions=[count(1), avg(salary#11)])\n",
      "   +- Exchange hashpartitioning(dept_id#10, 50), ENSURE_REQUIREMENTS, [plan_id=1516]\n",
      "      +- HashAggregate(keys=[dept_id#10], functions=[partial_count(1), partial_avg(salary#11)])\n",
      "         +- Project [dept_id#10, salary#11]\n",
      "            +- Scan ExistingRDD[id#8,name#9,dept_id#10,salary#11,hire_date#12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable Adaptive Query Execution to automatically optimize shuffle partitions\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Run a query with many initial partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
    "adaptive_query = employees_df.groupBy(\"dept_id\").agg(\n",
    "    count(\"*\").alias(\"emp_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ")\n",
    "\n",
    "print(\"Query execution plan with Adaptive Execution enabled:\")\n",
    "adaptive_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Data Skew\n",
    "\n",
    "Data skew occurs when data is unevenly distributed across partitions, causing some tasks to run much longer than others. This can significantly impact performance in both grouping and join operations.\n",
    "\n",
    "#### What to Look For:\n",
    "- In the Spark UI: tasks in a stage taking much longer than others\n",
    "- Uneven partition sizes in the input or after a shuffle\n",
    "- High standard deviation in execution times\n",
    "\n",
    "Let's create a skewed dataset to demonstrate the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of employees across departments:\n",
      "+-------+-----+\n",
      "|dept_id|count|\n",
      "+-------+-----+\n",
      "|      1| 8092|\n",
      "|      2|  499|\n",
      "|      3|  449|\n",
      "|      4|  482|\n",
      "|      5|  478|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a skewed dataset\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Generate skewed data where 80% of records have dept_id = 1\n",
    "skewed_employee_data = []\n",
    "for i in range(1, 10001):  # 10,000 employees\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    # Create skew: 80% in dept_id 1\n",
    "    dept_id = 1 if random.random() < 0.8 else random.randint(2, 5)\n",
    "    salary = round(random.uniform(50000, 150000), 2)\n",
    "    random_days = random.randint(0, delta)\n",
    "    hire_date = start_date + timedelta(days=random_days)\n",
    "    skewed_employee_data.append((i, name, dept_id, salary, hire_date))\n",
    "\n",
    "skewed_employees_df = spark.createDataFrame(skewed_employee_data, employee_schema)\n",
    "skewed_employees_df.createOrReplaceTempView(\"skewed_employees\")\n",
    "\n",
    "# Check the distribution\n",
    "print(\"Distribution of employees across departments:\")\n",
    "skewed_employees_df.groupBy(\"dept_id\").count().orderBy(\"dept_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues Caused by Data Skew\n",
    "\n",
    "Let's observe the performance impact of skewed data in group by and join operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupBy on skewed data execution time: 0.1667 seconds\n",
      "Join on skewed data execution time: 0.3821 seconds\n"
     ]
    }
   ],
   "source": [
    "# GroupBy on skewed column\n",
    "start = time.time()\n",
    "skewed_agg = skewed_employees_df.groupBy(\"dept_id\").agg(\n",
    "    count(\"*\").alias(\"emp_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ")\n",
    "time_skewed_group, _ = time_execution(skewed_agg)\n",
    "\n",
    "# Join on skewed column\n",
    "start = time.time()\n",
    "skewed_join = skewed_employees_df.join(\n",
    "    departments_df,\n",
    "    skewed_employees_df[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "time_skewed_join, _ = time_execution(skewed_join)\n",
    "\n",
    "print(f\"GroupBy on skewed data execution time: {time_skewed_group:.4f} seconds\")\n",
    "print(f\"Join on skewed data execution time: {time_skewed_join:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques to Handle Data Skew\n",
    "\n",
    "1. **Salting**: Add a random number to the skewed key to distribute it\n",
    "2. **Two-phase aggregation**: Local aggregation followed by global aggregation\n",
    "3. **Separate processing**: Handle the skewed values separately\n",
    "\n",
    "Let's implement the salting technique to handle skew in joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified skewed keys: [1]\n",
      "Regular join on skewed data: 0.3821 seconds\n",
      "Salted join: 0.3510 seconds\n",
      "Performance improvement: 1.09x faster with salting\n"
     ]
    }
   ],
   "source": [
    "# Technique 1: Salting for skewed joins\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat\n",
    "\n",
    "# Step 1: Identify skewed values\n",
    "skewed_keys = skewed_employees_df.groupBy(\"dept_id\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1000) \\\n",
    "    .select(\"dept_id\") \\\n",
    "    .collect()\n",
    "\n",
    "skewed_keys_list = [row[0] for row in skewed_keys]\n",
    "print(f\"Identified skewed keys: {skewed_keys_list}\")\n",
    "\n",
    "# Step 2: Add salt to skewed keys\n",
    "salt_factor = 10  # Number of salt values to use\n",
    "\n",
    "# Add a salt value to skewed records\n",
    "salted_employees = skewed_employees_df.withColumn(\n",
    "    \"salt\", \n",
    "    when(col(\"dept_id\").isin(skewed_keys_list), \n",
    "         monotonically_increasing_id() % salt_factor)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Create a salted key for joining\n",
    "salted_employees = salted_employees.withColumn(\n",
    "    \"salted_dept_id\", \n",
    "    when(col(\"salt\") > 0, \n",
    "         concat(col(\"dept_id\").cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\")))\n",
    "    .otherwise(col(\"dept_id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Step 3: Explode the department table for skewed keys\n",
    "from pyspark.sql.functions import explode, array, lit\n",
    "\n",
    "# Generate salted keys for the departments table\n",
    "exploded_depts = departments_df.withColumn(\n",
    "    \"is_skewed\", col(\"id\").isin(skewed_keys_list)\n",
    ")\n",
    "\n",
    "# For skewed keys, create multiple copies with salts\n",
    "salted_depts = exploded_depts.withColumn(\n",
    "    \"salt_values\",\n",
    "    when(col(\"is_skewed\"), \n",
    "         array([lit(i) for i in range(salt_factor)]))\n",
    "    .otherwise(array(lit(0)))\n",
    ")\n",
    "\n",
    "# Explode to create multiple rows for skewed keys\n",
    "salted_depts = salted_depts.withColumn(\"salt\", explode(\"salt_values\")).drop(\"salt_values\")\n",
    "\n",
    "# Create matching salted key\n",
    "salted_depts = salted_depts.withColumn(\n",
    "    \"salted_id\", \n",
    "    when(col(\"salt\") > 0, \n",
    "         concat(col(\"id\").cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\")))\n",
    "    .otherwise(col(\"id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Step 4: Join on the salted keys\n",
    "salted_join = salted_employees.join(\n",
    "    salted_depts,\n",
    "    salted_employees[\"salted_dept_id\"] == salted_depts[\"salted_id\"]\n",
    ")\n",
    "\n",
    "# Measure performance\n",
    "time_salted_join, count_salted = time_execution(salted_join)\n",
    "\n",
    "print(f\"Regular join on skewed data: {time_skewed_join:.4f} seconds\")\n",
    "print(f\"Salted join: {time_salted_join:.4f} seconds\")\n",
    "if time_skewed_join > time_salted_join:\n",
    "    print(f\"Performance improvement: {(time_skewed_join/time_salted_join):.2f}x faster with salting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Approach: Handle Skewed Values Separately\n",
    "\n",
    "Another strategy is to process skewed values separately from the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed records: 8092, Normal records: 1908\n",
      "Regular join on skewed data: 0.3821 seconds\n",
      "Split processing: 0.6445 seconds\n"
     ]
    }
   ],
   "source": [
    "# Technique 2: Process skewed values separately\n",
    "\n",
    "# Split the dataset\n",
    "skewed_records = skewed_employees_df.filter(col(\"dept_id\").isin(skewed_keys_list))\n",
    "normal_records = skewed_employees_df.filter(~col(\"dept_id\").isin(skewed_keys_list))\n",
    "\n",
    "print(f\"Skewed records: {skewed_records.count()}, Normal records: {normal_records.count()}\")\n",
    "\n",
    "# Process the skewed records with broadcast join\n",
    "start = time.time()\n",
    "skewed_result = skewed_records.join(\n",
    "    broadcast(departments_df),\n",
    "    skewed_records[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "\n",
    "# Process normal records with regular join\n",
    "normal_result = normal_records.join(\n",
    "    departments_df,\n",
    "    normal_records[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "combined_result = skewed_result.union(normal_result)\n",
    "time_split_process, count_split = time_execution(combined_result)\n",
    "\n",
    "print(f\"Regular join on skewed data: {time_skewed_join:.4f} seconds\")\n",
    "print(f\"Split processing: {time_split_process:.4f} seconds\")\n",
    "if time_skewed_join > time_split_process:\n",
    "    print(f\"Performance improvement: {(time_skewed_join/time_split_process):.2f}x faster with split processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. End-to-End Optimization Example\n",
    "\n",
    "Let's bring everything together with an end-to-end example. We'll start with a suboptimal query and improve it step by step using the techniques we've discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created datasets with 100000 orders, 1000 customers, and 100 products.\n"
     ]
    }
   ],
   "source": [
    "# Reset configuration to defaults\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Default value\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # 10MB\n",
    "\n",
    "# Create larger datasets for this example\n",
    "from pyspark.sql.functions import current_date, datediff, rand\n",
    "\n",
    "# Create a large fact table (orders)\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", DateType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"price\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Generate 100,000 orders\n",
    "order_data = []\n",
    "for i in range(1, 100001):\n",
    "    customer_id = random.randint(1, 1000)\n",
    "    product_id = random.randint(1, 100)\n",
    "    random_days = random.randint(0, 365*3)  # Last 3 years\n",
    "    order_date = datetime.now() - timedelta(days=random_days)\n",
    "    quantity = random.randint(1, 10)\n",
    "    price = round(random.uniform(10, 1000), 2)\n",
    "    order_data.append((i, customer_id, product_id, order_date, quantity, price))\n",
    "\n",
    "orders_df = spark.createDataFrame(order_data, orders_schema)\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# Create dimension tables\n",
    "# Customers\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"signup_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Generate 1,000 customers\n",
    "customer_data = []\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\"]\n",
    "states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\", \"PA\", \"TX\", \"CA\"]\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    email = f\"{name.lower().replace(' ', '.')}@example.com\"\n",
    "    city_idx = random.randint(0, len(cities)-1)\n",
    "    city = cities[city_idx]\n",
    "    state = states[city_idx]\n",
    "    random_days = random.randint(365, 365*5)  # 1-5 years ago\n",
    "    signup_date = datetime.now() - timedelta(days=random_days)\n",
    "    customer_data.append((i, name, email, city, state, signup_date))\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# Products\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"base_price\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Generate 100 products\n",
    "product_data = []\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Sports\"]\n",
    "product_names = [\"Laptop\", \"Phone\", \"Tablet\", \"TV\", \"Camera\", \"Shirt\", \"Pants\", \"Shoes\", \"Jacket\", \"Sofa\", \n",
    "                 \"Chair\", \"Table\", \"Bed\", \"Novel\", \"Textbook\", \"Cookbook\", \"Basketball\", \"Tennis Racket\", \"Bicycle\"]\n",
    "\n",
    "for i in range(1, 101):\n",
    "    name = random.choice(product_names)\n",
    "    category = random.choice(categories)\n",
    "    base_price = round(random.uniform(10, 500), 2)\n",
    "    product_data.append((i, name, category, base_price))\n",
    "\n",
    "products_df = spark.createDataFrame(product_data, product_schema)\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(f\"Created datasets with {orders_df.count()} orders, {customers_df.count()} customers, and {products_df.count()} products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Suboptimal Query\n",
    "\n",
    "Let's start with a suboptimal query that computes total sales by state and category for the last year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query Plan:\n",
      "== Physical Plan ==\n",
      "*(12) Sort [total_sales#10908 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_sales#10908 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2770]\n",
      "   +- *(11) HashAggregate(keys=[state#10860, category#10870], functions=[sum((cast(quantity#10848 as double) * price#10849)), count(distinct order_id#10844)])\n",
      "      +- Exchange hashpartitioning(state#10860, category#10870, 200), ENSURE_REQUIREMENTS, [plan_id=2766]\n",
      "         +- *(10) HashAggregate(keys=[state#10860, category#10870], functions=[merge_sum((cast(quantity#10848 as double) * price#10849)), partial_count(distinct order_id#10844)])\n",
      "            +- *(10) HashAggregate(keys=[state#10860, category#10870, order_id#10844], functions=[merge_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "               +- Exchange hashpartitioning(state#10860, category#10870, order_id#10844, 200), ENSURE_REQUIREMENTS, [plan_id=2761]\n",
      "                  +- *(9) HashAggregate(keys=[state#10860, category#10870, order_id#10844], functions=[partial_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "                     +- *(9) Project [order_id#10844, quantity#10848, price#10849, state#10860, category#10870]\n",
      "                        +- *(9) SortMergeJoin [product_id#10846], [product_id#10868], Inner\n",
      "                           :- *(6) Sort [product_id#10846 ASC NULLS FIRST], false, 0\n",
      "                           :  +- Exchange hashpartitioning(product_id#10846, 200), ENSURE_REQUIREMENTS, [plan_id=2746]\n",
      "                           :     +- *(5) Project [order_id#10844, product_id#10846, quantity#10848, price#10849, state#10860]\n",
      "                           :        +- *(5) SortMergeJoin [customer_id#10845], [customer_id#10856], Inner\n",
      "                           :           :- *(2) Sort [customer_id#10845 ASC NULLS FIRST], false, 0\n",
      "                           :           :  +- Exchange hashpartitioning(customer_id#10845, 200), ENSURE_REQUIREMENTS, [plan_id=2732]\n",
      "                           :           :     +- *(1) Project [order_id#10844, customer_id#10845, product_id#10846, quantity#10848, price#10849]\n",
      "                           :           :        +- *(1) Filter (order_date#10847 >= 2024-04-18)\n",
      "                           :           :           +- *(1) Scan ExistingRDD[order_id#10844,customer_id#10845,product_id#10846,order_date#10847,quantity#10848,price#10849]\n",
      "                           :           +- *(4) Sort [customer_id#10856 ASC NULLS FIRST], false, 0\n",
      "                           :              +- Exchange hashpartitioning(customer_id#10856, 200), ENSURE_REQUIREMENTS, [plan_id=2738]\n",
      "                           :                 +- *(3) Project [customer_id#10856, state#10860]\n",
      "                           :                    +- *(3) Scan ExistingRDD[customer_id#10856,name#10857,email#10858,city#10859,state#10860,signup_date#10861]\n",
      "                           +- *(8) Sort [product_id#10868 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(product_id#10868, 200), ENSURE_REQUIREMENTS, [plan_id=2752]\n",
      "                                 +- *(7) Project [product_id#10868, category#10870]\n",
      "                                    +- *(7) Scan ExistingRDD[product_id#10868,name#10869,category#10870,base_price#10871]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original query execution time: 2.5330 seconds\n"
     ]
    }
   ],
   "source": [
    "# Original inefficient query\n",
    "def run_original_query():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.state, \n",
    "        p.category, \n",
    "        COUNT(DISTINCT o.order_id) as num_orders,\n",
    "        SUM(o.quantity * o.price) as total_sales\n",
    "    FROM \n",
    "        orders o\n",
    "    JOIN \n",
    "        customers c ON o.customer_id = c.customer_id\n",
    "    JOIN \n",
    "        products p ON o.product_id = p.product_id\n",
    "    WHERE \n",
    "        o.order_date >= date_sub(current_date(), 365)\n",
    "    GROUP BY \n",
    "        c.state, p.category\n",
    "    ORDER BY \n",
    "        total_sales DESC\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Original Query Plan:\")\n",
    "original_query = run_original_query()\n",
    "original_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "original_query.collect()\n",
    "original_time = time.time() - start\n",
    "print(f\"\\nOriginal query execution time: {original_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 1: Filter Pushdown and Early Projections\n",
    "\n",
    "Our first optimization will be to apply filters early and use only necessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Query Plan (Filter Pushdown and Column Pruning):\n",
      "== Physical Plan ==\n",
      "*(11) Sort [total_sales#10963 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_sales#10963 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2954]\n",
      "   +- *(10) HashAggregate(keys=[state#10860, category#10870], functions=[count(1), sum((cast(quantity#10848 as double) * price#10849))])\n",
      "      +- Exchange hashpartitioning(state#10860, category#10870, 200), ENSURE_REQUIREMENTS, [plan_id=2950]\n",
      "         +- *(9) HashAggregate(keys=[state#10860, category#10870], functions=[partial_count(1), partial_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "            +- *(9) Project [quantity#10848, price#10849, state#10860, category#10870]\n",
      "               +- *(9) SortMergeJoin [product_id#10846], [product_id#10868], Inner\n",
      "                  :- *(6) Sort [product_id#10846 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(product_id#10846, 200), ENSURE_REQUIREMENTS, [plan_id=2935]\n",
      "                  :     +- *(5) Project [product_id#10846, quantity#10848, price#10849, state#10860]\n",
      "                  :        +- *(5) SortMergeJoin [customer_id#10845], [customer_id#10856], Inner\n",
      "                  :           :- *(2) Sort [customer_id#10845 ASC NULLS FIRST], false, 0\n",
      "                  :           :  +- Exchange hashpartitioning(customer_id#10845, 200), ENSURE_REQUIREMENTS, [plan_id=2921]\n",
      "                  :           :     +- *(1) Project [customer_id#10845, product_id#10846, quantity#10848, price#10849]\n",
      "                  :           :        +- *(1) Filter (order_date#10847 >= 2024-04-18)\n",
      "                  :           :           +- *(1) Scan ExistingRDD[order_id#10844,customer_id#10845,product_id#10846,order_date#10847,quantity#10848,price#10849]\n",
      "                  :           +- *(4) Sort [customer_id#10856 ASC NULLS FIRST], false, 0\n",
      "                  :              +- Exchange hashpartitioning(customer_id#10856, 200), ENSURE_REQUIREMENTS, [plan_id=2927]\n",
      "                  :                 +- *(3) Project [customer_id#10856, state#10860]\n",
      "                  :                    +- *(3) Scan ExistingRDD[customer_id#10856,name#10857,email#10858,city#10859,state#10860,signup_date#10861]\n",
      "                  +- *(8) Sort [product_id#10868 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(product_id#10868, 200), ENSURE_REQUIREMENTS, [plan_id=2941]\n",
      "                        +- *(7) Project [product_id#10868, category#10870]\n",
      "                           +- *(7) Scan ExistingRDD[product_id#10868,name#10869,category#10870,base_price#10871]\n",
      "\n",
      "\n",
      "\n",
      "Step 1 query execution time: 1.3588 seconds\n",
      "Improvement from original: 1.86x faster\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter pushdown and early projections\n",
    "def run_optimized_query_step1():\n",
    "    # First filter the orders\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    # Use only necessary columns from dimension tables\n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    # Join and aggregate\n",
    "    result = filtered_orders \\\n",
    "        .join(customers_slim, \"customer_id\") \\\n",
    "        .join(products_slim, \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 1 Query Plan (Filter Pushdown and Column Pruning):\")\n",
    "step1_query = run_optimized_query_step1()\n",
    "step1_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step1_query.collect()\n",
    "step1_time = time.time() - start\n",
    "print(f\"\\nStep 1 query execution time: {step1_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step1_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 2: Broadcast Small Tables\n",
    "\n",
    "Next, let's use broadcast joins for the dimension tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Query Plan (With Broadcast Joins):\n",
      "== Physical Plan ==\n",
      "*(5) Sort [total_sales#11007 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_sales#11007 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=3104]\n",
      "   +- *(4) HashAggregate(keys=[state#10860, category#10870], functions=[count(1), sum((cast(quantity#10848 as double) * price#10849))])\n",
      "      +- Exchange hashpartitioning(state#10860, category#10870, 200), ENSURE_REQUIREMENTS, [plan_id=3100]\n",
      "         +- *(3) HashAggregate(keys=[state#10860, category#10870], functions=[partial_count(1), partial_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "            +- *(3) Project [quantity#10848, price#10849, state#10860, category#10870]\n",
      "               +- *(3) BroadcastHashJoin [product_id#10846], [product_id#10868], Inner, BuildRight, false\n",
      "                  :- *(3) Project [product_id#10846, quantity#10848, price#10849, state#10860]\n",
      "                  :  +- *(3) BroadcastHashJoin [customer_id#10845], [customer_id#10856], Inner, BuildRight, false\n",
      "                  :     :- *(3) Project [customer_id#10845, product_id#10846, quantity#10848, price#10849]\n",
      "                  :     :  +- *(3) Filter (order_date#10847 >= 2024-04-18)\n",
      "                  :     :     +- *(3) Scan ExistingRDD[order_id#10844,customer_id#10845,product_id#10846,order_date#10847,quantity#10848,price#10849]\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3089]\n",
      "                  :        +- *(1) Project [customer_id#10856, state#10860]\n",
      "                  :           +- *(1) Scan ExistingRDD[customer_id#10856,name#10857,email#10858,city#10859,state#10860,signup_date#10861]\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3094]\n",
      "                     +- *(2) Project [product_id#10868, category#10870]\n",
      "                        +- *(2) Scan ExistingRDD[product_id#10868,name#10869,category#10870,base_price#10871]\n",
      "\n",
      "\n",
      "\n",
      "Step 2 query execution time: 0.6238 seconds\n",
      "Improvement from original: 4.06x faster\n",
      "Improvement from step 1: 2.18x faster\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add broadcast joins\n",
    "def run_optimized_query_step2():\n",
    "    # First filter the orders\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    # Use only necessary columns from dimension tables\n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    # Broadcast the dimension tables for more efficient joins\n",
    "    result = filtered_orders \\\n",
    "        .join(broadcast(customers_slim), \"customer_id\") \\\n",
    "        .join(broadcast(products_slim), \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 2 Query Plan (With Broadcast Joins):\")\n",
    "step2_query = run_optimized_query_step2()\n",
    "step2_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step2_query.collect()\n",
    "step2_time = time.time() - start\n",
    "print(f\"\\nStep 2 query execution time: {step2_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step2_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 1: {(step1_time/step2_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 3: Tune Shuffle Partitions\n",
    "\n",
    "Now, let's adjust the number of shuffle partitions to better match our dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Query Plan (With Adjusted Shuffle Partitions):\n",
      "== Physical Plan ==\n",
      "*(5) Sort [total_sales#11051 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_sales#11051 DESC NULLS LAST, 20), ENSURE_REQUIREMENTS, [plan_id=3228]\n",
      "   +- *(4) HashAggregate(keys=[state#10860, category#10870], functions=[count(1), sum((cast(quantity#10848 as double) * price#10849))])\n",
      "      +- Exchange hashpartitioning(state#10860, category#10870, 20), ENSURE_REQUIREMENTS, [plan_id=3224]\n",
      "         +- *(3) HashAggregate(keys=[state#10860, category#10870], functions=[partial_count(1), partial_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "            +- *(3) Project [quantity#10848, price#10849, state#10860, category#10870]\n",
      "               +- *(3) BroadcastHashJoin [product_id#10846], [product_id#10868], Inner, BuildRight, false\n",
      "                  :- *(3) Project [product_id#10846, quantity#10848, price#10849, state#10860]\n",
      "                  :  +- *(3) BroadcastHashJoin [customer_id#10845], [customer_id#10856], Inner, BuildRight, false\n",
      "                  :     :- *(3) Project [customer_id#10845, product_id#10846, quantity#10848, price#10849]\n",
      "                  :     :  +- *(3) Filter (order_date#10847 >= 2024-04-18)\n",
      "                  :     :     +- *(3) Scan ExistingRDD[order_id#10844,customer_id#10845,product_id#10846,order_date#10847,quantity#10848,price#10849]\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3213]\n",
      "                  :        +- *(1) Project [customer_id#10856, state#10860]\n",
      "                  :           +- *(1) Scan ExistingRDD[customer_id#10856,name#10857,email#10858,city#10859,state#10860,signup_date#10861]\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3218]\n",
      "                     +- *(2) Project [product_id#10868, category#10870]\n",
      "                        +- *(2) Scan ExistingRDD[product_id#10868,name#10869,category#10870,base_price#10871]\n",
      "\n",
      "\n",
      "\n",
      "Step 3 query execution time: 0.3980 seconds\n",
      "Improvement from original: 6.36x faster\n",
      "Improvement from step 2: 1.57x faster\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tune shuffle partitions\n",
    "# Default is 200, which is likely too high for our relatively small dataset\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)  # Adjusted value\n",
    "\n",
    "def run_optimized_query_step3():\n",
    "    # Same query as step 2, but with adjusted shuffle partitions\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    result = filtered_orders \\\n",
    "        .join(broadcast(customers_slim), \"customer_id\") \\\n",
    "        .join(broadcast(products_slim), \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 3 Query Plan (With Adjusted Shuffle Partitions):\")\n",
    "step3_query = run_optimized_query_step3()\n",
    "step3_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step3_query.collect()\n",
    "step3_time = time.time() - start\n",
    "print(f\"\\nStep 3 query execution time: {step3_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step3_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 2: {(step2_time/step3_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 4: Enable Adaptive Query Execution\n",
    "\n",
    "Finally, let's enable Adaptive Query Execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 Query Plan (With Adaptive Query Execution):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_sales#11095 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_sales#11095 DESC NULLS LAST, 20), ENSURE_REQUIREMENTS, [plan_id=3343]\n",
      "      +- HashAggregate(keys=[state#10860, category#10870], functions=[count(1), sum((cast(quantity#10848 as double) * price#10849))])\n",
      "         +- Exchange hashpartitioning(state#10860, category#10870, 20), ENSURE_REQUIREMENTS, [plan_id=3340]\n",
      "            +- HashAggregate(keys=[state#10860, category#10870], functions=[partial_count(1), partial_sum((cast(quantity#10848 as double) * price#10849))])\n",
      "               +- Project [quantity#10848, price#10849, state#10860, category#10870]\n",
      "                  +- SortMergeJoin [product_id#10846], [product_id#10868], Inner\n",
      "                     :- Sort [product_id#10846 ASC NULLS FIRST], false, 0\n",
      "                     :  +- Exchange hashpartitioning(product_id#10846, 20), ENSURE_REQUIREMENTS, [plan_id=3332]\n",
      "                     :     +- Project [product_id#10846, quantity#10848, price#10849, state#10860]\n",
      "                     :        +- SortMergeJoin [customer_id#10845], [customer_id#10856], Inner\n",
      "                     :           :- Sort [customer_id#10845 ASC NULLS FIRST], false, 0\n",
      "                     :           :  +- Exchange hashpartitioning(customer_id#10845, 20), ENSURE_REQUIREMENTS, [plan_id=3324]\n",
      "                     :           :     +- Project [customer_id#10845, product_id#10846, quantity#10848, price#10849]\n",
      "                     :           :        +- Filter (order_date#10847 >= 2024-04-18)\n",
      "                     :           :           +- Scan ExistingRDD[order_id#10844,customer_id#10845,product_id#10846,order_date#10847,quantity#10848,price#10849]\n",
      "                     :           +- Sort [customer_id#10856 ASC NULLS FIRST], false, 0\n",
      "                     :              +- Exchange hashpartitioning(customer_id#10856, 20), ENSURE_REQUIREMENTS, [plan_id=3325]\n",
      "                     :                 +- Project [customer_id#10856, state#10860]\n",
      "                     :                    +- Scan ExistingRDD[customer_id#10856,name#10857,email#10858,city#10859,state#10860,signup_date#10861]\n",
      "                     +- Sort [product_id#10868 ASC NULLS FIRST], false, 0\n",
      "                        +- Exchange hashpartitioning(product_id#10868, 20), ENSURE_REQUIREMENTS, [plan_id=3333]\n",
      "                           +- Project [product_id#10868, category#10870]\n",
      "                              +- Scan ExistingRDD[product_id#10868,name#10869,category#10870,base_price#10871]\n",
      "\n",
      "\n",
      "\n",
      "Step 4 query execution time: 0.4916 seconds\n",
      "Improvement from original: 5.15x faster\n",
      "Improvement from step 3: 0.81x faster\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Enable Adaptive Query Execution\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "def run_optimized_query_step4():\n",
    "    # Same query as step 3, but with AQE enabled\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    result = filtered_orders \\\n",
    "        .join(customers_slim, \"customer_id\") \\\n",
    "        .join(products_slim, \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 4 Query Plan (With Adaptive Query Execution):\")\n",
    "step4_query = run_optimized_query_step4()\n",
    "step4_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step4_query.collect()\n",
    "step4_time = time.time() - start\n",
    "print(f\"\\nStep 4 query execution time: {step4_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step4_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 3: {(step3_time/step4_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing All Optimizations\n",
    "\n",
    "Let's summarize the improvements from each optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Summary:\n",
      "Original query: 2.5330 seconds\n",
      "Step 1 (Filter Pushdown & Column Pruning): 1.3588 seconds, 1.86x faster\n",
      "Step 2 (Broadcast Joins): 0.6238 seconds, 4.06x faster\n",
      "Step 3 (Tuned Shuffle Partitions): 0.3980 seconds, 6.36x faster\n",
      "Step 4 (Adaptive Query Execution): 0.4916 seconds, 5.15x faster\n"
     ]
    }
   ],
   "source": [
    "# Summary of performance improvements\n",
    "print(\"Performance Summary:\")\n",
    "print(f\"Original query: {original_time:.4f} seconds\")\n",
    "print(f\"Step 1 (Filter Pushdown & Column Pruning): {step1_time:.4f} seconds, {(original_time/step1_time):.2f}x faster\")\n",
    "print(f\"Step 2 (Broadcast Joins): {step2_time:.4f} seconds, {(original_time/step2_time):.2f}x faster\")\n",
    "print(f\"Step 3 (Tuned Shuffle Partitions): {step3_time:.4f} seconds, {(original_time/step3_time):.2f}x faster\")\n",
    "print(f\"Step 4 (Adaptive Query Execution): {step4_time:.4f} seconds, {(original_time/step4_time):.2f}x faster\")\n",
    "\n",
    "# Reset configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Default value\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices and Guidelines\n",
    "\n",
    "Let's summarize the key best practices for optimizing Spark performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization by Area\n",
    "\n",
    "#### 1. Data Reading and Filtering\n",
    "- **Select only necessary columns**: Reduces I/O and memory usage\n",
    "- **Apply filters as early as possible**: Reduces data volume for downstream operations\n",
    "- **Use appropriate file formats**: Parquet/ORC > CSV/JSON for analytical workloads\n",
    "- **Leverage partition pruning**: Choose good partition columns for large tables\n",
    "- **Compute and maintain statistics**: For better query planning\n",
    "\n",
    "#### 2. Joins\n",
    "- **Optimize join order**: Join the most filtered/smallest tables first\n",
    "- **Use broadcast joins for small tables**: Avoid shuffling large tables\n",
    "- **Handle skew in joins**: Use salting or separate processing for skewed keys\n",
    "- **Pre-partition data on join keys**: To reduce shuffling\n",
    "- **Filter before joining**: Reduces the size of the join operation\n",
    "\n",
    "#### 3. Aggregations\n",
    "- **Tune shuffle partitions**: Based on data size and cluster capacity\n",
    "- **Use window functions efficiently**: For complex analytical queries\n",
    "- **Pre-aggregate data when possible**: Reduces data volume for global aggregations\n",
    "- **Handle skew in groupBy**: Similar to join skew handling\n",
    "\n",
    "#### 4. General Optimizations\n",
    "- **Use built-in functions over UDFs**: Better performance with codegen\n",
    "- **Enable Adaptive Query Execution**: For dynamic optimizations\n",
    "- **Cache judiciously**: Only for frequently accessed datasets\n",
    "- **Monitor and analyze query plans**: Identify bottlenecks\n",
    "- **Set appropriate configurations**: Memory, cores, shuffle partitions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "\n",
    "Here are some important configuration parameters to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration:\n",
      "spark.sql.shuffle.partitions: 200\n",
      "spark.sql.autoBroadcastJoinThreshold: 10485760\n",
      "spark.sql.adaptive.enabled: false\n",
      "spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "spark.driver.memory: Not set\n",
      "spark.executor.memory: Not set\n",
      "spark.sql.files.maxPartitionBytes: 134217728b\n",
      "spark.default.parallelism: Not set\n"
     ]
    }
   ],
   "source": [
    "# Show current configuration values\n",
    "print(\"Current configuration:\")\n",
    "config_params = [\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.sql.files.maxPartitionBytes\",\n",
    "    \"spark.default.parallelism\"\n",
    "]\n",
    "\n",
    "for param in config_params:\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"{param}: {value}\")\n",
    "    except:\n",
    "        print(f\"{param}: Not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Configuration Tuning\n",
    "\n",
    "1. **spark.sql.shuffle.partitions**:\n",
    "   - Default: 200\n",
    "   - Start with 2-3 times the number of cores for small-medium datasets\n",
    "   - For large datasets (TB+), increase based on data size\n",
    "\n",
    "2. **spark.sql.autoBroadcastJoinThreshold**:\n",
    "   - Default: 10MB\n",
    "   - Increase for larger driver memory (e.g., 50-100MB)\n",
    "   - Set to -1 to disable automatic broadcasting\n",
    "\n",
    "3. **spark.sql.adaptive.enabled**:\n",
    "   - Default: true (in Spark 3.x)\n",
    "   - Recommended: true for most workloads\n",
    "\n",
    "4. **spark.driver.memory** and **spark.executor.memory**:\n",
    "   - Depends on your cluster resources\n",
    "   - Avoid OOM errors with appropriate sizing\n",
    "\n",
    "5. **spark.sql.files.maxPartitionBytes**:\n",
    "   - Default: 128MB\n",
    "   - Adjust based on file size and memory available\n",
    "\n",
    "### When to Use Each Optimization Technique\n",
    "\n",
    "| Technique               | When to Use                                 | Example Scenario                             |\n",
    "|-------------------------|---------------------------------------------|----------------------------------------------|\n",
    "| Filter Pushdown         | Large datasets with filtering conditions    | Querying subset of time-series data          |\n",
    "| Broadcast Joins         | Joining with small dimension tables         | Fact table with dimension lookups            |\n",
    "| Repartitioning          | Before joins on non-uniform data            | Pre-shuffle data on join key                 |\n",
    "| Caching                 | Repeatedly accessed intermediate results    | Iterative algorithms, multiple queries       |\n",
    "| Salting                 | Highly skewed join or groupBy keys          | User activity data with popular items        |\n",
    "| Shuffle Partition Tuning| Performance tuning for specific data size   | Adjust based on cluster and dataset size     |\n",
    "| Adaptive Execution      | Complex queries with unpredictable stats    | Ad-hoc analytical queries                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Tips\n",
    "\n",
    "1. **Understand your data**: Distribution, size, access patterns\n",
    "2. **Analyze query plans**: Identify bottlenecks early\n",
    "3. **Test incrementally**: Apply one optimization at a time\n",
    "4. **Monitor Spark UI**: For execution details and skew detection\n",
    "5. **Prefer simple optimizations first**: Often the biggest gains come from basic techniques\n",
    "\n",
    "By carefully analyzing execution plans and applying these optimization techniques, you can significantly improve the performance of your Spark queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources cleaned up. Notebook complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 09:25:52 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-2842cb6d-cd48-48a1-95cb-946c414be354. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-2842cb6d-cd48-48a1-95cb-946c414be354\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/18 09:25:52 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-2842cb6d-cd48-48a1-95cb-946c414be354/38. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-2842cb6d-cd48-48a1-95cb-946c414be354/38\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:129)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "spark.catalog.clearCache()\n",
    "print(\"Resources cleaned up. Notebook complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
