{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex UDFs with Dictionary Return Types in PySpark\n",
    "\n",
    "This notebook demonstrates how to create and use PySpark UDFs that read multiple input columns and return Python dictionaries containing various data types including:\n",
    "- Integers\n",
    "- Strings\n",
    "- Nested objects/dictionaries\n",
    "\n",
    "We'll cover:\n",
    "1. Creating UDFs that return dictionaries\n",
    "2. Converting Python dictionaries to structured data in Spark\n",
    "3. Handling nested structures efficiently\n",
    "4. Performance considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 06:51:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55570)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, struct, to_json, from_json\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, BooleanType, \n",
    "    StructType, StructField, MapType, ArrayType\n",
    ")\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Dictionary UDF Examples\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Data\n",
    "\n",
    "Let's create a sample DataFrame with various data types to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n",
      "+---+-------+---+-------------+--------+\n",
      "| id|   name|age|         city|  salary|\n",
      "+---+-------+---+-------------+--------+\n",
      "|  1|   John| 32|     New York| 75000.0|\n",
      "|  2|  Alice| 28|San Francisco| 95000.0|\n",
      "|  3|    Bob| 45|      Chicago| 85000.0|\n",
      "|  4|   Emma| 36|      Seattle| 92000.0|\n",
      "|  5|Michael| 52|       Boston|120000.0|\n",
      "+---+-------+---+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"John\", 32, \"New York\", 75000.0),\n",
    "    (2, \"Alice\", 28, \"San Francisco\", 95000.0),\n",
    "    (3, \"Bob\", 45, \"Chicago\", 85000.0),\n",
    "    (4, \"Emma\", 36, \"Seattle\", 92000.0),\n",
    "    (5, \"Michael\", 52, \"Boston\", 120000.0)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"city\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UDF that Returns a Simple Dictionary\n",
    "\n",
    "First, let's create a UDF that processes multiple columns and returns a simple dictionary containing string and integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Dictionary UDF result:\n",
      "+---+-------+---+-------------+--------+---------------------------------------------------------------------------------------+\n",
      "|id |name   |age|city         |salary  |person_info                                                                            |\n",
      "+---+-------+---+-------------+--------+---------------------------------------------------------------------------------------+\n",
      "|1  |John   |32 |New York     |75000.0 |{\"full_name\":\"John\",\"age_in_years\":32,\"residence\":\"New York\",\"age_group\":\"middle\"}     |\n",
      "|2  |Alice  |28 |San Francisco|95000.0 |{\"full_name\":\"Alice\",\"age_in_years\":28,\"residence\":\"San Francisco\",\"age_group\":\"young\"}|\n",
      "|3  |Bob    |45 |Chicago      |85000.0 |{\"full_name\":\"Bob\",\"age_in_years\":45,\"residence\":\"Chicago\",\"age_group\":\"middle\"}       |\n",
      "|4  |Emma   |36 |Seattle      |92000.0 |{\"full_name\":\"Emma\",\"age_in_years\":36,\"residence\":\"Seattle\",\"age_group\":\"middle\"}      |\n",
      "|5  |Michael|52 |Boston       |120000.0|{\"full_name\":\"Michael\",\"age_in_years\":52,\"residence\":\"Boston\",\"age_group\":\"senior\"}    |\n",
      "+---+-------+---+-------------+--------+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF that returns a simple dictionary\n",
    "def create_person_dict(name, age, city):\n",
    "    \"\"\"\n",
    "    Create a dictionary with person information\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"full_name\": name,\n",
    "        \"age_in_years\": age,\n",
    "        \"residence\": city,\n",
    "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
    "    }\n",
    "\n",
    "# Define the schema for the struct we want to return\n",
    "person_struct_schema = StructType([\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"age_in_years\", IntegerType(), True),\n",
    "    StructField(\"residence\", StringType(), True),\n",
    "    StructField(\"age_group\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register UDF with struct return type that can be converted to JSON\n",
    "person_struct_udf = udf(create_person_dict, person_struct_schema)\n",
    "\n",
    "# Apply the UDF and convert resulting struct to JSON\n",
    "df_with_dict = df.withColumn(\n",
    "    \"person_info\", \n",
    "    to_json(person_struct_udf(col(\"name\"), col(\"age\"), col(\"city\")))\n",
    ")\n",
    "\n",
    "print(\"DataFrame with Dictionary UDF result:\")\n",
    "df_with_dict.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approach: Direct JSON String\n",
    "\n",
    "Another approach is to return a JSON string directly from the UDF instead of using the `to_json` function. This is often simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with JSON UDF result:\n",
      "+---+-------+---+-------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|id |name   |age|city         |salary  |person_info                                                                                   |\n",
      "+---+-------+---+-------------+--------+----------------------------------------------------------------------------------------------+\n",
      "|1  |John   |32 |New York     |75000.0 |{\"full_name\": \"John\", \"age_in_years\": 32, \"residence\": \"New York\", \"age_group\": \"middle\"}     |\n",
      "|2  |Alice  |28 |San Francisco|95000.0 |{\"full_name\": \"Alice\", \"age_in_years\": 28, \"residence\": \"San Francisco\", \"age_group\": \"young\"}|\n",
      "|3  |Bob    |45 |Chicago      |85000.0 |{\"full_name\": \"Bob\", \"age_in_years\": 45, \"residence\": \"Chicago\", \"age_group\": \"middle\"}       |\n",
      "|4  |Emma   |36 |Seattle      |92000.0 |{\"full_name\": \"Emma\", \"age_in_years\": 36, \"residence\": \"Seattle\", \"age_group\": \"middle\"}      |\n",
      "|5  |Michael|52 |Boston       |120000.0|{\"full_name\": \"Michael\", \"age_in_years\": 52, \"residence\": \"Boston\", \"age_group\": \"senior\"}    |\n",
      "+---+-------+---+-------------+--------+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Better approach: Return a JSON string directly from the UDF\n",
    "import json\n",
    "\n",
    "def create_person_json(name, age, city):\n",
    "    \"\"\"\n",
    "    Create a dictionary and convert it to a JSON string\n",
    "    \"\"\"\n",
    "    person_dict = {\n",
    "        \"full_name\": name,\n",
    "        \"age_in_years\": age,\n",
    "        \"residence\": city,\n",
    "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
    "    }\n",
    "    return json.dumps(person_dict)\n",
    "\n",
    "# Register UDF with return type as string\n",
    "person_json_udf = udf(create_person_json, StringType())\n",
    "\n",
    "# Apply the UDF\n",
    "df_with_json = df.withColumn(\n",
    "    \"person_info\", \n",
    "    person_json_udf(col(\"name\"), col(\"age\"), col(\"city\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame with JSON UDF result:\")\n",
    "df_with_json.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parsing the JSON Back to a Structured Format\n",
    "\n",
    "Now, let's parse the JSON string back to a structured column using `from_json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with parsed JSON fields:\n",
      "+---+--------+---------+------------+-------------+---------+\n",
      "| id|  salary|full_name|age_in_years|    residence|age_group|\n",
      "+---+--------+---------+------------+-------------+---------+\n",
      "|  1| 75000.0|     John|          32|     New York|   middle|\n",
      "|  2| 95000.0|    Alice|          28|San Francisco|    young|\n",
      "|  3| 85000.0|      Bob|          45|      Chicago|   middle|\n",
      "|  4| 92000.0|     Emma|          36|      Seattle|   middle|\n",
      "|  5|120000.0|  Michael|          52|       Boston|   senior|\n",
      "+---+--------+---------+------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for the person_info JSON\n",
    "person_schema = StructType([\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"age_in_years\", IntegerType(), True),\n",
    "    StructField(\"residence\", StringType(), True),\n",
    "    StructField(\"age_group\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the JSON string back to a struct\n",
    "df_parsed = df_with_json.withColumn(\n",
    "    \"person_struct\",\n",
    "    from_json(col(\"person_info\"), person_schema)\n",
    ")\n",
    "\n",
    "# Access fields from the parsed struct\n",
    "df_parsed_fields = df_parsed.select(\n",
    "    \"id\",\n",
    "    \"salary\",\n",
    "    \"person_struct.full_name\",\n",
    "    \"person_struct.age_in_years\",\n",
    "    \"person_struct.residence\",\n",
    "    \"person_struct.age_group\"\n",
    ")\n",
    "\n",
    "print(\"DataFrame with parsed JSON fields:\")\n",
    "df_parsed_fields.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complex UDF with Nested Dictionaries\n",
    "\n",
    "Let's create a more complex UDF that returns nested dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with complex nested dictionary:\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |profile                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |{\"personal\": {\"id\": 1, \"name\": \"John\", \"age\": 32, \"location\": {\"city\": \"New York\", \"country\": \"USA\", \"is_metro\": true}}, \"financial\": {\"income\": {\"gross_salary\": 75000.0, \"tax_rate\": 0.2, \"take_home\": 60000.0}, \"retirement\": {\"years_to_retirement\": 33, \"retirement_age\": 65}}, \"summary\": \"John from New York, age 32, earning $75000.00\"}            |\n",
      "|2  |{\"personal\": {\"id\": 2, \"name\": \"Alice\", \"age\": 28, \"location\": {\"city\": \"San Francisco\", \"country\": \"USA\", \"is_metro\": true}}, \"financial\": {\"income\": {\"gross_salary\": 95000.0, \"tax_rate\": 0.3, \"take_home\": 66500.0}, \"retirement\": {\"years_to_retirement\": 37, \"retirement_age\": 65}}, \"summary\": \"Alice from San Francisco, age 28, earning $95000.00\"}|\n",
      "|3  |{\"personal\": {\"id\": 3, \"name\": \"Bob\", \"age\": 45, \"location\": {\"city\": \"Chicago\", \"country\": \"USA\", \"is_metro\": true}}, \"financial\": {\"income\": {\"gross_salary\": 85000.0, \"tax_rate\": 0.3, \"take_home\": 59499.99999999999}, \"retirement\": {\"years_to_retirement\": 20, \"retirement_age\": 65}}, \"summary\": \"Bob from Chicago, age 45, earning $85000.00\"}      |\n",
      "|4  |{\"personal\": {\"id\": 4, \"name\": \"Emma\", \"age\": 36, \"location\": {\"city\": \"Seattle\", \"country\": \"USA\", \"is_metro\": false}}, \"financial\": {\"income\": {\"gross_salary\": 92000.0, \"tax_rate\": 0.3, \"take_home\": 64399.99999999999}, \"retirement\": {\"years_to_retirement\": 29, \"retirement_age\": 65}}, \"summary\": \"Emma from Seattle, age 36, earning $92000.00\"}   |\n",
      "|5  |{\"personal\": {\"id\": 5, \"name\": \"Michael\", \"age\": 52, \"location\": {\"city\": \"Boston\", \"country\": \"USA\", \"is_metro\": true}}, \"financial\": {\"income\": {\"gross_salary\": 120000.0, \"tax_rate\": 0.3, \"take_home\": 84000.0}, \"retirement\": {\"years_to_retirement\": 13, \"retirement_age\": 65}}, \"summary\": \"Michael from Boston, age 52, earning $120000.00\"}        |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_complex_profile(id, name, age, city, salary):\n",
    "    \"\"\"\n",
    "    Create a complex dictionary with nested structures\n",
    "    \"\"\"\n",
    "    # Calculate some derived values\n",
    "    tax_rate = 0.20 if salary < 85000 else 0.30\n",
    "    take_home = salary * (1 - tax_rate)\n",
    "    retirement_years = 65 - age\n",
    "    \n",
    "    # Create nested dictionary\n",
    "    profile = {\n",
    "        \"personal\": {\n",
    "            \"id\": id,\n",
    "            \"name\": name,\n",
    "            \"age\": age,\n",
    "            \"location\": {\n",
    "                \"city\": city,\n",
    "                \"country\": \"USA\",  # Assuming USA for all\n",
    "                \"is_metro\": city in [\"New York\", \"San Francisco\", \"Chicago\", \"Boston\"]\n",
    "            }\n",
    "        },\n",
    "        \"financial\": {\n",
    "            \"income\": {\n",
    "                \"gross_salary\": salary,\n",
    "                \"tax_rate\": tax_rate,\n",
    "                \"take_home\": take_home\n",
    "            },\n",
    "            \"retirement\": {\n",
    "                \"years_to_retirement\": retirement_years,\n",
    "                \"retirement_age\": 65\n",
    "            }\n",
    "        },\n",
    "        \"summary\": f\"{name} from {city}, age {age}, earning ${salary:.2f}\"\n",
    "    }\n",
    "    \n",
    "    return json.dumps(profile)\n",
    "\n",
    "# Register UDF\n",
    "complex_profile_udf = udf(create_complex_profile, StringType())\n",
    "\n",
    "# Apply the UDF\n",
    "df_complex = df.withColumn(\n",
    "    \"profile\",\n",
    "    complex_profile_udf(\n",
    "        col(\"id\"), col(\"name\"), col(\"age\"), col(\"city\"), col(\"salary\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"DataFrame with complex nested dictionary:\")\n",
    "df_complex.select(\"id\", \"profile\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse Complex Nested Structure\n",
    "\n",
    "Now, let's define a schema for our complex structure and parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of parsed profile:\n",
      "root\n",
      " |-- parsed_profile: struct (nullable = true)\n",
      " |    |-- personal: struct (nullable = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- age: integer (nullable = true)\n",
      " |    |    |-- location: struct (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- is_metro: boolean (nullable = true)\n",
      " |    |-- financial: struct (nullable = true)\n",
      " |    |    |-- income: struct (nullable = true)\n",
      " |    |    |    |-- gross_salary: double (nullable = true)\n",
      " |    |    |    |-- tax_rate: double (nullable = true)\n",
      " |    |    |    |-- take_home: double (nullable = true)\n",
      " |    |    |-- retirement: struct (nullable = true)\n",
      " |    |    |    |-- years_to_retirement: integer (nullable = true)\n",
      " |    |    |    |-- retirement_age: integer (nullable = true)\n",
      " |    |-- summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for the complex profile\n",
    "location_schema = StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"is_metro\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "personal_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"location\", location_schema, True)\n",
    "])\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"gross_salary\", DoubleType(), True),\n",
    "    StructField(\"tax_rate\", DoubleType(), True),\n",
    "    StructField(\"take_home\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "retirement_schema = StructType([\n",
    "    StructField(\"years_to_retirement\", IntegerType(), True),\n",
    "    StructField(\"retirement_age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "financial_schema = StructType([\n",
    "    StructField(\"income\", income_schema, True),\n",
    "    StructField(\"retirement\", retirement_schema, True)\n",
    "])\n",
    "\n",
    "profile_schema = StructType([\n",
    "    StructField(\"personal\", personal_schema, True),\n",
    "    StructField(\"financial\", financial_schema, True),\n",
    "    StructField(\"summary\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the complex JSON\n",
    "df_complex_parsed = df_complex.withColumn(\n",
    "    \"parsed_profile\",\n",
    "    from_json(col(\"profile\"), profile_schema)\n",
    ")\n",
    "\n",
    "# Show schema of the parsed profile\n",
    "print(\"Schema of parsed profile:\")\n",
    "df_complex_parsed.select(\"parsed_profile\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Specific Fields from Nested Structure\n",
    "\n",
    "Now let's extract and work with specific fields from our complex nested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted fields from nested structure:\n",
      "+-------+-------------+--------+------------+-----------------+-------------------+---------------------------------------------------+\n",
      "|name   |city         |is_metro|gross_salary|take_home        |years_to_retirement|summary                                            |\n",
      "+-------+-------------+--------+------------+-----------------+-------------------+---------------------------------------------------+\n",
      "|John   |New York     |true    |75000.0     |60000.0          |33                 |John from New York, age 32, earning $75000.00      |\n",
      "|Alice  |San Francisco|true    |95000.0     |66500.0          |37                 |Alice from San Francisco, age 28, earning $95000.00|\n",
      "|Bob    |Chicago      |true    |85000.0     |59499.99999999999|20                 |Bob from Chicago, age 45, earning $85000.00        |\n",
      "|Emma   |Seattle      |false   |92000.0     |64399.99999999999|29                 |Emma from Seattle, age 36, earning $92000.00       |\n",
      "|Michael|Boston       |true    |120000.0    |84000.0          |13                 |Michael from Boston, age 52, earning $120000.00    |\n",
      "+-------+-------------+--------+------------+-----------------+-------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract specific fields\n",
    "df_extracted = df_complex_parsed.select(\n",
    "    \"parsed_profile.personal.name\",\n",
    "    \"parsed_profile.personal.location.city\",\n",
    "    \"parsed_profile.personal.location.is_metro\",\n",
    "    \"parsed_profile.financial.income.gross_salary\",\n",
    "    \"parsed_profile.financial.income.take_home\",\n",
    "    \"parsed_profile.financial.retirement.years_to_retirement\",\n",
    "    \"parsed_profile.summary\"\n",
    ")\n",
    "\n",
    "print(\"Extracted fields from nested structure:\")\n",
    "df_extracted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating a MapType UDF\n",
    "\n",
    "Another approach is to use a MapType return type instead of converting to/from JSON. However, this has limitations on the types of values that can be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with MapType result:\n",
      "+---+-------+---+-------------+--------+-------------------------------------------------+\n",
      "|id |name   |age|city         |salary  |person_map                                       |\n",
      "+---+-------+---+-------------+--------+-------------------------------------------------+\n",
      "|1  |John   |32 |New York     |75000.0 |{name -> John, age_group -> middle, age -> 32}   |\n",
      "|2  |Alice  |28 |San Francisco|95000.0 |{name -> Alice, age_group -> young, age -> 28}   |\n",
      "|3  |Bob    |45 |Chicago      |85000.0 |{name -> Bob, age_group -> middle, age -> 45}    |\n",
      "|4  |Emma   |36 |Seattle      |92000.0 |{name -> Emma, age_group -> middle, age -> 36}   |\n",
      "|5  |Michael|52 |Boston       |120000.0|{name -> Michael, age_group -> senior, age -> 52}|\n",
      "+---+-------+---+-------------+--------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF that returns a map (key-value pairs)\n",
    "def create_person_map(name, age):\n",
    "    \"\"\"\n",
    "    Create a map of person attributes\n",
    "    Note: All values must be of the same type when using MapType\n",
    "    \"\"\"\n",
    "    # Convert all values to strings for consistency\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"age\": str(age),\n",
    "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
    "    }\n",
    "\n",
    "# Register UDF with MapType return type\n",
    "# Note: Both keys and values must be of uniform types\n",
    "person_map_udf = udf(create_person_map, MapType(StringType(), StringType()))\n",
    "\n",
    "# Apply the UDF\n",
    "df_with_map = df.withColumn(\n",
    "    \"person_map\",\n",
    "    person_map_udf(col(\"name\"), col(\"age\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame with MapType result:\")\n",
    "df_with_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access individual values from the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing specific keys from map:\n",
      "+---+-------+---+--------+-------------+\n",
      "| id|   name|age|map_name|map_age_group|\n",
      "+---+-------+---+--------+-------------+\n",
      "|  1|   John| 32|    John|       middle|\n",
      "|  2|  Alice| 28|   Alice|        young|\n",
      "|  3|    Bob| 45|     Bob|       middle|\n",
      "|  4|   Emma| 36|    Emma|       middle|\n",
      "|  5|Michael| 52| Michael|       senior|\n",
      "+---+-------+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access a specific key from the map\n",
    "df_map_access = df_with_map.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    col(\"person_map\")[\"name\"].alias(\"map_name\"),\n",
    "    col(\"person_map\")[\"age_group\"].alias(\"map_age_group\")\n",
    ")\n",
    "\n",
    "print(\"Accessing specific keys from map:\")\n",
    "df_map_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison\n",
    "\n",
    "Let's look at the performance implications of these different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:================================================>    (354 + 12) / 384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created larger DataFrame with 160 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create a larger DataFrame for performance testing\n",
    "from pyspark.sql.functions import lit, rand, monotonically_increasing_id\n",
    "import time\n",
    "\n",
    "# Multiply our DataFrame to create more rows\n",
    "df_large = df.union(df).union(df).union(df)  # 20 rows\n",
    "for i in range(3):  # Multiple more times to get ~160 rows\n",
    "    df_large = df_large.union(df_large)\n",
    "\n",
    "# Add some random variation\n",
    "df_large = df_large.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
    "df_large = df_large.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "print(f\"Created larger DataFrame with {df_large.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:================================================>    (348 + 12) / 384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created larger DataFrame with 160 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Fix error with previous cell\n",
    "df_large = df.union(df).union(df).union(df)  # 20 rows\n",
    "for i in range(3):  # Multiple more times to get ~160 rows\n",
    "    df_large = df_large.union(df_large)\n",
    "\n",
    "# Add some random variation\n",
    "df_large = df_large.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
    "df_large = df_large.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "print(f\"Created larger DataFrame with {df_large.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:============================================>        (326 + 12) / 384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for JSON string approach: 2.70 seconds\n",
      "Time for MapType approach: 2.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 06:52:28 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-268a8dc4-280e-44f9-81ab-00bc1f25f6ab. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-268a8dc4-280e-44f9-81ab-00bc1f25f6ab\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Test JSON string approach\n",
    "start_time = time.time()\n",
    "df_large_json = df_large.withColumn(\n",
    "    \"profile\",\n",
    "    complex_profile_udf(\n",
    "        col(\"id\"), col(\"name\"), col(\"age\"), col(\"city\"), col(\"salary\")\n",
    "    )\n",
    ")\n",
    "df_large_json.select(\"id\", \"profile\").count()  # Force execution\n",
    "json_time = time.time() - start_time\n",
    "\n",
    "# Test MapType approach\n",
    "start_time = time.time()\n",
    "df_large_map = df_large.withColumn(\n",
    "    \"person_map\",\n",
    "    person_map_udf(col(\"name\"), col(\"age\"))\n",
    ")\n",
    "df_large_map.select(\"id\", \"person_map\").count()  # Force execution\n",
    "map_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time for JSON string approach: {json_time:.2f} seconds\")\n",
    "print(f\"Time for MapType approach: {map_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "Based on our exploration, here are some best practices for working with dictionary-based UDFs in PySpark:\n",
    "\n",
    "1. **For simple dictionaries with homogeneous value types:**\n",
    "   - Use `MapType` for better performance and direct access\n",
    "   - Ensure all values are of the same type\n",
    "\n",
    "2. **For complex nested dictionaries with heterogeneous types:**\n",
    "   - Convert to JSON strings in the UDF\n",
    "   - Use `from_json` with explicit schema to parse\n",
    "   - Leverage dot notation to access nested fields\n",
    "\n",
    "3. **Performance considerations:**\n",
    "   - JSON serialization/deserialization adds overhead\n",
    "   - Consider using Pandas UDFs for better performance with large datasets\n",
    "   - If possible, restructure your data model to avoid complex nested structures\n",
    "\n",
    "4. **Schema Management:**\n",
    "   - Always define explicit schemas for JSON parsing\n",
    "   - Use appropriate data types to avoid later conversions\n",
    "   - Document your schema structure for future reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
