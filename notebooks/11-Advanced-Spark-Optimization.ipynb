{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spark Query Plan Analysis and Optimization\n",
    "\n",
    "This notebook provides an in-depth exploration of Spark's query plans and advanced optimization techniques, building on top of basic optimizations. We'll analyze logical and physical plans in detail to identify performance bottlenecks and apply targeted optimizations.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Advanced Query Plan Analysis\n",
    "2. Setting Up the Environment\n",
    "3. Deep Dive into Logical and Physical Plans\n",
    "   - Analyzing Scan Operations\n",
    "   - Understanding Filter and Projection Pushdown\n",
    "   - Detailed Join Strategy Analysis\n",
    "   - Optimizing Shuffle Operations\n",
    "   - Partition Tuning for Performance\n",
    "   - Broadcast Operations and Memory Management\n",
    "   - Aggregation Optimization Techniques\n",
    "   - Spill to Disk Detection and Prevention\n",
    "   - Skew Detection and Handling\n",
    "   - Whole-Stage Codegen Analysis\n",
    "4. Adaptive Query Execution Deep Dive\n",
    "5. Cost-Based Optimization in Spark\n",
    "6. Advanced Performance Tuning Configurations\n",
    "7. Plan Metrics Analysis and Optimization\n",
    "8. Real-world Examples and Solutions\n",
    "\n",
    "Let's start by understanding what makes query plan analysis essential for advanced Spark optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Advanced Query Plan Analysis\n",
    "\n",
    "While basic optimization strategies can significantly improve Spark performance, advanced optimization requires deep understanding of how Spark transforms queries into executable code. By analyzing both logical and physical plans, we can identify inefficiencies that aren't obvious from the DataFrame API or SQL interface.\n",
    "\n",
    "Key components of advanced plan analysis include:\n",
    "\n",
    "- **Logical Plan Transformation Rules**: Understanding how Catalyst applies rules to optimize logical plans\n",
    "- **Physical Plan Selection Criteria**: How Spark decides which physical strategies to use\n",
    "- **Cost Model Analysis**: How statistics influence plan decisions\n",
    "- **Runtime Adaptations**: How plans are modified during execution\n",
    "- **Performance Bottleneck Identification**: Finding specific operations that limit performance\n",
    "- **Resource Usage Patterns**: Understanding memory, CPU, and I/O patterns\n",
    "\n",
    "The `explain()` method with extended modes and additional utilities in the `ExplainCommand` provide the tools needed for this deep analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "Let's set up our environment and create test datasets for our advanced analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 10:11:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Session initialized with detailed configuration!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, sum, max, min, lit, concat, broadcast, expr, \n",
    "    when, coalesce, array, explode, struct, to_json, from_json, \n",
    "    window, row_number, rank, dense_rank, ntile, lead, lag,\n",
    "    udf, pandas_udf, PandasUDFType\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, \n",
    "    DateType, TimestampType, ArrayType, MapType, BooleanType\n",
    ")\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session with detailed configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Spark Optimization\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable us to view full query plans\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")  # Initially disabled for clearer analysis\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")  # Enable whole-stage codegen\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Session initialized with detailed configuration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to time execution and get metrics\n",
    "def time_execution_with_metrics(df, action=\"count\", name=\"query\"):\n",
    "    \"\"\"Time and collect metrics for a DataFrame action\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    if action == \"count\":\n",
    "        result = df.count()\n",
    "    elif action == \"collect\":\n",
    "        result = df.collect()\n",
    "    elif action == \"show\":\n",
    "        result = df.show(n=10, truncate=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported action: {action}\")\n",
    "        \n",
    "    execution_time = time.time() - start\n",
    "    \n",
    "    print(f\"{name} execution time: {execution_time:.4f} seconds\")\n",
    "    return execution_time, result\n",
    "\n",
    "# Create a function to show both logical and physical plans with different detail levels\n",
    "def analyze_plans(df, name=\"Query\"):\n",
    "    \"\"\"Show logical and physical plans at different detail levels\"\"\"\n",
    "    print(f\"\\n{'='*20} {name} Plan Analysis {'='*20}\\n\")\n",
    "    \n",
    "    print(\"--- Logical Plan (Parsed) ---\")\n",
    "    df._jdf.queryExecution().logical().toJSON()\n",
    "    df._jdf.queryExecution().logical().toString()\n",
    "    \n",
    "    print(\"\\n--- Logical Plan (Analyzed) ---\")\n",
    "    df._jdf.queryExecution().analyzed().toString()\n",
    "    \n",
    "    print(\"\\n--- Logical Plan (Optimized) ---\")\n",
    "    df._jdf.queryExecution().optimizedPlan().toString()\n",
    "    \n",
    "    print(\"\\n--- Physical Plan ---\")\n",
    "    df.explain()\n",
    "    \n",
    "    print(\"\\n--- Detailed Physical Plan ---\")\n",
    "    df.explain(mode=\"extended\")\n",
    "    \n",
    "    print(\"\\n--- Cost Analysis ---\")\n",
    "    df.explain(mode=\"cost\")\n",
    "    \n",
    "    print(\"\\n--- Codegen Details ---\")\n",
    "    df.explain(mode=\"codegen\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a complex dataset with various data characteristics that will allow us to explore advanced optimization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created products dimension table with 5000 rows\n"
     ]
    }
   ],
   "source": [
    "# Create a larger, more complex dataset with multiple tables\n",
    "# 1. Fact table (transactions) with millions of rows\n",
    "# 2. Multiple dimension tables with different sizes\n",
    "# 3. Complex data types (arrays, maps, structs)\n",
    "# 4. Skewed data distributions\n",
    "# 5. Partitioned and bucketed tables\n",
    "\n",
    "# First, let's create dimension tables\n",
    "\n",
    "# Products dimension\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"category_id\", IntegerType(), False),\n",
    "    StructField(\"subcategory_id\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), False),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"created_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Generate 5,000 products\n",
    "product_data = []\n",
    "product_names = [\"Laptop\", \"Phone\", \"Tablet\", \"TV\", \"Camera\", \"Headphones\", \"Speaker\", \"Watch\", \"Keyboard\", \"Mouse\"]\n",
    "product_prefixes = [\"Pro\", \"Elite\", \"Ultra\", \"Mini\", \"Max\", \"Lite\", \"Premium\", \"Basic\", \"Standard\", \"Advanced\"]\n",
    "product_suffixes = [\"2023\", \"Plus\", \"XL\", \"SE\", \"X\", \"S\", \"Air\", \"Book\", \"Pad\", \"Vision\"]\n",
    "\n",
    "categories = 10\n",
    "subcategories = 5  # per category\n",
    "\n",
    "# Tag options\n",
    "all_tags = [\"new\", \"bestseller\", \"sale\", \"clearance\", \"limited\", \"exclusive\", \"featured\", \"discontinued\", \"popular\", \"trending\"]\n",
    "\n",
    "# Attribute options\n",
    "attribute_keys = [\"color\", \"size\", \"weight\", \"material\", \"connectivity\", \"power\", \"warranty\", \"origin\"]\n",
    "attribute_values = {\n",
    "    \"color\": [\"black\", \"white\", \"silver\", \"gold\", \"blue\", \"red\", \"green\"],\n",
    "    \"size\": [\"small\", \"medium\", \"large\", \"XL\", \"compact\"],\n",
    "    \"weight\": [\"light\", \"medium\", \"heavy\", \"ultra-light\"],\n",
    "    \"material\": [\"plastic\", \"metal\", \"glass\", \"aluminum\", \"carbon fiber\"],\n",
    "    \"connectivity\": [\"bluetooth\", \"wifi\", \"wired\", \"usb-c\", \"lightning\"],\n",
    "    \"power\": [\"battery\", \"plug-in\", \"solar\", \"hybrid\"],\n",
    "    \"warranty\": [\"1-year\", \"2-year\", \"lifetime\", \"extended\"],\n",
    "    \"origin\": [\"USA\", \"China\", \"Japan\", \"Korea\", \"Germany\"]\n",
    "}\n",
    "\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime.now()\n",
    "date_range = (end_date - start_date).days\n",
    "\n",
    "for i in range(1, 5001):\n",
    "    # Create skewed distribution for categories (more products in lower categories)\n",
    "    category_id = int(random.paretovariate(1.5))\n",
    "    if category_id > categories:\n",
    "        category_id = categories\n",
    "    subcategory_id = random.randint(1, subcategories) if random.random() > 0.1 else None\n",
    "    \n",
    "    # Generate product name\n",
    "    name = f\"{random.choice(product_prefixes)} {random.choice(product_names)} {random.choice(product_suffixes)}\"\n",
    "    \n",
    "    # Generate price with some skew\n",
    "    if category_id <= 3:  # Higher priced categories\n",
    "        price = round(random.uniform(500, 2000), 2)\n",
    "    else:\n",
    "        price = round(random.uniform(10, 500), 2)\n",
    "    \n",
    "    # Generate random tags (0-5 tags)\n",
    "    num_tags = random.randint(0, 5)\n",
    "    tags = random.sample(all_tags, num_tags) if num_tags > 0 else None\n",
    "    \n",
    "    # Generate random attributes (0-5 attributes)\n",
    "    num_attrs = random.randint(0, 5)\n",
    "    if num_attrs > 0:\n",
    "        selected_keys = random.sample(attribute_keys, num_attrs)\n",
    "        attributes = {k: random.choice(attribute_values[k]) for k in selected_keys}\n",
    "    else:\n",
    "        attributes = None\n",
    "    \n",
    "    # Generate creation date\n",
    "    random_days = random.randint(0, date_range)\n",
    "    created_at = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    product_data.append((i, name, category_id, subcategory_id, price, attributes, tags, created_at))\n",
    "\n",
    "products_df = spark.createDataFrame(product_data, product_schema)\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Save as a permanent table for later use\n",
    "products_df.write.mode(\"overwrite\").saveAsTable(\"products_table\")\n",
    "\n",
    "print(f\"Created products dimension table with {products_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created customers dimension table with 50000 rows\n"
     ]
    }
   ],
   "source": [
    "# Create customers dimension table\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), False),\n",
    "        StructField(\"state\", StringType(), False),\n",
    "        StructField(\"zip\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), False)\n",
    "    ]), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), False),\n",
    "    StructField(\"last_activity\", TimestampType(), True),\n",
    "    StructField(\"tier\", StringType(), False),\n",
    "    StructField(\"preferences\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Generate 50,000 customers\n",
    "customer_data = []\n",
    "first_names = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \"William\", \"Elizabeth\"]\n",
    "last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\", \"Garcia\", \"Rodriguez\", \"Wilson\"]\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\"]\n",
    "states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\", \"PA\", \"TX\", \"CA\", \"TX\", \"CA\"]\n",
    "countries = [\"USA\" for _ in range(10)]\n",
    "tiers = [\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]\n",
    "tier_weights = [0.5, 0.3, 0.15, 0.05]  # Distribution weights\n",
    "\n",
    "preference_keys = [\"communication\", \"payment\", \"shipping\", \"recommendations\", \"notifications\"]\n",
    "preference_values = {\n",
    "    \"communication\": [\"email\", \"sms\", \"phone\", \"mail\"],\n",
    "    \"payment\": [\"credit\", \"debit\", \"paypal\", \"crypto\", \"bank_transfer\"],\n",
    "    \"shipping\": [\"standard\", \"express\", \"overnight\", \"pickup\"],\n",
    "    \"recommendations\": [\"enabled\", \"disabled\"],\n",
    "    \"notifications\": [\"high\", \"medium\", \"low\", \"none\"]\n",
    "}\n",
    "\n",
    "for i in range(1, 50001):\n",
    "    # Name and email\n",
    "    first = random.choice(first_names)\n",
    "    last = random.choice(last_names)\n",
    "    name = f\"{first} {last}\"\n",
    "    email = f\"{first.lower()}.{last.lower()}@example.com\" if random.random() > 0.1 else None\n",
    "    \n",
    "    # Address\n",
    "    if random.random() > 0.05:  # 5% have no address\n",
    "        street = f\"{random.randint(100, 9999)} Main St\" if random.random() > 0.1 else None\n",
    "        city_idx = random.randint(0, 9)\n",
    "        city = cities[city_idx]\n",
    "        state = states[city_idx]\n",
    "        zip_code = f\"{random.randint(10000, 99999)}\" if random.random() > 0.1 else None\n",
    "        country = countries[city_idx]\n",
    "        address = (street, city, state, zip_code, country)\n",
    "    else:\n",
    "        address = None\n",
    "    \n",
    "    # Phone\n",
    "    phone = f\"{random.randint(100, 999)}-{random.randint(100, 999)}-{random.randint(1000, 9999)}\" if random.random() > 0.2 else None\n",
    "    \n",
    "    # Dates\n",
    "    signup_days = random.randint(0, date_range)\n",
    "    signup_date = start_date.date() + timedelta(days=signup_days)\n",
    "    \n",
    "    if random.random() > 0.1:  # 10% have no activity\n",
    "        activity_days = random.randint(0, date_range - signup_days)  # Activity after signup\n",
    "        last_activity = start_date + timedelta(days=signup_days + activity_days)\n",
    "    else:\n",
    "        last_activity = None\n",
    "    \n",
    "    # Tier (weighted selection)\n",
    "    tier = random.choices(tiers, weights=tier_weights)[0]\n",
    "    \n",
    "    # Preferences\n",
    "    if random.random() > 0.3:  # 30% have no preferences\n",
    "        num_prefs = random.randint(1, len(preference_keys))\n",
    "        selected_prefs = random.sample(preference_keys, num_prefs)\n",
    "        preferences = {k: random.choice(preference_values[k]) for k in selected_prefs}\n",
    "    else:\n",
    "        preferences = None\n",
    "        \n",
    "    customer_data.append((i, name, email, address, phone, signup_date, last_activity, tier, preferences))\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# Save as a permanent table for later use\n",
    "customers_df.write.mode(\"overwrite\").saveAsTable(\"customers_table\")\n",
    "\n",
    "print(f\"Created customers dimension table with {customers_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created transactions fact table with 100000 rows\n"
     ]
    }
   ],
   "source": [
    "# Create a fact table (transactions) with highly skewed data\n",
    "# Import Python's built-in sum function\n",
    "from builtins import sum as py_sum\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"transaction_date\", TimestampType(), False),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"product_id\", IntegerType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"discount\", DoubleType(), True)\n",
    "    ])), False),\n",
    "    StructField(\"payment_method\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"total_amount\", DoubleType(), False),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Generate a smaller sample of 100,000 transactions for notebook performance\n",
    "transaction_data = []\n",
    "payment_methods = [\"credit_card\", \"debit_card\", \"paypal\", \"apple_pay\", \"bank_transfer\", \"gift_card\", \"crypto\"]\n",
    "payment_weights = [0.4, 0.3, 0.1, 0.1, 0.05, 0.04, 0.01]  # Distribution weights\n",
    "statuses = [\"completed\", \"shipped\", \"delivered\", \"cancelled\", \"refunded\", \"processing\"]\n",
    "status_weights = [0.6, 0.2, 0.1, 0.05, 0.03, 0.02]  # Distribution weights\n",
    "store_count = 50\n",
    "\n",
    "metadata_keys = [\"source\", \"device\", \"coupon_code\", \"promotion\", \"referral\"]\n",
    "metadata_values = {\n",
    "    \"source\": [\"web\", \"mobile_app\", \"store\", \"phone\", \"partner\"],\n",
    "    \"device\": [\"desktop\", \"mobile\", \"tablet\", \"kiosk\", \"pos\"],\n",
    "    \"coupon_code\": [\"SAVE10\", \"WELCOME20\", \"FLASH30\", \"SEASON25\", None],\n",
    "    \"promotion\": [\"holiday_sale\", \"clearance\", \"new_customer\", \"loyalty\", None],\n",
    "    \"referral\": [\"friend\", \"search\", \"social\", \"email\", None]\n",
    "}\n",
    "\n",
    "# Create some skewed distributions\n",
    "# 1. Few customers make many purchases (80/20 rule)\n",
    "# 2. Some products are much more popular than others\n",
    "# 3. Some days have much higher transaction volumes\n",
    "\n",
    "# Calculate weighted customer IDs (power law distribution)\n",
    "customer_weights = [1/(i**0.8) for i in range(1, 50001)]\n",
    "# Then use it instead of the PySpark sum\n",
    "total_weight = py_sum(customer_weights)\n",
    "customer_weights = [w/total_weight for w in customer_weights]\n",
    "\n",
    "# Calculate weighted product IDs (power law distribution)\n",
    "product_weights = [1/(i**0.9) for i in range(1, 5001)]\n",
    "# Then use it instead of the PySpark sum\n",
    "total_weight = py_sum(product_weights)\n",
    "product_weights = [w/total_weight for w in product_weights]\n",
    "\n",
    "# Generate transactions\n",
    "for i in range(1, 100001):\n",
    "    # Select customer with power law distribution\n",
    "    customer_id = int(np.random.choice(range(1, 50001), p=customer_weights))\n",
    "    \n",
    "    # Generate transaction date with seasonal patterns\n",
    "    # More transactions during holidays and weekends\n",
    "    random_days = random.randint(0, date_range)\n",
    "    base_date = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    # Adjust for seasonal patterns\n",
    "    month = base_date.month\n",
    "    day_of_week = base_date.weekday()\n",
    "    \n",
    "    # Boost December (holiday season)\n",
    "    if month == 12 and random.random() < 0.6:\n",
    "        # Shift to December\n",
    "        holiday_year = random.choice([2020, 2021, 2022])\n",
    "        base_date = datetime(holiday_year, 12, random.randint(1, 31))\n",
    "    \n",
    "    # Boost weekends\n",
    "    if day_of_week < 5 and random.random() < 0.4:  # Weekday\n",
    "        # Shift to a weekend\n",
    "        weekend_offset = random.choice([5, 6]) - day_of_week  # Shift to Saturday or Sunday\n",
    "        base_date = base_date + timedelta(days=weekend_offset)\n",
    "    \n",
    "    transaction_date = base_date\n",
    "    \n",
    "    # Generate 1-5 items per transaction\n",
    "    num_items = random.choices([1, 2, 3, 4, 5], weights=[0.5, 0.25, 0.15, 0.07, 0.03])[0]\n",
    "    items = []\n",
    "    \n",
    "    total_amount = 0.0\n",
    "    for _ in range(num_items):\n",
    "        # Select product with power law distribution\n",
    "        product_id = int(np.random.choice(range(1, 5001), p=product_weights))\n",
    "        quantity = random.randint(1, 5)\n",
    "        \n",
    "        # Price depends on product category\n",
    "        if product_id <= 1000:  # High-priced items (ids 1-1000)\n",
    "            price = round(random.uniform(500, 2000), 2)\n",
    "        else:\n",
    "            price = round(random.uniform(10, 500), 2)\n",
    "        \n",
    "        # Apply discount sometimes\n",
    "        discount = round(price * random.uniform(0.05, 0.3), 2) if random.random() < 0.3 else None\n",
    "        item_total = price * quantity\n",
    "        if discount:\n",
    "            item_total -= discount * quantity\n",
    "            \n",
    "        total_amount += item_total\n",
    "        items.append((product_id, quantity, price, discount))\n",
    "    \n",
    "    # Select payment method and status with weighted distribution\n",
    "    payment_method = random.choices(payment_methods, weights=payment_weights)[0]\n",
    "    status = random.choices(statuses, weights=status_weights)[0]\n",
    "    \n",
    "    # Store ID (NULL for online orders)\n",
    "    store_id = random.randint(1, store_count) if random.random() < 0.3 else None\n",
    "    \n",
    "    # Transaction metadata\n",
    "    if random.random() > 0.2:  # 20% have no metadata\n",
    "        num_meta = random.randint(1, 3)\n",
    "        selected_meta = random.sample(metadata_keys, num_meta)\n",
    "        metadata = {k: random.choice(metadata_values[k]) for k in selected_meta if metadata_values[k][-1] is not None or random.random() > 0.2}\n",
    "    else:\n",
    "        metadata = None\n",
    "    \n",
    "    # Add transaction to dataset\n",
    "    transaction_data.append((\n",
    "        i, customer_id, transaction_date, items, payment_method, status, \n",
    "        total_amount, store_id, metadata, transaction_date.year, transaction_date.month\n",
    "    ))\n",
    "\n",
    "transactions_df = spark.createDataFrame(transaction_data, transaction_schema)\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# Save as partitioned table by year and month for partition pruning demos\n",
    "transactions_df.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").saveAsTable(\"transactions_table\")\n",
    "\n",
    "print(f\"Created transactions fact table with {transactions_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Dive into Logical and Physical Plans\n",
    "\n",
    "Now that we have our test datasets ready, let's explore the various aspects of Spark's query planning and optimization in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Analyzing Scan Operations\n",
    "\n",
    "Scan operations determine how Spark reads data from sources and are often the first point of optimization. Let's explore different types of scans and their performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Full Table Scan Plan Analysis ====================\n",
      "\n",
      "--- Logical Plan (Parsed) ---\n",
      "\n",
      "--- Logical Plan (Analyzed) ---\n",
      "\n",
      "--- Logical Plan (Optimized) ---\n",
      "\n",
      "--- Physical Plan ---\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "\n",
      "--- Detailed Physical Plan ---\n",
      "== Parsed Logical Plan ==\n",
      "'UnresolvedRelation [products_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: int, name: string, category_id: int, subcategory_id: int, price: double, attributes: map<string,string>, tags: array<string>, created_at: timestamp\n",
      "SubqueryAlias spark_catalog.default.products_table\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "--- Cost Analysis ---\n",
      "== Optimized Logical Plan ==\n",
      "Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet, Statistics(sizeInBytes=179.4 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "\n",
      "--- Codegen Details ---\n",
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:1082; maxConstantPoolSize:249(0.38% used); numInnerClasses:0) ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[8];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] columnartorow_mutableStateArray_4 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[3];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 024 */\n",
      "/* 025 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 96);\n",
      "/* 026 */     columnartorow_mutableStateArray_4[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 027 */     columnartorow_mutableStateArray_4[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 028 */     columnartorow_mutableStateArray_4[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 033 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 034 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 037 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 038 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 039 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 040 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 041 */       columnartorow_mutableStateArray_2[3] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(3);\n",
      "/* 042 */       columnartorow_mutableStateArray_2[4] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(4);\n",
      "/* 043 */       columnartorow_mutableStateArray_2[5] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(5);\n",
      "/* 044 */       columnartorow_mutableStateArray_2[6] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(6);\n",
      "/* 045 */       columnartorow_mutableStateArray_2[7] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(7);\n",
      "/* 046 */\n",
      "/* 047 */     }\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   protected void processNext() throws java.io.IOException {\n",
      "/* 051 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 052 */       columnartorow_nextBatch_0();\n",
      "/* 053 */     }\n",
      "/* 054 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 055 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 056 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 057 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 058 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 059 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 060 */         int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));\n",
      "/* 061 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 062 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 063 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 064 */         int columnartorow_value_2 = columnartorow_isNull_2 ? -1 : (columnartorow_mutableStateArray_2[2].getInt(columnartorow_rowIdx_0));\n",
      "/* 065 */         boolean columnartorow_isNull_3 = columnartorow_mutableStateArray_2[3].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 066 */         int columnartorow_value_3 = columnartorow_isNull_3 ? -1 : (columnartorow_mutableStateArray_2[3].getInt(columnartorow_rowIdx_0));\n",
      "/* 067 */         boolean columnartorow_isNull_4 = columnartorow_mutableStateArray_2[4].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 068 */         double columnartorow_value_4 = columnartorow_isNull_4 ? -1.0 : (columnartorow_mutableStateArray_2[4].getDouble(columnartorow_rowIdx_0));\n",
      "/* 069 */         boolean columnartorow_isNull_5 = columnartorow_mutableStateArray_2[5].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 070 */         MapData columnartorow_value_5 = columnartorow_isNull_5 ? null : (columnartorow_mutableStateArray_2[5].getMap(columnartorow_rowIdx_0));\n",
      "/* 071 */         boolean columnartorow_isNull_6 = columnartorow_mutableStateArray_2[6].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 072 */         ArrayData columnartorow_value_6 = columnartorow_isNull_6 ? null : (columnartorow_mutableStateArray_2[6].getArray(columnartorow_rowIdx_0));\n",
      "/* 073 */         boolean columnartorow_isNull_7 = columnartorow_mutableStateArray_2[7].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 074 */         long columnartorow_value_7 = columnartorow_isNull_7 ? -1L : (columnartorow_mutableStateArray_2[7].getLong(columnartorow_rowIdx_0));\n",
      "/* 075 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 076 */\n",
      "/* 077 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 078 */\n",
      "/* 079 */         if (columnartorow_isNull_0) {\n",
      "/* 080 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 081 */         } else {\n",
      "/* 082 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 083 */         }\n",
      "/* 084 */\n",
      "/* 085 */         if (columnartorow_isNull_1) {\n",
      "/* 086 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 087 */         } else {\n",
      "/* 088 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 089 */         }\n",
      "/* 090 */\n",
      "/* 091 */         if (columnartorow_isNull_2) {\n",
      "/* 092 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 093 */         } else {\n",
      "/* 094 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 095 */         }\n",
      "/* 096 */\n",
      "/* 097 */         if (columnartorow_isNull_3) {\n",
      "/* 098 */           columnartorow_mutableStateArray_3[0].setNullAt(3);\n",
      "/* 099 */         } else {\n",
      "/* 100 */           columnartorow_mutableStateArray_3[0].write(3, columnartorow_value_3);\n",
      "/* 101 */         }\n",
      "/* 102 */\n",
      "/* 103 */         if (columnartorow_isNull_4) {\n",
      "/* 104 */           columnartorow_mutableStateArray_3[0].setNullAt(4);\n",
      "/* 105 */         } else {\n",
      "/* 106 */           columnartorow_mutableStateArray_3[0].write(4, columnartorow_value_4);\n",
      "/* 107 */         }\n",
      "/* 108 */\n",
      "/* 109 */         if (columnartorow_isNull_5) {\n",
      "/* 110 */           columnartorow_mutableStateArray_3[0].setNullAt(5);\n",
      "/* 111 */         } else {\n",
      "/* 112 */           final MapData columnartorow_tmpInput_0 = columnartorow_value_5;\n",
      "/* 113 */           if (columnartorow_tmpInput_0 instanceof UnsafeMapData) {\n",
      "/* 114 */             columnartorow_mutableStateArray_3[0].write(5, (UnsafeMapData) columnartorow_tmpInput_0);\n",
      "/* 115 */           } else {\n",
      "/* 116 */             // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 117 */             // written later.\n",
      "/* 118 */             final int columnartorow_previousCursor_0 = columnartorow_mutableStateArray_3[0].cursor();\n",
      "/* 119 */\n",
      "/* 120 */             // preserve 8 bytes to write the key array numBytes later.\n",
      "/* 121 */             columnartorow_mutableStateArray_3[0].grow(8);\n",
      "/* 122 */             columnartorow_mutableStateArray_3[0].increaseCursor(8);\n",
      "/* 123 */\n",
      "/* 124 */             // Remember the current cursor so that we can write numBytes of key array later.\n",
      "/* 125 */             final int columnartorow_tmpCursor_0 = columnartorow_mutableStateArray_3[0].cursor();\n",
      "/* 126 */\n",
      "/* 127 */             final ArrayData columnartorow_tmpInput_1 = columnartorow_tmpInput_0.keyArray();\n",
      "/* 128 */             if (columnartorow_tmpInput_1 instanceof UnsafeArrayData) {\n",
      "/* 129 */               columnartorow_mutableStateArray_3[0].write((UnsafeArrayData) columnartorow_tmpInput_1);\n",
      "/* 130 */             } else {\n",
      "/* 131 */               final int columnartorow_numElements_0 = columnartorow_tmpInput_1.numElements();\n",
      "/* 132 */               columnartorow_mutableStateArray_4[0].initialize(columnartorow_numElements_0);\n",
      "/* 133 */\n",
      "/* 134 */               for (int columnartorow_index_0 = 0; columnartorow_index_0 < columnartorow_numElements_0; columnartorow_index_0++) {\n",
      "/* 135 */                 columnartorow_mutableStateArray_4[0].write(columnartorow_index_0, columnartorow_tmpInput_1.getUTF8String(columnartorow_index_0));\n",
      "/* 136 */               }\n",
      "/* 137 */             }\n",
      "/* 138 */\n",
      "/* 139 */             // Write the numBytes of key array into the first 8 bytes.\n",
      "/* 140 */             Platform.putLong(\n",
      "/* 141 */               columnartorow_mutableStateArray_3[0].getBuffer(),\n",
      "/* 142 */               columnartorow_tmpCursor_0 - 8,\n",
      "/* 143 */               columnartorow_mutableStateArray_3[0].cursor() - columnartorow_tmpCursor_0);\n",
      "/* 144 */\n",
      "/* 145 */             final ArrayData columnartorow_tmpInput_2 = columnartorow_tmpInput_0.valueArray();\n",
      "/* 146 */             if (columnartorow_tmpInput_2 instanceof UnsafeArrayData) {\n",
      "/* 147 */               columnartorow_mutableStateArray_3[0].write((UnsafeArrayData) columnartorow_tmpInput_2);\n",
      "/* 148 */             } else {\n",
      "/* 149 */               final int columnartorow_numElements_1 = columnartorow_tmpInput_2.numElements();\n",
      "/* 150 */               columnartorow_mutableStateArray_4[1].initialize(columnartorow_numElements_1);\n",
      "/* 151 */\n",
      "/* 152 */               for (int columnartorow_index_1 = 0; columnartorow_index_1 < columnartorow_numElements_1; columnartorow_index_1++) {\n",
      "/* 153 */                 if (columnartorow_tmpInput_2.isNullAt(columnartorow_index_1)) {\n",
      "/* 154 */                   columnartorow_mutableStateArray_4[1].setNull8Bytes(columnartorow_index_1);\n",
      "/* 155 */                 } else {\n",
      "/* 156 */                   columnartorow_mutableStateArray_4[1].write(columnartorow_index_1, columnartorow_tmpInput_2.getUTF8String(columnartorow_index_1));\n",
      "/* 157 */                 }\n",
      "/* 158 */\n",
      "/* 159 */               }\n",
      "/* 160 */             }\n",
      "/* 161 */\n",
      "/* 162 */             columnartorow_mutableStateArray_3[0].setOffsetAndSizeFromPreviousCursor(5, columnartorow_previousCursor_0);\n",
      "/* 163 */           }\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */         if (columnartorow_isNull_6) {\n",
      "/* 167 */           columnartorow_mutableStateArray_3[0].setNullAt(6);\n",
      "/* 168 */         } else {\n",
      "/* 169 */           // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 170 */           // written later.\n",
      "/* 171 */           final int columnartorow_previousCursor_1 = columnartorow_mutableStateArray_3[0].cursor();\n",
      "/* 172 */\n",
      "/* 173 */           final ArrayData columnartorow_tmpInput_3 = columnartorow_value_6;\n",
      "/* 174 */           if (columnartorow_tmpInput_3 instanceof UnsafeArrayData) {\n",
      "/* 175 */             columnartorow_mutableStateArray_3[0].write((UnsafeArrayData) columnartorow_tmpInput_3);\n",
      "/* 176 */           } else {\n",
      "/* 177 */             final int columnartorow_numElements_2 = columnartorow_tmpInput_3.numElements();\n",
      "/* 178 */             columnartorow_mutableStateArray_4[2].initialize(columnartorow_numElements_2);\n",
      "/* 179 */\n",
      "/* 180 */             for (int columnartorow_index_2 = 0; columnartorow_index_2 < columnartorow_numElements_2; columnartorow_index_2++) {\n",
      "/* 181 */               if (columnartorow_tmpInput_3.isNullAt(columnartorow_index_2)) {\n",
      "/* 182 */                 columnartorow_mutableStateArray_4[2].setNull8Bytes(columnartorow_index_2);\n",
      "/* 183 */               } else {\n",
      "/* 184 */                 columnartorow_mutableStateArray_4[2].write(columnartorow_index_2, columnartorow_tmpInput_3.getUTF8String(columnartorow_index_2));\n",
      "/* 185 */               }\n",
      "/* 186 */\n",
      "/* 187 */             }\n",
      "/* 188 */           }\n",
      "/* 189 */\n",
      "/* 190 */           columnartorow_mutableStateArray_3[0].setOffsetAndSizeFromPreviousCursor(6, columnartorow_previousCursor_1);\n",
      "/* 191 */         }\n",
      "/* 192 */\n",
      "/* 193 */         if (columnartorow_isNull_7) {\n",
      "/* 194 */           columnartorow_mutableStateArray_3[0].setNullAt(7);\n",
      "/* 195 */         } else {\n",
      "/* 196 */           columnartorow_mutableStateArray_3[0].write(7, columnartorow_value_7);\n",
      "/* 197 */         }\n",
      "/* 198 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 199 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 200 */       }\n",
      "/* 201 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 202 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 203 */       columnartorow_nextBatch_0();\n",
      "/* 204 */     }\n",
      "/* 205 */   }\n",
      "/* 206 */\n",
      "/* 207 */ }\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Full table scan execution time: 0.2021 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2021493911743164, 5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with basic scan operations and examine their plans\n",
    "# 1. Full table scan\n",
    "full_scan = spark.table(\"products_table\")\n",
    "analyze_plans(full_scan, \"Full Table Scan\")\n",
    "\n",
    "# Check execution time\n",
    "time_execution_with_metrics(full_scan, name=\"Full table scan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Column Pruning Plan Analysis ====================\n",
      "\n",
      "--- Logical Plan (Parsed) ---\n",
      "\n",
      "--- Logical Plan (Analyzed) ---\n",
      "\n",
      "--- Logical Plan (Optimized) ---\n",
      "\n",
      "--- Physical Plan ---\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,price#153] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double>\n",
      "\n",
      "\n",
      "\n",
      "--- Detailed Physical Plan ---\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['product_id, 'name, 'price]\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: int, name: string, price: double\n",
      "Project [product_id#149, name#150, price#153]\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [product_id#149, name#150, price#153]\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,price#153] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double>\n",
      "\n",
      "\n",
      "--- Cost Analysis ---\n",
      "== Optimized Logical Plan ==\n",
      "Project [product_id#149, name#150, price#153], Statistics(sizeInBytes=61.9 KiB)\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet, Statistics(sizeInBytes=179.4 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,price#153] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double>\n",
      "\n",
      "\n",
      "\n",
      "--- Codegen Details ---\n",
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:324; maxConstantPoolSize:160(0.24% used); numInnerClasses:0) ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,price#153] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 033 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 037 */\n",
      "/* 038 */     }\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */   protected void processNext() throws java.io.IOException {\n",
      "/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 043 */       columnartorow_nextBatch_0();\n",
      "/* 044 */     }\n",
      "/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 050 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 051 */         int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));\n",
      "/* 052 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 053 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 054 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 055 */         double columnartorow_value_2 = columnartorow_isNull_2 ? -1.0 : (columnartorow_mutableStateArray_2[2].getDouble(columnartorow_rowIdx_0));\n",
      "/* 056 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 057 */\n",
      "/* 058 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 059 */\n",
      "/* 060 */         if (columnartorow_isNull_0) {\n",
      "/* 061 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 062 */         } else {\n",
      "/* 063 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 064 */         }\n",
      "/* 065 */\n",
      "/* 066 */         if (columnartorow_isNull_1) {\n",
      "/* 067 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 068 */         } else {\n",
      "/* 069 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (columnartorow_isNull_2) {\n",
      "/* 073 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 076 */         }\n",
      "/* 077 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 078 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 079 */       }\n",
      "/* 080 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 081 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 082 */       columnartorow_nextBatch_0();\n",
      "/* 083 */     }\n",
      "/* 084 */   }\n",
      "/* 085 */\n",
      "/* 086 */ }\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Column pruning scan execution time: 0.0512 seconds\n",
      "\n",
      "Parquet file metadata:\n",
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                              |comment|\n",
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "|product_id                  |int                                                    |NULL   |\n",
      "|name                        |string                                                 |NULL   |\n",
      "|category_id                 |int                                                    |NULL   |\n",
      "|subcategory_id              |int                                                    |NULL   |\n",
      "|price                       |double                                                 |NULL   |\n",
      "|attributes                  |map<string,string>                                     |NULL   |\n",
      "|tags                        |array<string>                                          |NULL   |\n",
      "|created_at                  |timestamp                                              |NULL   |\n",
      "|                            |                                                       |       |\n",
      "|# Detailed Table Information|                                                       |       |\n",
      "|Catalog                     |spark_catalog                                          |       |\n",
      "|Database                    |default                                                |       |\n",
      "|Table                       |products_table                                         |       |\n",
      "|Created Time                |Fri Apr 18 10:11:22 UTC 2025                           |       |\n",
      "|Last Access                 |UNKNOWN                                                |       |\n",
      "|Created By                  |Spark 3.5.1                                            |       |\n",
      "|Type                        |MANAGED                                                |       |\n",
      "|Provider                    |parquet                                                |       |\n",
      "|Location                    |file:/opt/spark/work-dir/spark-warehouse/products_table|       |\n",
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Column Pruning - Selecting only specific columns\n",
    "column_pruning = spark.table(\"products_table\").select(\"product_id\", \"name\", \"price\")\n",
    "analyze_plans(column_pruning, \"Column Pruning\")\n",
    "\n",
    "# Performance comparison\n",
    "pruning_time, _ = time_execution_with_metrics(column_pruning, name=\"Column pruning scan\")\n",
    "\n",
    "# Examine file format statistics\n",
    "print(\"\\nParquet file metadata:\")\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE FORMATTED products_table\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# # Alternatively, you can use the DataFrame API directly:\n",
    "# # Get table metadata using DataFrame API\n",
    "# spark.table(\"products_table\").describe().show(truncate=False)\n",
    "\n",
    "# # For storage information\n",
    "# print(\"\\nParquet file metadata:\")\n",
    "# metadata = spark.sql(\"SHOW TABLE EXTENDED LIKE 'products_table'\").collect()\n",
    "# for row in metadata:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Complex Type Field Access Plan Analysis ====================\n",
      "\n",
      "--- Logical Plan (Parsed) ---\n",
      "\n",
      "--- Logical Plan (Analyzed) ---\n",
      "\n",
      "--- Logical Plan (Optimized) ---\n",
      "\n",
      "--- Physical Plan ---\n",
      "== Physical Plan ==\n",
      "*(1) Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,attributes#154,tags#155] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,attributes:map<string,string>,tags:array<string>>\n",
      "\n",
      "\n",
      "\n",
      "--- Detailed Physical Plan ---\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['product_id, 'name, 'attributes[color] AS color#233, 'tags[0] AS first_tag#234]\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: int, name: string, color: string, first_tag: string\n",
      "Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,attributes#154,tags#155] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,attributes:map<string,string>,tags:array<string>>\n",
      "\n",
      "\n",
      "--- Cost Analysis ---\n",
      "== Optimized Logical Plan ==\n",
      "Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234], Statistics(sizeInBytes=111.3 KiB)\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet, Statistics(sizeInBytes=179.4 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,attributes#154,tags#155] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,attributes:map<string,string>,tags:array<string>>\n",
      "\n",
      "\n",
      "\n",
      "--- Codegen Details ---\n",
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:573; maxConstantPoolSize:196(0.30% used); numInnerClasses:0) ==\n",
      "*(1) Project [product_id#149, name#150, attributes#154[color] AS color#233, tags#155[0] AS first_tag#234]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,attributes#154,tags#155] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,attributes:map<string,string>,tags:array<string>>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[4];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] columnartorow_mutableStateArray_4 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[3];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 024 */\n",
      "/* 025 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 96);\n",
      "/* 026 */     columnartorow_mutableStateArray_4[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 027 */     columnartorow_mutableStateArray_4[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 028 */     columnartorow_mutableStateArray_4[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 029 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 96);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 034 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 035 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 037 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 038 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 039 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 040 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 041 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 042 */       columnartorow_mutableStateArray_2[3] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(3);\n",
      "/* 043 */\n",
      "/* 044 */     }\n",
      "/* 045 */   }\n",
      "/* 046 */\n",
      "/* 047 */   protected void processNext() throws java.io.IOException {\n",
      "/* 048 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 049 */       columnartorow_nextBatch_0();\n",
      "/* 050 */     }\n",
      "/* 051 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 052 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 053 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 054 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 055 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 056 */         // common sub-expressions\n",
      "/* 057 */\n",
      "/* 058 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 059 */         int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));\n",
      "/* 060 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 061 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 062 */         boolean project_isNull_2 = true;\n",
      "/* 063 */         UTF8String project_value_2 = null;\n",
      "/* 064 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 065 */         MapData columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getMap(columnartorow_rowIdx_0));\n",
      "/* 066 */         if (!columnartorow_isNull_2) {\n",
      "/* 067 */           project_isNull_2 = false; // resultCode could change nullability.\n",
      "/* 068 */\n",
      "/* 069 */           final int project_length_0 = columnartorow_value_2.numElements();\n",
      "/* 070 */           final ArrayData project_keys_0 = columnartorow_value_2.keyArray();\n",
      "/* 071 */           final ArrayData project_values_0 = columnartorow_value_2.valueArray();\n",
      "/* 072 */\n",
      "/* 073 */           int project_index_0 = 0;\n",
      "/* 074 */           while (project_index_0 < project_length_0) {\n",
      "/* 075 */             final UTF8String project_key_0 = project_keys_0.getUTF8String(project_index_0);\n",
      "/* 076 */             if (project_key_0.equals(((UTF8String) references[2] /* literal */))) {\n",
      "/* 077 */               break;\n",
      "/* 078 */             } else {\n",
      "/* 079 */               project_index_0++;\n",
      "/* 080 */             }\n",
      "/* 081 */           }\n",
      "/* 082 */\n",
      "/* 083 */           if (project_index_0 == project_length_0 || project_values_0.isNullAt(project_index_0)) {\n",
      "/* 084 */             project_isNull_2 = true;\n",
      "/* 085 */           } else {\n",
      "/* 086 */             project_value_2 = project_values_0.getUTF8String(project_index_0);\n",
      "/* 087 */           }\n",
      "/* 088 */\n",
      "/* 089 */         }\n",
      "/* 090 */         boolean project_isNull_5 = true;\n",
      "/* 091 */         UTF8String project_value_5 = null;\n",
      "/* 092 */         boolean columnartorow_isNull_3 = columnartorow_mutableStateArray_2[3].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 093 */         ArrayData columnartorow_value_3 = columnartorow_isNull_3 ? null : (columnartorow_mutableStateArray_2[3].getArray(columnartorow_rowIdx_0));\n",
      "/* 094 */         if (!columnartorow_isNull_3) {\n",
      "/* 095 */           project_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 096 */\n",
      "/* 097 */           final int project_index_1 = (int) 0;\n",
      "/* 098 */           if (project_index_1 >= columnartorow_value_3.numElements() || project_index_1 < 0) {\n",
      "/* 099 */             project_isNull_5 = true;\n",
      "/* 100 */           } else if (columnartorow_value_3.isNullAt(project_index_1)) {\n",
      "/* 101 */             project_isNull_5 = true;\n",
      "/* 102 */           }\n",
      "/* 103 */           else {\n",
      "/* 104 */             project_value_5 = columnartorow_value_3.getUTF8String(project_index_1);\n",
      "/* 105 */           }\n",
      "/* 106 */\n",
      "/* 107 */         }\n",
      "/* 108 */         columnartorow_mutableStateArray_3[1].reset();\n",
      "/* 109 */\n",
      "/* 110 */         columnartorow_mutableStateArray_3[1].zeroOutNullBytes();\n",
      "/* 111 */\n",
      "/* 112 */         if (columnartorow_isNull_0) {\n",
      "/* 113 */           columnartorow_mutableStateArray_3[1].setNullAt(0);\n",
      "/* 114 */         } else {\n",
      "/* 115 */           columnartorow_mutableStateArray_3[1].write(0, columnartorow_value_0);\n",
      "/* 116 */         }\n",
      "/* 117 */\n",
      "/* 118 */         if (columnartorow_isNull_1) {\n",
      "/* 119 */           columnartorow_mutableStateArray_3[1].setNullAt(1);\n",
      "/* 120 */         } else {\n",
      "/* 121 */           columnartorow_mutableStateArray_3[1].write(1, columnartorow_value_1);\n",
      "/* 122 */         }\n",
      "/* 123 */\n",
      "/* 124 */         if (project_isNull_2) {\n",
      "/* 125 */           columnartorow_mutableStateArray_3[1].setNullAt(2);\n",
      "/* 126 */         } else {\n",
      "/* 127 */           columnartorow_mutableStateArray_3[1].write(2, project_value_2);\n",
      "/* 128 */         }\n",
      "/* 129 */\n",
      "/* 130 */         if (project_isNull_5) {\n",
      "/* 131 */           columnartorow_mutableStateArray_3[1].setNullAt(3);\n",
      "/* 132 */         } else {\n",
      "/* 133 */           columnartorow_mutableStateArray_3[1].write(3, project_value_5);\n",
      "/* 134 */         }\n",
      "/* 135 */         append((columnartorow_mutableStateArray_3[1].getRow()));\n",
      "/* 136 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 137 */       }\n",
      "/* 138 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 139 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 140 */       columnartorow_nextBatch_0();\n",
      "/* 141 */     }\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */ }\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Complex field access execution time: 0.0539 seconds\n"
     ]
    }
   ],
   "source": [
    "# 3. Complex types handling in scans\n",
    "# Reading and projecting into complex types can impact performance\n",
    "complex_scan = spark.table(\"products_table\").select(\n",
    "    \"product_id\", \n",
    "    \"name\", \n",
    "    col(\"attributes\").getItem(\"color\").alias(\"color\"),\n",
    "    col(\"tags\")[0].alias(\"first_tag\")\n",
    ")\n",
    "analyze_plans(complex_scan, \"Complex Type Field Access\")\n",
    "\n",
    "# Performance measurement\n",
    "complex_time, _ = time_execution_with_metrics(complex_scan, name=\"Complex field access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Scan Plan:\n",
      "== Physical Plan ==\n",
      "FileScan json [attributes#280,category_id#281L,created_at#282,name#283,price#284,product_id#285L,subcategory_id#286L,tags#287] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/tmp/products_json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<attributes:struct<color:string,connectivity:string,material:string,origin:string,power:str...\n",
      "\n",
      "\n",
      "\n",
      "Parquet Scan Plan:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [product_id#296,name#297,category_id#298,subcategory_id#299,price#300,attributes#301,tags#302,created_at#303] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/products_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "JSON scan execution time: 0.0636 seconds\n",
      "Parquet scan execution time: 0.0362 seconds\n",
      "Performance difference: Parquet is 1.76x faster than JSON\n"
     ]
    }
   ],
   "source": [
    "# 4. File format impact on scan\n",
    "# Compare Parquet read vs JSON read\n",
    "\n",
    "# First, save a sample in both JSON and Parquet format\n",
    "sample_df = spark.table(\"products_table\").limit(1000)\n",
    "sample_df.write.json(\"/tmp/products_json\", mode=\"overwrite\")\n",
    "sample_df.write.parquet(\"/tmp/products_parquet\", mode=\"overwrite\")  # Save to different path\n",
    "\n",
    "# Now read them back and compare scan plans\n",
    "json_scan = spark.read.json(\"/tmp/products_json\")\n",
    "parquet_scan = spark.read.parquet(\"/tmp/products_parquet\")  # Read from the correct path\n",
    "\n",
    "print(\"JSON Scan Plan:\")\n",
    "json_scan.explain()\n",
    "\n",
    "print(\"\\nParquet Scan Plan:\")\n",
    "parquet_scan.explain()\n",
    "\n",
    "# Compare performance\n",
    "json_time, _ = time_execution_with_metrics(json_scan, name=\"JSON scan\")\n",
    "parquet_time, _ = time_execution_with_metrics(parquet_scan, name=\"Parquet scan\")\n",
    "\n",
    "print(f\"Performance difference: Parquet is {json_time/parquet_time:.2f}x faster than JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Scan Optimization Techniques\n",
    "\n",
    "1. **File Format Selection**: The file format significantly impacts scan performance:\n",
    "   - Parquet and ORC use columnar storage, allowing efficient column pruning and predicate pushdown\n",
    "   - JSON and CSV require parsing the entire row, even if you only need a few columns\n",
    "   - Avro provides good compression and schema evolution\n",
    "\n",
    "2. **File Size and Splitting**: \n",
    "   - Too many small files create overhead (\"small file problem\")\n",
    "   - Files that are too large might not utilize parallelism efficiently\n",
    "   - Ideal file size typically ranges from 64MB to 1GB\n",
    "\n",
    "3. **Statistics Utilization**:\n",
    "   - File formats like Parquet store statistics (min/max values) for columns\n",
    "   - These statistics enable file pruning before reading data\n",
    "   - Computing and maintaining statistics for tables improves planner decisions\n",
    "\n",
    "4. **Vectorization**:\n",
    "   - Modern file formats support vectorized reads (batch processing)\n",
    "   - Column vectors are processed more efficiently than row-by-row\n",
    "   - Parquet and ORC have built-in support for vectorized reading\n",
    "\n",
    "Let's examine file pruning using statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in transactions_table: 64\n",
      "year=2020/month=1\n",
      "year=2020/month=10\n",
      "year=2020/month=11\n",
      "year=2020/month=12\n",
      "year=2020/month=2\n",
      "year=2020/month=3\n",
      "year=2020/month=4\n",
      "year=2020/month=5\n",
      "year=2020/month=6\n",
      "year=2020/month=7\n",
      "...\n",
      "\n",
      "==================== Statistics-based Pruning Plan Analysis ====================\n",
      "\n",
      "--- Logical Plan (Parsed) ---\n",
      "\n",
      "--- Logical Plan (Analyzed) ---\n",
      "\n",
      "--- Logical Plan (Optimized) ---\n",
      "\n",
      "--- Physical Plan ---\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(price#153) AND (price#153 > 1500.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [isnotnull(price#153), (price#153 > 1500.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(price), GreaterThan(price,1500.0)], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "\n",
      "--- Detailed Physical Plan ---\n",
      "== Parsed Logical Plan ==\n",
      "'Filter ('price > 1500)\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: int, name: string, category_id: int, subcategory_id: int, price: double, attributes: map<string,string>, tags: array<string>, created_at: timestamp\n",
      "Filter (price#153 > cast(1500 as double))\n",
      "+- SubqueryAlias spark_catalog.default.products_table\n",
      "   +- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(price#153) AND (price#153 > 1500.0))\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(price#153) AND (price#153 > 1500.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [isnotnull(price#153), (price#153 > 1500.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(price), GreaterThan(price,1500.0)], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "--- Cost Analysis ---\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(price#153) AND (price#153 > 1500.0)), Statistics(sizeInBytes=179.4 KiB)\n",
      "+- Relation spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] parquet, Statistics(sizeInBytes=179.4 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(price#153) AND (price#153 > 1500.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [isnotnull(price#153), (price#153 > 1500.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(price), GreaterThan(price,1500.0)], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "\n",
      "--- Codegen Details ---\n",
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:1132; maxConstantPoolSize:263(0.40% used); numInnerClasses:0) ==\n",
      "*(1) Filter (isnotnull(price#153) AND (price#153 > 1500.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#149,name#150,category_id#151,subcategory_id#152,price#153,attributes#154,tags#155,created_at#156] Batched: true, DataFilters: [isnotnull(price#153), (price#153 > 1500.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(price), GreaterThan(price,1500.0)], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[8];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] columnartorow_mutableStateArray_4 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[6];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */     wholestagecodegen_init_0_0();\n",
      "/* 024 */     wholestagecodegen_init_0_1();\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void wholestagecodegen_init_0_1() {\n",
      "/* 029 */     columnartorow_mutableStateArray_4[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[1], 8);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 034 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 035 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 037 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 038 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 039 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 040 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 041 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 042 */       columnartorow_mutableStateArray_2[3] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(3);\n",
      "/* 043 */       columnartorow_mutableStateArray_2[4] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(4);\n",
      "/* 044 */       columnartorow_mutableStateArray_2[5] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(5);\n",
      "/* 045 */       columnartorow_mutableStateArray_2[6] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(6);\n",
      "/* 046 */       columnartorow_mutableStateArray_2[7] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(7);\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */   protected void processNext() throws java.io.IOException {\n",
      "/* 052 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 053 */       columnartorow_nextBatch_0();\n",
      "/* 054 */     }\n",
      "/* 055 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 056 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 057 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 058 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 059 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 060 */         do {\n",
      "/* 061 */           boolean columnartorow_isNull_4 = columnartorow_mutableStateArray_2[4].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 062 */           double columnartorow_value_4 = columnartorow_isNull_4 ? -1.0 : (columnartorow_mutableStateArray_2[4].getDouble(columnartorow_rowIdx_0));\n",
      "/* 063 */\n",
      "/* 064 */           boolean filter_value_2 = !columnartorow_isNull_4;\n",
      "/* 065 */           if (!filter_value_2) continue;\n",
      "/* 066 */\n",
      "/* 067 */           boolean filter_value_3 = false;\n",
      "/* 068 */           filter_value_3 = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(columnartorow_value_4, 1500.0D) > 0;\n",
      "/* 069 */           if (!filter_value_3) continue;\n",
      "/* 070 */\n",
      "/* 071 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);\n",
      "/* 072 */\n",
      "/* 073 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 074 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));\n",
      "/* 075 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 076 */           UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 077 */           boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 078 */           int columnartorow_value_2 = columnartorow_isNull_2 ? -1 : (columnartorow_mutableStateArray_2[2].getInt(columnartorow_rowIdx_0));\n",
      "/* 079 */           boolean columnartorow_isNull_3 = columnartorow_mutableStateArray_2[3].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 080 */           int columnartorow_value_3 = columnartorow_isNull_3 ? -1 : (columnartorow_mutableStateArray_2[3].getInt(columnartorow_rowIdx_0));\n",
      "/* 081 */           boolean columnartorow_isNull_5 = columnartorow_mutableStateArray_2[5].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 082 */           MapData columnartorow_value_5 = columnartorow_isNull_5 ? null : (columnartorow_mutableStateArray_2[5].getMap(columnartorow_rowIdx_0));\n",
      "/* 083 */           boolean columnartorow_isNull_6 = columnartorow_mutableStateArray_2[6].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 084 */           ArrayData columnartorow_value_6 = columnartorow_isNull_6 ? null : (columnartorow_mutableStateArray_2[6].getArray(columnartorow_rowIdx_0));\n",
      "/* 085 */           boolean columnartorow_isNull_7 = columnartorow_mutableStateArray_2[7].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 086 */           long columnartorow_value_7 = columnartorow_isNull_7 ? -1L : (columnartorow_mutableStateArray_2[7].getLong(columnartorow_rowIdx_0));\n",
      "/* 087 */           columnartorow_mutableStateArray_3[1].reset();\n",
      "/* 088 */\n",
      "/* 089 */           columnartorow_mutableStateArray_3[1].zeroOutNullBytes();\n",
      "/* 090 */\n",
      "/* 091 */           if (columnartorow_isNull_0) {\n",
      "/* 092 */             columnartorow_mutableStateArray_3[1].setNullAt(0);\n",
      "/* 093 */           } else {\n",
      "/* 094 */             columnartorow_mutableStateArray_3[1].write(0, columnartorow_value_0);\n",
      "/* 095 */           }\n",
      "/* 096 */\n",
      "/* 097 */           if (columnartorow_isNull_1) {\n",
      "/* 098 */             columnartorow_mutableStateArray_3[1].setNullAt(1);\n",
      "/* 099 */           } else {\n",
      "/* 100 */             columnartorow_mutableStateArray_3[1].write(1, columnartorow_value_1);\n",
      "/* 101 */           }\n",
      "/* 102 */\n",
      "/* 103 */           if (columnartorow_isNull_2) {\n",
      "/* 104 */             columnartorow_mutableStateArray_3[1].setNullAt(2);\n",
      "/* 105 */           } else {\n",
      "/* 106 */             columnartorow_mutableStateArray_3[1].write(2, columnartorow_value_2);\n",
      "/* 107 */           }\n",
      "/* 108 */\n",
      "/* 109 */           if (columnartorow_isNull_3) {\n",
      "/* 110 */             columnartorow_mutableStateArray_3[1].setNullAt(3);\n",
      "/* 111 */           } else {\n",
      "/* 112 */             columnartorow_mutableStateArray_3[1].write(3, columnartorow_value_3);\n",
      "/* 113 */           }\n",
      "/* 114 */\n",
      "/* 115 */           columnartorow_mutableStateArray_3[1].write(4, columnartorow_value_4);\n",
      "/* 116 */\n",
      "/* 117 */           if (columnartorow_isNull_5) {\n",
      "/* 118 */             columnartorow_mutableStateArray_3[1].setNullAt(5);\n",
      "/* 119 */           } else {\n",
      "/* 120 */             final MapData filter_tmpInput_0 = columnartorow_value_5;\n",
      "/* 121 */             if (filter_tmpInput_0 instanceof UnsafeMapData) {\n",
      "/* 122 */               columnartorow_mutableStateArray_3[1].write(5, (UnsafeMapData) filter_tmpInput_0);\n",
      "/* 123 */             } else {\n",
      "/* 124 */               // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 125 */               // written later.\n",
      "/* 126 */               final int filter_previousCursor_0 = columnartorow_mutableStateArray_3[1].cursor();\n",
      "/* 127 */\n",
      "/* 128 */               // preserve 8 bytes to write the key array numBytes later.\n",
      "/* 129 */               columnartorow_mutableStateArray_3[1].grow(8);\n",
      "/* 130 */               columnartorow_mutableStateArray_3[1].increaseCursor(8);\n",
      "/* 131 */\n",
      "/* 132 */               // Remember the current cursor so that we can write numBytes of key array later.\n",
      "/* 133 */               final int filter_tmpCursor_0 = columnartorow_mutableStateArray_3[1].cursor();\n",
      "/* 134 */\n",
      "/* 135 */               final ArrayData filter_tmpInput_1 = filter_tmpInput_0.keyArray();\n",
      "/* 136 */               if (filter_tmpInput_1 instanceof UnsafeArrayData) {\n",
      "/* 137 */                 columnartorow_mutableStateArray_3[1].write((UnsafeArrayData) filter_tmpInput_1);\n",
      "/* 138 */               } else {\n",
      "/* 139 */                 final int filter_numElements_0 = filter_tmpInput_1.numElements();\n",
      "/* 140 */                 columnartorow_mutableStateArray_4[3].initialize(filter_numElements_0);\n",
      "/* 141 */\n",
      "/* 142 */                 for (int filter_index_0 = 0; filter_index_0 < filter_numElements_0; filter_index_0++) {\n",
      "/* 143 */                   columnartorow_mutableStateArray_4[3].write(filter_index_0, filter_tmpInput_1.getUTF8String(filter_index_0));\n",
      "/* 144 */                 }\n",
      "/* 145 */               }\n",
      "/* 146 */\n",
      "/* 147 */               // Write the numBytes of key array into the first 8 bytes.\n",
      "/* 148 */               Platform.putLong(\n",
      "/* 149 */                 columnartorow_mutableStateArray_3[1].getBuffer(),\n",
      "/* 150 */                 filter_tmpCursor_0 - 8,\n",
      "/* 151 */                 columnartorow_mutableStateArray_3[1].cursor() - filter_tmpCursor_0);\n",
      "/* 152 */\n",
      "/* 153 */               final ArrayData filter_tmpInput_2 = filter_tmpInput_0.valueArray();\n",
      "/* 154 */               if (filter_tmpInput_2 instanceof UnsafeArrayData) {\n",
      "/* 155 */                 columnartorow_mutableStateArray_3[1].write((UnsafeArrayData) filter_tmpInput_2);\n",
      "/* 156 */               } else {\n",
      "/* 157 */                 final int filter_numElements_1 = filter_tmpInput_2.numElements();\n",
      "/* 158 */                 columnartorow_mutableStateArray_4[4].initialize(filter_numElements_1);\n",
      "/* 159 */\n",
      "/* 160 */                 for (int filter_index_1 = 0; filter_index_1 < filter_numElements_1; filter_index_1++) {\n",
      "/* 161 */                   if (filter_tmpInput_2.isNullAt(filter_index_1)) {\n",
      "/* 162 */                     columnartorow_mutableStateArray_4[4].setNull8Bytes(filter_index_1);\n",
      "/* 163 */                   } else {\n",
      "/* 164 */                     columnartorow_mutableStateArray_4[4].write(filter_index_1, filter_tmpInput_2.getUTF8String(filter_index_1));\n",
      "/* 165 */                   }\n",
      "/* 166 */\n",
      "/* 167 */                 }\n",
      "/* 168 */               }\n",
      "/* 169 */\n",
      "/* 170 */               columnartorow_mutableStateArray_3[1].setOffsetAndSizeFromPreviousCursor(5, filter_previousCursor_0);\n",
      "/* 171 */             }\n",
      "/* 172 */           }\n",
      "/* 173 */\n",
      "/* 174 */           if (columnartorow_isNull_6) {\n",
      "/* 175 */             columnartorow_mutableStateArray_3[1].setNullAt(6);\n",
      "/* 176 */           } else {\n",
      "/* 177 */             // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 178 */             // written later.\n",
      "/* 179 */             final int filter_previousCursor_1 = columnartorow_mutableStateArray_3[1].cursor();\n",
      "/* 180 */\n",
      "/* 181 */             final ArrayData filter_tmpInput_3 = columnartorow_value_6;\n",
      "/* 182 */             if (filter_tmpInput_3 instanceof UnsafeArrayData) {\n",
      "/* 183 */               columnartorow_mutableStateArray_3[1].write((UnsafeArrayData) filter_tmpInput_3);\n",
      "/* 184 */             } else {\n",
      "/* 185 */               final int filter_numElements_2 = filter_tmpInput_3.numElements();\n",
      "/* 186 */               columnartorow_mutableStateArray_4[5].initialize(filter_numElements_2);\n",
      "/* 187 */\n",
      "/* 188 */               for (int filter_index_2 = 0; filter_index_2 < filter_numElements_2; filter_index_2++) {\n",
      "/* 189 */                 if (filter_tmpInput_3.isNullAt(filter_index_2)) {\n",
      "/* 190 */                   columnartorow_mutableStateArray_4[5].setNull8Bytes(filter_index_2);\n",
      "/* 191 */                 } else {\n",
      "/* 192 */                   columnartorow_mutableStateArray_4[5].write(filter_index_2, filter_tmpInput_3.getUTF8String(filter_index_2));\n",
      "/* 193 */                 }\n",
      "/* 194 */\n",
      "/* 195 */               }\n",
      "/* 196 */             }\n",
      "/* 197 */\n",
      "/* 198 */             columnartorow_mutableStateArray_3[1].setOffsetAndSizeFromPreviousCursor(6, filter_previousCursor_1);\n",
      "/* 199 */           }\n",
      "/* 200 */\n",
      "/* 201 */           if (columnartorow_isNull_7) {\n",
      "/* 202 */             columnartorow_mutableStateArray_3[1].setNullAt(7);\n",
      "/* 203 */           } else {\n",
      "/* 204 */             columnartorow_mutableStateArray_3[1].write(7, columnartorow_value_7);\n",
      "/* 205 */           }\n",
      "/* 206 */           append((columnartorow_mutableStateArray_3[1].getRow()));\n",
      "/* 207 */\n",
      "/* 208 */         } while(false);\n",
      "/* 209 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 210 */       }\n",
      "/* 211 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 212 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 213 */       columnartorow_nextBatch_0();\n",
      "/* 214 */     }\n",
      "/* 215 */   }\n",
      "/* 216 */\n",
      "/* 217 */   private void wholestagecodegen_init_0_0() {\n",
      "/* 218 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 219 */\n",
      "/* 220 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 96);\n",
      "/* 221 */     columnartorow_mutableStateArray_4[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 222 */     columnartorow_mutableStateArray_4[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 223 */     columnartorow_mutableStateArray_4[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[0], 8);\n",
      "/* 224 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 96);\n",
      "/* 225 */     columnartorow_mutableStateArray_4[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[1], 8);\n",
      "/* 226 */     columnartorow_mutableStateArray_4[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(columnartorow_mutableStateArray_3[1], 8);\n",
      "/* 227 */\n",
      "/* 228 */   }\n",
      "/* 229 */\n",
      "/* 230 */ }\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Current histogram stats setting: false\n",
      "+--------------+------------------------------------------------------------+\n",
      "|info_name     |info_value                                                  |\n",
      "+--------------+------------------------------------------------------------+\n",
      "|col_name      |price                                                       |\n",
      "|data_type     |double                                                      |\n",
      "|comment       |NULL                                                        |\n",
      "|min           |11.39                                                       |\n",
      "|max           |1999.89                                                     |\n",
      "|num_nulls     |0                                                           |\n",
      "|distinct_count|4947                                                        |\n",
      "|avg_col_len   |8                                                           |\n",
      "|max_col_len   |8                                                           |\n",
      "|histogram     |height: 19.68503937007874, num_of_bins: 254                 |\n",
      "|bin_0         |lower_bound: 11.39, upper_bound: 33.99, distinct_count: 20  |\n",
      "|bin_1         |lower_bound: 33.99, upper_bound: 48.54, distinct_count: 20  |\n",
      "|bin_2         |lower_bound: 48.54, upper_bound: 68.57, distinct_count: 20  |\n",
      "|bin_3         |lower_bound: 68.57, upper_bound: 80.49, distinct_count: 18  |\n",
      "|bin_4         |lower_bound: 80.49, upper_bound: 93.21, distinct_count: 20  |\n",
      "|bin_5         |lower_bound: 93.21, upper_bound: 110.41, distinct_count: 20 |\n",
      "|bin_6         |lower_bound: 110.41, upper_bound: 131.2, distinct_count: 18 |\n",
      "|bin_7         |lower_bound: 131.2, upper_bound: 145.34, distinct_count: 19 |\n",
      "|bin_8         |lower_bound: 145.34, upper_bound: 157.75, distinct_count: 19|\n",
      "|bin_9         |lower_bound: 157.75, upper_bound: 169.74, distinct_count: 19|\n",
      "+--------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine statistics-based file pruning\n",
    "# First, let's analyze a partitioned table to see partition pruning\n",
    "partition_info = spark.sql(\"\"\"\n",
    "    SHOW PARTITIONS transactions_table\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Number of partitions in transactions_table: {len(partition_info)}\")\n",
    "for p in partition_info[:10]:\n",
    "    print(p[0])\n",
    "print(\"...\")\n",
    "\n",
    "# Statistics-based file skipping with Parquet\n",
    "high_price_scan = spark.table(\"products_table\").filter(col(\"price\") > 1500)\n",
    "analyze_plans(high_price_scan, \"Statistics-based Pruning\")\n",
    "\n",
    "# Force enable/disable stats collection for comparison\n",
    "current_stats = spark.conf.get(\"spark.sql.statistics.histogram.enabled\")\n",
    "print(f\"Current histogram stats setting: {current_stats}\")\n",
    "\n",
    "# Enable detailed statistics\n",
    "spark.conf.set(\"spark.sql.statistics.histogram.enabled\", \"true\")\n",
    "spark.sql(\"ANALYZE TABLE products_table COMPUTE STATISTICS FOR COLUMNS price\")\n",
    "\n",
    "# Check collected statistics\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE EXTENDED products_table price\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding Filter and Projection Pushdown\n",
    "\n",
    "Filter and projection pushdown are optimization techniques that reduce the amount of data read from disk and transferred between nodes. Let's examine these in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Filter Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 1000.0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,1000.0)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Detailed Simple Filter Query Plan:\n",
      "== Parsed Logical Plan ==\n",
      "'Filter ('total_amount > 1000)\n",
      "+- SubqueryAlias spark_catalog.default.transactions_table\n",
      "   +- Relation spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: int, customer_id: int, transaction_date: timestamp, items: array<struct<product_id:int,quantity:int,price:double,discount:double>>, payment_method: string, status: string, total_amount: double, store_id: int, metadata: map<string,string>, year: int, month: int\n",
      "Filter (total_amount#6375 > cast(1000 as double))\n",
      "+- SubqueryAlias spark_catalog.default.transactions_table\n",
      "   +- Relation spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- Relation spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 1000.0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,1000.0)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate filter pushdown capabilities\n",
    "simple_filter = spark.table(\"transactions_table\").filter(col(\"total_amount\") > 1000)\n",
    "print(\"Simple Filter Query Plan:\")\n",
    "simple_filter.explain()\n",
    "\n",
    "# Examine PushedFilters in the plan\n",
    "print(\"\\nDetailed Simple Filter Query Plan:\")\n",
    "simple_filter.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter 1 - Simple Comparison:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 1000.0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,1000.0)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Filter 2 - Compound Condition:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(total_amount#6375) AND isnotnull(payment_method#6373)) AND (total_amount#6375 > 1000.0)) AND (payment_method#6373 = credit_card))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), isnotnull(payment_method#6373), (total_amount#6375 > 1000.0), (pay..., Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(total_amount), IsNotNull(payment_method), GreaterThan(total_amount,1000.0), EqualTo(pa..., ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Filter 3 - Built-in Function:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(transaction_date#6371) AND (year(cast(transaction_date#6371 as date)) = 2022))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(transaction_date#6371), (year(cast(transaction_date#6371 as date)) = 2022)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(transaction_date)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Filter 4 - UDF:\n",
      "== Physical Plan ==\n",
      "*(2) Project [transaction_id#6369, customer_id#6370, transaction_date#6371, items#6372, payment_method#6373, status#6374, total_amount#6375, store_id#6376, metadata#6377, year#6378, month#6379]\n",
      "+- *(2) Filter pythonUDF0#6437: boolean\n",
      "   +- BatchEvalPython [is_expensive(total_amount#6375)#6436], [pythonUDF0#6437]\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex expressions that may or may not be pushed down\n",
    "from pyspark.sql.functions import col, year, udf\n",
    "# 1. Simple comparison (usually pushes down)\n",
    "filter1 = spark.table(\"transactions_table\").filter(col(\"total_amount\") > 1000)\n",
    "\n",
    "# 2. Compound condition (usually pushes down)\n",
    "filter2 = spark.table(\"transactions_table\").filter(\n",
    "    (col(\"total_amount\") > 1000) & \n",
    "    (col(\"payment_method\") == \"credit_card\")\n",
    ")\n",
    "\n",
    "# 3. Functions that may not push down\n",
    "filter3 = spark.table(\"transactions_table\").filter(\n",
    "    year(col(\"transaction_date\")) == 2022\n",
    ")\n",
    "\n",
    "# 4. UDFs (typically don't push down)\n",
    "@udf(\"boolean\")\n",
    "def is_expensive(amount):\n",
    "    return amount > 1000\n",
    "\n",
    "filter4 = spark.table(\"transactions_table\").filter(\n",
    "    is_expensive(col(\"total_amount\"))\n",
    ")\n",
    "\n",
    "# Display plans to compare\n",
    "print(\"Filter 1 - Simple Comparison:\")\n",
    "filter1.explain()\n",
    "\n",
    "print(\"\\nFilter 2 - Compound Condition:\")\n",
    "filter2.explain()\n",
    "\n",
    "print(\"\\nFilter 3 - Built-in Function:\")\n",
    "filter3.explain()\n",
    "\n",
    "print(\"\\nFilter 4 - UDF:\")\n",
    "filter4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=========================>                             (11 + 11) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built-in function filter: 1.7885 seconds\n",
      "UDF filter (no pushdown): 1.1917 seconds\n",
      "Performance difference: 0.67x slower without pushdown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Performance impact of filter pushdown vs. no pushdown\n",
    "# Using built-in functions (can be pushed down) vs UDFs (cannot be pushed down)\n",
    "\n",
    "# Filter using built-in functions\n",
    "time_start = time.time()\n",
    "builtin_result = spark.table(\"transactions_table\").filter(\n",
    "    (col(\"total_amount\") > 1000) & \n",
    "    (col(\"status\") == \"completed\")\n",
    ").select(\"transaction_id\", \"customer_id\", \"total_amount\").count()\n",
    "builtin_time = time.time() - time_start\n",
    "\n",
    "# Filter using UDFs (prevents pushdown)\n",
    "@udf(\"boolean\")\n",
    "def matches_criteria(amount, status):\n",
    "    return amount > 1000 and status == \"completed\"\n",
    "\n",
    "time_start = time.time()\n",
    "udf_result = spark.table(\"transactions_table\").filter(\n",
    "    matches_criteria(col(\"total_amount\"), col(\"status\"))\n",
    ").select(\"transaction_id\", \"customer_id\", \"total_amount\").count()\n",
    "udf_time = time.time() - time_start\n",
    "\n",
    "print(f\"Built-in function filter: {builtin_time:.4f} seconds\")\n",
    "print(f\"UDF filter (no pushdown): {udf_time:.4f} seconds\")\n",
    "print(f\"Performance difference: {udf_time/builtin_time:.2f}x slower without pushdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Pushdown and Column Pruning\n",
    "\n",
    "Projection pushdown (also known as column pruning) works by reading only the columns needed for a query. This reduces I/O, network transfer, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few Columns Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#6344,name#6345,price#6348] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double>\n",
      "\n",
      "\n",
      "\n",
      "Many Columns Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.products_table[product_id#6344,name#6345,category_id#6346,subcategory_id#6347,price#6348,attributes#6349,tags#6350,created_at#6351] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,category_id:int,subcategory_id:int,price:double,attributes:map<...\n",
      "\n",
      "\n",
      "\n",
      "Complex Columns Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) Project [product_id#6344, name#6345, price#6348, attributes#6349[color] AS color#6520, attributes#6349[size] AS size#6521, tags#6350[0] AS first_tag#6522, (price#6348 * 1.1) AS price_with_tax#6523]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.products_table[product_id#6344,name#6345,price#6348,attributes#6349,tags#6350] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,name:string,price:double,attributes:map<string,string>,tags:array<string>>\n",
      "\n",
      "\n",
      "Few columns execution time: 0.0390 seconds\n",
      "Many columns execution time: 0.0445 seconds\n",
      "Complex columns execution time: 0.0322 seconds\n",
      "\n",
      "Performance comparison:\n",
      "Reading all columns is 1.14x slower than selecting few columns\n",
      "Complex column selection is 0.82x slower than simple column selection\n"
     ]
    }
   ],
   "source": [
    "# Compare column pruning effectiveness with different queries\n",
    "\n",
    "# 1. Select few columns\n",
    "few_columns = spark.table(\"products_table\").select(\"product_id\", \"name\", \"price\")\n",
    "\n",
    "# 2. Select many columns\n",
    "many_columns = spark.table(\"products_table\").select(\"*\")\n",
    "\n",
    "# 3. Select with complex derived columns\n",
    "complex_columns = spark.table(\"products_table\").select(\n",
    "    \"product_id\", \n",
    "    \"name\", \n",
    "    \"price\",\n",
    "    col(\"attributes\").getItem(\"color\").alias(\"color\"),\n",
    "    col(\"attributes\").getItem(\"size\").alias(\"size\"),\n",
    "    col(\"tags\")[0].alias(\"first_tag\"),\n",
    "    (col(\"price\") * 1.1).alias(\"price_with_tax\")\n",
    ")\n",
    "\n",
    "# Compare plans and execution times\n",
    "print(\"Few Columns Query Plan:\")\n",
    "few_columns.explain()\n",
    "\n",
    "print(\"\\nMany Columns Query Plan:\")\n",
    "many_columns.explain()\n",
    "\n",
    "print(\"\\nComplex Columns Query Plan:\")\n",
    "complex_columns.explain()\n",
    "\n",
    "# Measure performance\n",
    "few_time, _ = time_execution_with_metrics(few_columns, name=\"Few columns\")\n",
    "many_time, _ = time_execution_with_metrics(many_columns, name=\"Many columns\")\n",
    "complex_time, _ = time_execution_with_metrics(complex_columns, name=\"Complex columns\")\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"Reading all columns is {many_time/few_time:.2f}x slower than selecting few columns\")\n",
    "print(f\"Complex column selection is {complex_time/few_time:.2f}x slower than simple column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Filter Optimization Techniques\n",
    "\n",
    "1. **Filter Order**: Apply the most selective filters first to reduce data volume early\n",
    "2. **Partition Pruning**: Filter on partition columns for massive performance gains\n",
    "3. **File Skipping**: Use filters that can leverage statistics for file-level skipping\n",
    "4. **Predicate Reordering**: The optimizer can reorder predicates for better performance\n",
    "5. **Avoid UDFs in Filters**: UDFs prevent pushdown, use built-in functions when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition First Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 1000.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022/..., PartitionFilters: [isnotnull(year#6378), isnotnull(month#6379), (year#6378 = 2022), (month#6379 = 12)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,1000.0)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Non-Partition First Query Plan:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(total_amount#6375) AND (total_amount#6375 > 1000.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 1000.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022/..., PartitionFilters: [isnotnull(year#6378), isnotnull(month#6379), (year#6378 = 2022), (month#6379 = 12)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,1000.0)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "Partition first execution time: 0.0589 seconds\n",
      "Non-partition first execution time: 0.0454 seconds\n",
      "\n",
      "Note: Both plans should have similar performance due to the query optimizer's ability to reorder filters.\n",
      "Actual performance ratio: 0.77x\n"
     ]
    }
   ],
   "source": [
    "# Let's examine partition pruning with different filter orderings\n",
    "\n",
    "# 1. Filter on partition columns first\n",
    "partition_first = spark.table(\"transactions_table\") \\\n",
    "    .filter(col(\"year\") == 2022) \\\n",
    "    .filter(col(\"month\") == 12) \\\n",
    "    .filter(col(\"total_amount\") > 1000)\n",
    "\n",
    "# 2. Filter on non-partition columns first\n",
    "non_partition_first = spark.table(\"transactions_table\") \\\n",
    "    .filter(col(\"total_amount\") > 1000) \\\n",
    "    .filter(col(\"year\") == 2022) \\\n",
    "    .filter(col(\"month\") == 12)\n",
    "\n",
    "# Compare plans\n",
    "print(\"Partition First Query Plan:\")\n",
    "partition_first.explain()\n",
    "\n",
    "print(\"\\nNon-Partition First Query Plan:\")\n",
    "non_partition_first.explain()\n",
    "\n",
    "# Measure performance\n",
    "partition_time, _ = time_execution_with_metrics(partition_first, name=\"Partition first\")\n",
    "non_partition_time, _ = time_execution_with_metrics(non_partition_first, name=\"Non-partition first\")\n",
    "\n",
    "print(f\"\\nNote: Both plans should have similar performance due to the query optimizer's ability to reorder filters.\")\n",
    "print(f\"Actual performance ratio: {non_partition_time/partition_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Partition Pruning\n",
    "\n",
    "Dynamic partition pruning is an optimization technique that determines which partitions to read based on filters that are only known at runtime. This often happens with join conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with Dynamic Partition Pruning:\n",
      "== Physical Plan ==\n",
      "*(3) Project [transaction_id#6369, customer_id#6370, total_amount#6375]\n",
      "+- *(3) BroadcastHashJoin [year#6378, month#6379], [year#6639, month#6640], Inner, BuildRight, false\n",
      "   :- *(3) ColumnarToRow\n",
      "   :  +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(64 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2024..., PartitionFilters: [isnotnull(year#6378), isnotnull(month#6379), dynamicpruningexpression(year#6378 IN dynamicprunin..., PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,total_amount:double>\n",
      "   :        :- SubqueryBroadcast dynamicpruning#6645, 0, [year#6639, month#6640], [id=#1022]\n",
      "   :        :  +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[1, int, true] as bigint) & 4294967295))),false), [plan_id=1021]\n",
      "   :        :     +- *(2) Project [year#6639, month#6640]\n",
      "   :        :        +- *(2) Filter (isnotnull(total_sales#6623) AND (total_sales#6623 > 100000.0))\n",
      "   :        :           +- *(2) HashAggregate(keys=[year#6639, month#6640], functions=[sum(total_amount#6636)])\n",
      "   :        :              +- Exchange hashpartitioning(year#6639, month#6640, 10), ENSURE_REQUIREMENTS, [plan_id=1015]\n",
      "   :        :                 +- *(1) HashAggregate(keys=[year#6639, month#6640], functions=[partial_sum(total_amount#6636)])\n",
      "   :        :                    +- *(1) ColumnarToRow\n",
      "   :        :                       +- FileScan parquet spark_catalog.default.transactions_table[total_amount#6636,year#6639,month#6640] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(64 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2024..., PartitionFilters: [isnotnull(year#6639), isnotnull(month#6640)], PushedFilters: [], ReadSchema: struct<total_amount:double>\n",
      "   :        +- SubqueryBroadcast dynamicpruning#6646, 1, [year#6639, month#6640], [id=#1146]\n",
      "   :           +- ReusedExchange [year#6639, month#6640], BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[1, int, true] as bigint) & 4294967295))),false), [plan_id=1021]\n",
      "   +- ReusedExchange [year#6639, month#6640], BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[1, int, true] as bigint) & 4294967295))),false), [plan_id=1021]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic partition pruning execution time: 1.2028 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.202821969985962, 100000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample partitioned table for this example\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_by_month AS\n",
    "SELECT\n",
    "    year,\n",
    "    month,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(total_amount) as total_sales\n",
    "FROM transactions_table\n",
    "GROUP BY year, month\n",
    "\"\"\")\n",
    "\n",
    "# Now let's try dynamic partition pruning with a join\n",
    "query = spark.sql(\"\"\"\n",
    "SELECT t.transaction_id, t.customer_id, t.total_amount\n",
    "FROM transactions_table t\n",
    "JOIN sales_by_month s ON t.year = s.year AND t.month = s.month\n",
    "WHERE s.total_sales > 100000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join with Dynamic Partition Pruning:\")\n",
    "query.explain()\n",
    "\n",
    "# Check execution metrics\n",
    "time_execution_with_metrics(query, name=\"Dynamic partition pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Filter and Projection Pushdown\n",
    "\n",
    "To maximize the benefits of filter and projection pushdown:\n",
    "\n",
    "1. **Use compatible file formats**: Parquet, ORC, and Delta support pushdown effectively\n",
    "2. **Select only required columns**: Avoid `select(\"*\")` when possible\n",
    "3. **Use built-in functions**: Avoid UDFs in filter conditions\n",
    "4. **Filter on partition columns**: Design your partitioning scheme for common query patterns\n",
    "5. **Compute and maintain statistics**: Enable better file skipping\n",
    "6. **Apply filters early**: Push filters as close to the data source as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Detailed Join Strategy Analysis\n",
    "\n",
    "Joins are often the most expensive operations in Spark queries. Understanding different join strategies and their performance characteristics is crucial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current broadcast join threshold: 10m\n",
      "\n",
      "Table sizes:\n",
      "+------------------+\n",
      "|transactions_count|\n",
      "+------------------+\n",
      "|            100000|\n",
      "+------------------+\n",
      "\n",
      "+---------------+\n",
      "|customers_count|\n",
      "+---------------+\n",
      "|          50000|\n",
      "+---------------+\n",
      "\n",
      "+--------------+\n",
      "|products_count|\n",
      "+--------------+\n",
      "|          5000|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's first check the current broadcast join threshold configuration\n",
    "broadcast_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Current broadcast join threshold: {broadcast_threshold}\")\n",
    "\n",
    "# Check table sizes to understand broadcasting decisions\n",
    "print(\"\\nTable sizes:\")\n",
    "spark.sql(\"SELECT COUNT(*) AS transactions_count FROM transactions_table\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS customers_count FROM customers_table\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS products_count FROM products_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Join Strategy:\n",
      "== Physical Plan ==\n",
      "*(2) Project [transaction_id#6369, name#6698 AS customer_name#6724, total_amount#6375]\n",
      "+- *(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=1594]\n",
      "   :  +- *(1) Project [transaction_id#6369, customer_id#6370, total_amount#6375]\n",
      "   :     +- *(1) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,total_amount:double>\n",
      "   +- *(2) Filter isnotnull(customer_id#6697)\n",
      "      +- *(2) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's examine different join types and how Spark chooses them\n",
    "# First, create a common join query\n",
    "customer_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "# Analyze the join plan\n",
    "print(\"Default Join Strategy:\")\n",
    "customer_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join without broadcasting:\n",
      "== Physical Plan ==\n",
      "*(5) Project [transaction_id#6369, name#6698 AS customer_name#6729, total_amount#6375]\n",
      "+- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "   :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#6370, 10), ENSURE_REQUIREMENTS, [plan_id=1659]\n",
      "   :     +- *(1) Project [transaction_id#6369, customer_id#6370, total_amount#6375]\n",
      "   :        +- *(1) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      "   :           +- *(1) ColumnarToRow\n",
      "   :              +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,total_amount:double>\n",
      "   +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#6697, 10), ENSURE_REQUIREMENTS, [plan_id=1668]\n",
      "         +- *(3) Filter isnotnull(customer_id#6697)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's disable auto-broadcasting and force a shuffle hash join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Run the same query with broadcast disabled\n",
    "no_broadcast_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join without broadcasting:\")\n",
    "no_broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with broadcast hint:\n",
      "== Physical Plan ==\n",
      "*(2) Project [transaction_id#6369, name#6698 AS customer_name#6733, total_amount#6375]\n",
      "+- *(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildRight, false\n",
      "   :- *(2) Project [transaction_id#6369, customer_id#6370, total_amount#6375]\n",
      "   :  +- *(2) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      "   :     +- *(2) ColumnarToRow\n",
      "   :        +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,total_amount:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1738]\n",
      "      +- *(1) Filter isnotnull(customer_id#6697)\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explicitly force a broadcast join using the broadcast hint\n",
    "# Reset broadcast threshold first\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", broadcast_threshold)\n",
    "\n",
    "# Use broadcast hint\n",
    "forced_broadcast = spark.sql(\"\"\"\n",
    "SELECT /*+ BROADCAST(c) */\n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join with broadcast hint:\")\n",
    "forced_broadcast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Join Strategies\n",
    "\n",
    "Let's compare the performance of different join strategies on the same query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast Hash Join Plan:\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildRight, false\n",
      ":- *(2) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      ":  +- *(2) ColumnarToRow\n",
      ":     +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1785]\n",
      "   +- *(1) Filter isnotnull(customer_id#6697)\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "Broadcast Hash Join execution time: 0.2435 seconds\n",
      "\n",
      "Sort Merge Join Plan:\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      ":- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(customer_id#6370, 10), ENSURE_REQUIREMENTS, [plan_id=1927]\n",
      ":     +- *(1) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      ":        +- *(1) ColumnarToRow\n",
      ":           +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "+- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(customer_id#6697, 10), ENSURE_REQUIREMENTS, [plan_id=1936]\n",
      "      +- *(3) Filter isnotnull(customer_id#6697)\n",
      "         +- *(3) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "Sort Merge Join execution time: 0.3647 seconds\n",
      "\n",
      "Shuffle Hash Join Plan:\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      ":- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(customer_id#6370, 10), ENSURE_REQUIREMENTS, [plan_id=2119]\n",
      ":     +- *(1) Filter ((isnotnull(total_amount#6375) AND (total_amount#6375 > 500.0)) AND isnotnull(customer_id#6370))\n",
      ":        +- *(1) ColumnarToRow\n",
      ":           +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(total_amount#6375), (total_amount#6375 > 500.0), isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(total_amount), GreaterThan(total_amount,500.0), IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "+- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(customer_id#6697, 10), ENSURE_REQUIREMENTS, [plan_id=2128]\n",
      "      +- *(3) Filter isnotnull(customer_id#6697)\n",
      "         +- *(3) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "Shuffle Hash Join execution time: 0.1865 seconds\n",
      "\n",
      "Join Strategy Performance Comparison:\n",
      "Broadcast Hash Join: 0.2435 seconds (baseline)\n",
      "Sort Merge Join: 0.3647 seconds (1.50x vs broadcast)\n",
      "Shuffle Hash Join: 0.1865 seconds (0.77x vs broadcast)\n"
     ]
    }
   ],
   "source": [
    "# Let's systematically compare different join strategies\n",
    "# We'll use DataFrame API for easier control\n",
    "\n",
    "# Create the base tables for joins\n",
    "transactions = spark.table(\"transactions_table\").filter((col(\"year\") == 2022) & (col(\"total_amount\") > 500))\n",
    "customers = spark.table(\"customers_table\")\n",
    "\n",
    "# 1. Broadcast Hash Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50m\")  # Ensure broadcasting\n",
    "broadcast_join = transactions.join(broadcast(customers), transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"Broadcast Hash Join Plan:\")\n",
    "broadcast_join.explain()\n",
    "broadcast_time, _ = time_execution_with_metrics(broadcast_join, name=\"Broadcast Hash Join\")\n",
    "\n",
    "# 2. Sort Merge Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable broadcasting\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")  # Prefer sort merge\n",
    "sort_merge_join = transactions.join(customers, transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"\\nSort Merge Join Plan:\")\n",
    "sort_merge_join.explain()\n",
    "sort_merge_time, _ = time_execution_with_metrics(sort_merge_join, name=\"Sort Merge Join\")\n",
    "\n",
    "# 3. Shuffle Hash Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable broadcasting\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")  # Disable preference for sort merge\n",
    "shuffle_hash_join = transactions.join(customers, transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"\\nShuffle Hash Join Plan:\")\n",
    "shuffle_hash_join.explain()\n",
    "shuffle_hash_time, _ = time_execution_with_metrics(shuffle_hash_join, name=\"Shuffle Hash Join\")\n",
    "\n",
    "# Reset configs to default\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", broadcast_threshold)\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nJoin Strategy Performance Comparison:\")\n",
    "print(f\"Broadcast Hash Join: {broadcast_time:.4f} seconds (baseline)\")\n",
    "print(f\"Sort Merge Join: {sort_merge_time:.4f} seconds ({sort_merge_time/broadcast_time:.2f}x vs broadcast)\")\n",
    "print(f\"Shuffle Hash Join: {shuffle_hash_time:.4f} seconds ({shuffle_hash_time/broadcast_time:.2f}x vs broadcast)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Order and Join Reordering\n",
    "\n",
    "Join order can significantly impact performance. The Catalyst optimizer attempts to reorder joins optimally, but sometimes it needs help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction table schema:\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: timestamp (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- discount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- metadata: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Complex Join Plan:\n",
      "== Physical Plan ==\n",
      "*(3) Project [transaction_id#6369, customer_name#6997, name#6345 AS product_name#7037, quantity#7010, item_price#7008 AS price#7038]\n",
      "+- *(3) BroadcastHashJoin [product_id#7007], [product_id#6344], Inner, BuildRight, false\n",
      "   :- *(3) Project [transaction_id#6369, customer_name#6997, item#7002.product_id AS product_id#7007, item#7002.quantity AS quantity#7010, item#7002.price AS item_price#7008]\n",
      "   :  +- *(3) Filter isnotnull(item#7002.product_id)\n",
      "   :     +- *(3) Generate explode(items#6372), [transaction_id#6369, customer_name#6997], false, [item#7002]\n",
      "   :        +- *(3) Project [transaction_id#6369, name#6698 AS customer_name#6997, items#6372]\n",
      "   :           +- *(3) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildLeft, false\n",
      "   :              :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=2361]\n",
      "   :              :  +- *(1) Project [transaction_id#6369, customer_id#6370, items#6372]\n",
      "   :              :     +- *(1) Filter ((isnotnull(customer_id#6370) AND (size(items#6372, true) > 0)) AND isnotnull(items#6372))\n",
      "   :              :        +- *(1) ColumnarToRow\n",
      "   :              :           +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,items#6372,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370), (size(items#6372, true) > 0), isnotnull(items#6372)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022/..., PartitionFilters: [isnotnull(year#6378), isnotnull(month#6379), (year#6378 = 2022), (month#6379 = 12)], PushedFilters: [IsNotNull(customer_id), IsNotNull(items)], ReadSchema: struct<transaction_id:int,customer_id:int,items:array<struct<product_id:int,quantity:int,price:do...\n",
      "   :              +- *(3) Filter isnotnull(customer_id#6697)\n",
      "   :                 +- *(3) ColumnarToRow\n",
      "   :                    +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2375]\n",
      "      +- *(2) Filter isnotnull(product_id#6344)\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.products_table[product_id#6344,name#6345] Batched: true, DataFilters: [isnotnull(product_id#6344)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,name:string>\n",
      "\n",
      "\n",
      "\n",
      "Complex Join with Hints Plan:\n",
      "== Physical Plan ==\n",
      "*(3) Project [transaction_id#6369, customer_name#7078, name#6345 AS product_name#7118, quantity#7091, item_price#7089 AS price#7119]\n",
      "+- *(3) BroadcastHashJoin [product_id#7088], [product_id#6344], Inner, BuildRight, false\n",
      "   :- *(3) Project [transaction_id#6369, customer_name#7078, item#7083.product_id AS product_id#7088, item#7083.quantity AS quantity#7091, item#7083.price AS item_price#7089]\n",
      "   :  +- *(3) Filter isnotnull(item#7083.product_id)\n",
      "   :     +- *(3) Generate explode(items#6372), [transaction_id#6369, customer_name#7078], false, [item#7083]\n",
      "   :        +- *(3) Project [transaction_id#6369, name#6698 AS customer_name#7078, items#6372]\n",
      "   :           +- *(3) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildRight, false\n",
      "   :              :- *(3) Project [transaction_id#6369, customer_id#6370, items#6372]\n",
      "   :              :  +- *(3) Filter ((isnotnull(customer_id#6370) AND (size(items#6372, true) > 0)) AND isnotnull(items#6372))\n",
      "   :              :     +- *(3) ColumnarToRow\n",
      "   :              :        +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,items#6372,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370), (size(items#6372, true) > 0), isnotnull(items#6372)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022/..., PartitionFilters: [isnotnull(year#6378), isnotnull(month#6379), (year#6378 = 2022), (month#6379 = 12)], PushedFilters: [IsNotNull(customer_id), IsNotNull(items)], ReadSchema: struct<transaction_id:int,customer_id:int,items:array<struct<product_id:int,quantity:int,price:do...\n",
      "   :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2488]\n",
      "   :                 +- *(1) Filter isnotnull(customer_id#6697)\n",
      "   :                    +- *(1) ColumnarToRow\n",
      "   :                       +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2499]\n",
      "      +- *(2) Filter isnotnull(product_id#6344)\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.products_table[product_id#6344,name#6345] Batched: true, DataFilters: [isnotnull(product_id#6344)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/products_table], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,name:string>\n",
      "\n",
      "\n",
      "Standard Join Order execution time: 0.1840 seconds\n",
      "Hinted Join Order execution time: 0.1429 seconds\n",
      "\n",
      "Performance impact of join order: 1.29x improvement with hints\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze join order optimization with a 3-way join\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Check the schema to understand the structure\n",
    "print(\"Transaction table schema:\")\n",
    "spark.table(\"transactions_table\").printSchema()\n",
    "\n",
    "# Use DataFrame API approach which is more reliable for nested structures\n",
    "# Version without hints - fixing the ambiguous column references\n",
    "complex_join_df = (\n",
    "    spark.table(\"transactions_table\").filter((col(\"year\") == 2022) & (col(\"month\") == 12))\n",
    "    .select(\"transaction_id\", \"customer_id\", \"items\")\n",
    "    .join(spark.table(\"customers_table\"), \"customer_id\")\n",
    "    .select(\"transaction_id\", col(\"name\").alias(\"customer_name\"), \"items\")\n",
    "    .withColumn(\"item\", explode(\"items\"))\n",
    "    .select(\"transaction_id\", \"customer_name\", \n",
    "            col(\"item.product_id\").alias(\"product_id\"), \n",
    "            col(\"item.quantity\"), col(\"item.price\").alias(\"item_price\"))\n",
    "    .join(spark.table(\"products_table\"), \"product_id\")\n",
    "    .select(\"transaction_id\", \"customer_name\", col(\"name\").alias(\"product_name\"), \n",
    "           \"quantity\", col(\"item_price\").alias(\"price\"))\n",
    ")\n",
    "\n",
    "# Analyze the plan\n",
    "print(\"Complex Join Plan:\")\n",
    "complex_join_df.explain()\n",
    "\n",
    "# Now try with hints using DataFrame API \n",
    "hinted_join_df = (\n",
    "    spark.table(\"transactions_table\").filter((col(\"year\") == 2022) & (col(\"month\") == 12))\n",
    "    .select(\"transaction_id\", \"customer_id\", \"items\")\n",
    "    .join(spark.table(\"customers_table\").hint(\"broadcast\"), \"customer_id\")\n",
    "    .select(\"transaction_id\", col(\"name\").alias(\"customer_name\"), \"items\")\n",
    "    .withColumn(\"item\", explode(\"items\"))\n",
    "    .select(\"transaction_id\", \"customer_name\", \n",
    "            col(\"item.product_id\").alias(\"product_id\"), \n",
    "            col(\"item.quantity\"), col(\"item.price\").alias(\"item_price\"))\n",
    "    .join(spark.table(\"products_table\").hint(\"broadcast\"), \"product_id\")\n",
    "    .select(\"transaction_id\", \"customer_name\", col(\"name\").alias(\"product_name\"), \n",
    "           \"quantity\", col(\"item_price\").alias(\"price\"))\n",
    ")\n",
    "\n",
    "print(\"\\nComplex Join with Hints Plan:\")\n",
    "hinted_join_df.explain()\n",
    "\n",
    "# Compare performance\n",
    "complex_time, _ = time_execution_with_metrics(complex_join_df, name=\"Standard Join Order\")\n",
    "hinted_time, _ = time_execution_with_metrics(hinted_join_df, name=\"Hinted Join Order\")\n",
    "\n",
    "print(f\"\\nPerformance impact of join order: {complex_time/hinted_time:.2f}x improvement with hints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Data Skew in Joins\n",
    "\n",
    "Data skew can cause severe performance problems in joins, especially with sort-merge and shuffle hash joins. Let's examine techniques to identify and handle skew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with potential skew:\n",
      "== Physical Plan ==\n",
      "*(5) Project [transaction_id#6369, name#6698 AS customer_name#7173, total_amount#6375, transaction_date#6371]\n",
      "+- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "   :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#6370, 10), ENSURE_REQUIREMENTS, [plan_id=2904]\n",
      "   :     +- *(1) Project [transaction_id#6369, customer_id#6370, transaction_date#6371, total_amount#6375]\n",
      "   :        +- *(1) Filter isnotnull(customer_id#6370)\n",
      "   :           +- *(1) ColumnarToRow\n",
      "   :              +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,total_amount:double>\n",
      "   +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#6697, 10), ENSURE_REQUIREMENTS, [plan_id=2913]\n",
      "         +- *(3) Filter isnotnull(customer_id#6697)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n",
      "\n",
      "Top 10 customers by transaction count:\n",
      "+-----------+-----------------+\n",
      "|customer_id|transaction_count|\n",
      "+-----------+-----------------+\n",
      "|          1|             2589|\n",
      "|          2|             1450|\n",
      "|          3|             1141|\n",
      "|          4|              850|\n",
      "|          5|              692|\n",
      "|          6|              649|\n",
      "|          7|              542|\n",
      "|          8|              525|\n",
      "|          9|              462|\n",
      "|         10|              427|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a query that will exhibit skew due to the distribution of our data\n",
    "# Remember that we created transactions with power-law distribution of customer_ids\n",
    "skewed_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount,\n",
    "    t.transaction_date\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Force a sort merge join to see the impact of skew\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "print(\"Join with potential skew:\")\n",
    "skewed_join.explain()\n",
    "\n",
    "# Check distribution of customer_ids in our transactions\n",
    "customer_distribution = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id, \n",
    "    COUNT(*) as transaction_count\n",
    "FROM \n",
    "    transactions_table\n",
    "GROUP BY \n",
    "    customer_id\n",
    "ORDER BY \n",
    "    transaction_count DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTop 10 customers by transaction count:\")\n",
    "customer_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 1046 skewed customer keys\n",
      "\n",
      "Plan with salting technique:\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [customer_id#6370, salt#7243], [customer_id#6697, salt#7268], Inner\n",
      ":- *(2) Sort [customer_id#6370 ASC NULLS FIRST, salt#7243 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(customer_id#6370, salt#7243, 10), ENSURE_REQUIREMENTS, [plan_id=3135]\n",
      ":     +- *(1) Filter (isnotnull(customer_id#6370) AND isnotnull(salt#7243))\n",
      ":        +- *(1) Project [transaction_id#6369, customer_id#6370, transaction_date#6371, items#6372, payment_method#6373, status#6374, total_amount#6375, store_id#6376, metadata#6377, year#6378, month#6379, CASE WHEN customer_id#6370 INSET 1, 10, 100, 1005, 1007, 1008, 1009, 101, 1012, 1013, 1016, 1017, 1018, 1019, 102, 1020, 1021, 1022, 1025, 1028, 103, 1031, 1033, 1039, 104, 1041, 1042, 1043, 1044, 1045, 1047, 105, 1053, 1059, 106, 1061, 1062, 1063, 107, 1070, 1071, 1072, 1074, 108, 1080, 1081, 1084, 109, 1092, 1098, 1099, 11, 110, 1103, 1107, 1108, 111, 1111, 1112, 1114, 1118, 112, 1121, 1123, 1125, 1127, 1129, 113, 1131, 1132, 1133, 114, 1145, 1147, 1149, 115, 1153, 1156, 1157, 1158, 116, 1163, 1166, 1168, 1169, 117, 1172, 1173, 1175, 1179, 118, 1180, 1183, 1184, 1187, 1189, 119, 1190, 1194, 1196, 12, 120, 1200, 1206, 1209, 121, 1211, 122, 1225, 1227, 1228, 1229, 123, 1231, 1237, 1238, 1239, 124, 125, 126, 1260, 127, 1270, 1276, 128, 1281, 1284, 129, 1290, 1291, 1292, 1298, 13, 130, 1303, 131, 1312, 1315, 132, 1321, 1323, 1327, 133, 1333, 134, 1340, 1341, 135, 1356, 136, 1360, 1366, 1369, 137, 1371, 138, 1383, 1388, 139, 1392, 14, 140, 1405, 141, 1411, 1412, 1417, 1418, 142, 1422, 1424, 1426, 143, 144, 145, 1451, 146, 1465, 1467, 147, 1478, 1479, 148, 1481, 149, 1494, 15, 150, 151, 1516, 152, 153, 154, 1540, 1542, 1546, 1549, 155, 156, 1566, 1568, 157, 1573, 1574, 158, 1583, 159, 1597, 16, 160, 1600, 1603, 161, 162, 1626, 163, 164, 1648, 165, 1650, 166, 1664, 167, 168, 1682, 1685, 169, 17, 170, 171, 1710, 1712, 1718, 172, 1722, 173, 174, 1748, 175, 1750, 176, 1763, 1765, 177, 178, 179, 18, 180, 181, 1813, 182, 183, 184, 185, 1859, 186, 187, 188, 1880, 189, 19, 190, 191, 192, 193, 194, 1943, 195, 196, 1969, 197, 198, 1984, 199, 2, 20, 200, 201, 202, 2028, 203, 2031, 2037, 204, 205, 2055, 206, 2064, 207, 208, 2085, 209, 21, 210, 2100, 2105, 211, 212, 213, 214, 2148, 215, 216, 217, 218, 2182, 219, 22, 220, 221, 222, 223, 224, 225, 226, 2268, 227, 228, 2280, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 2393, 24, 240, 241, 2413, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 2518, 252, 253, 254, 255, 256, 257, 2577, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 3586, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 4628, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 628, 629, 63, 630, 631, 632, 633, 634, 635, 638, 639, 64, 641, 642, 643, 645, 646, 647, 648, 649, 65, 650, 653, 655, 656, 657, 658, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 673, 675, 677, 678, 68, 680, 681, 682, 683, 684, 685, 686, 687, 69, 690, 691, 692, 694, 696, 697, 698, 7, 70, 700, 702, 703, 704, 705, 706, 708, 709, 71, 710, 711, 713, 715, 716, 717, 718, 719, 72, 720, 721, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 748, 749, 75, 750, 752, 753, 755, 756, 757, 758, 759, 76, 761, 762, 763, 766, 767, 768, 769, 77, 774, 775, 779, 78, 780, 781, 782, 783, 784, 786, 789, 79, 790, 791, 792, 793, 796, 798, 799, 8, 80, 800, 801, 802, 803, 804, 806, 808, 809, 81, 810, 811, 812, 814, 815, 817, 819, 82, 820, 821, 822, 823, 824, 825, 826, 827, 828, 83, 831, 832, 834, 835, 836, 837, 839, 84, 840, 841, 842, 843, 845, 847, 849, 85, 850, 851, 853, 856, 857, 858, 86, 863, 864, 866, 868, 87, 870, 872, 873, 874, 875, 876, 877, 878, 879, 88, 882, 885, 886, 888, 889, 89, 890, 893, 894, 896, 897, 9, 90, 900, 901, 903, 904, 907, 909, 91, 913, 915, 916, 917, 918, 919, 92, 920, 921, 922, 923, 924, 925, 926, 93, 930, 934, 935, 938, 94, 940, 941, 942, 944, 945, 948, 95, 951, 96, 964, 967, 968, 969, 97, 970, 972, 974, 98, 980, 982, 983, 984, 985, 988, 989, 99, 990, 993, 995 THEN cast((rand(2680558529334826509) * 10.0) as int) ELSE 0 END AS salt#7243]\n",
      ":           +- *(1) ColumnarToRow\n",
      ":              +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "+- *(4) Sort [customer_id#6697 ASC NULLS FIRST, salt#7268 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(customer_id#6697, salt#7268, 10), ENSURE_REQUIREMENTS, [plan_id=3146]\n",
      "      +- *(3) Generate explode(salt_values#7256), [customer_id#6697, name#6698, email#6699, address#6700, phone#6701, signup_date#6702, last_activity#6703, tier#6704, preferences#6705], false, [salt#7268]\n",
      "         +- *(3) Project [customer_id#6697, name#6698, email#6699, address#6700, phone#6701, signup_date#6702, last_activity#6703, tier#6704, preferences#6705, CASE WHEN customer_id#6697 INSET 1, 10, 100, 1005, 1007, 1008, 1009, 101, 1012, 1013, 1016, 1017, 1018, 1019, 102, 1020, 1021, 1022, 1025, 1028, 103, 1031, 1033, 1039, 104, 1041, 1042, 1043, 1044, 1045, 1047, 105, 1053, 1059, 106, 1061, 1062, 1063, 107, 1070, 1071, 1072, 1074, 108, 1080, 1081, 1084, 109, 1092, 1098, 1099, 11, 110, 1103, 1107, 1108, 111, 1111, 1112, 1114, 1118, 112, 1121, 1123, 1125, 1127, 1129, 113, 1131, 1132, 1133, 114, 1145, 1147, 1149, 115, 1153, 1156, 1157, 1158, 116, 1163, 1166, 1168, 1169, 117, 1172, 1173, 1175, 1179, 118, 1180, 1183, 1184, 1187, 1189, 119, 1190, 1194, 1196, 12, 120, 1200, 1206, 1209, 121, 1211, 122, 1225, 1227, 1228, 1229, 123, 1231, 1237, 1238, 1239, 124, 125, 126, 1260, 127, 1270, 1276, 128, 1281, 1284, 129, 1290, 1291, 1292, 1298, 13, 130, 1303, 131, 1312, 1315, 132, 1321, 1323, 1327, 133, 1333, 134, 1340, 1341, 135, 1356, 136, 1360, 1366, 1369, 137, 1371, 138, 1383, 1388, 139, 1392, 14, 140, 1405, 141, 1411, 1412, 1417, 1418, 142, 1422, 1424, 1426, 143, 144, 145, 1451, 146, 1465, 1467, 147, 1478, 1479, 148, 1481, 149, 1494, 15, 150, 151, 1516, 152, 153, 154, 1540, 1542, 1546, 1549, 155, 156, 1566, 1568, 157, 1573, 1574, 158, 1583, 159, 1597, 16, 160, 1600, 1603, 161, 162, 1626, 163, 164, 1648, 165, 1650, 166, 1664, 167, 168, 1682, 1685, 169, 17, 170, 171, 1710, 1712, 1718, 172, 1722, 173, 174, 1748, 175, 1750, 176, 1763, 1765, 177, 178, 179, 18, 180, 181, 1813, 182, 183, 184, 185, 1859, 186, 187, 188, 1880, 189, 19, 190, 191, 192, 193, 194, 1943, 195, 196, 1969, 197, 198, 1984, 199, 2, 20, 200, 201, 202, 2028, 203, 2031, 2037, 204, 205, 2055, 206, 2064, 207, 208, 2085, 209, 21, 210, 2100, 2105, 211, 212, 213, 214, 2148, 215, 216, 217, 218, 2182, 219, 22, 220, 221, 222, 223, 224, 225, 226, 2268, 227, 228, 2280, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 2393, 24, 240, 241, 2413, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 2518, 252, 253, 254, 255, 256, 257, 2577, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 3586, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 4628, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 628, 629, 63, 630, 631, 632, 633, 634, 635, 638, 639, 64, 641, 642, 643, 645, 646, 647, 648, 649, 65, 650, 653, 655, 656, 657, 658, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 673, 675, 677, 678, 68, 680, 681, 682, 683, 684, 685, 686, 687, 69, 690, 691, 692, 694, 696, 697, 698, 7, 70, 700, 702, 703, 704, 705, 706, 708, 709, 71, 710, 711, 713, 715, 716, 717, 718, 719, 72, 720, 721, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 748, 749, 75, 750, 752, 753, 755, 756, 757, 758, 759, 76, 761, 762, 763, 766, 767, 768, 769, 77, 774, 775, 779, 78, 780, 781, 782, 783, 784, 786, 789, 79, 790, 791, 792, 793, 796, 798, 799, 8, 80, 800, 801, 802, 803, 804, 806, 808, 809, 81, 810, 811, 812, 814, 815, 817, 819, 82, 820, 821, 822, 823, 824, 825, 826, 827, 828, 83, 831, 832, 834, 835, 836, 837, 839, 84, 840, 841, 842, 843, 845, 847, 849, 85, 850, 851, 853, 856, 857, 858, 86, 863, 864, 866, 868, 87, 870, 872, 873, 874, 875, 876, 877, 878, 879, 88, 882, 885, 886, 888, 889, 89, 890, 893, 894, 896, 897, 9, 90, 900, 901, 903, 904, 907, 909, 91, 913, 915, 916, 917, 918, 919, 92, 920, 921, 922, 923, 924, 925, 926, 93, 930, 934, 935, 938, 94, 940, 941, 942, 944, 945, 948, 95, 951, 96, 964, 967, 968, 969, 97, 970, 972, 974, 98, 980, 982, 983, 984, 985, 988, 989, 99, 990, 993, 995 THEN [0,1,2,3,4,5,6,7,8,9] ELSE [0] END AS salt_values#7256]\n",
      "            +- *(3) Filter ((size(CASE WHEN customer_id#6697 INSET 1, 10, 100, 1005, 1007, 1008, 1009, 101, 1012, 1013, 1016, 1017, 1018, 1019, 102, 1020, 1021, 1022, 1025, 1028, 103, 1031, 1033, 1039, 104, 1041, 1042, 1043, 1044, 1045, 1047, 105, 1053, 1059, 106, 1061, 1062, 1063, 107, 1070, 1071, 1072, 1074, 108, 1080, 1081, 1084, 109, 1092, 1098, 1099, 11, 110, 1103, 1107, 1108, 111, 1111, 1112, 1114, 1118, 112, 1121, 1123, 1125, 1127, 1129, 113, 1131, 1132, 1133, 114, 1145, 1147, 1149, 115, 1153, 1156, 1157, 1158, 116, 1163, 1166, 1168, 1169, 117, 1172, 1173, 1175, 1179, 118, 1180, 1183, 1184, 1187, 1189, 119, 1190, 1194, 1196, 12, 120, 1200, 1206, 1209, 121, 1211, 122, 1225, 1227, 1228, 1229, 123, 1231, 1237, 1238, 1239, 124, 125, 126, 1260, 127, 1270, 1276, 128, 1281, 1284, 129, 1290, 1291, 1292, 1298, 13, 130, 1303, 131, 1312, 1315, 132, 1321, 1323, 1327, 133, 1333, 134, 1340, 1341, 135, 1356, 136, 1360, 1366, 1369, 137, 1371, 138, 1383, 1388, 139, 1392, 14, 140, 1405, 141, 1411, 1412, 1417, 1418, 142, 1422, 1424, 1426, 143, 144, 145, 1451, 146, 1465, 1467, 147, 1478, 1479, 148, 1481, 149, 1494, 15, 150, 151, 1516, 152, 153, 154, 1540, 1542, 1546, 1549, 155, 156, 1566, 1568, 157, 1573, 1574, 158, 1583, 159, 1597, 16, 160, 1600, 1603, 161, 162, 1626, 163, 164, 1648, 165, 1650, 166, 1664, 167, 168, 1682, 1685, 169, 17, 170, 171, 1710, 1712, 1718, 172, 1722, 173, 174, 1748, 175, 1750, 176, 1763, 1765, 177, 178, 179, 18, 180, 181, 1813, 182, 183, 184, 185, 1859, 186, 187, 188, 1880, 189, 19, 190, 191, 192, 193, 194, 1943, 195, 196, 1969, 197, 198, 1984, 199, 2, 20, 200, 201, 202, 2028, 203, 2031, 2037, 204, 205, 2055, 206, 2064, 207, 208, 2085, 209, 21, 210, 2100, 2105, 211, 212, 213, 214, 2148, 215, 216, 217, 218, 2182, 219, 22, 220, 221, 222, 223, 224, 225, 226, 2268, 227, 228, 2280, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 2393, 24, 240, 241, 2413, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 2518, 252, 253, 254, 255, 256, 257, 2577, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 3586, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 4628, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 628, 629, 63, 630, 631, 632, 633, 634, 635, 638, 639, 64, 641, 642, 643, 645, 646, 647, 648, 649, 65, 650, 653, 655, 656, 657, 658, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 673, 675, 677, 678, 68, 680, 681, 682, 683, 684, 685, 686, 687, 69, 690, 691, 692, 694, 696, 697, 698, 7, 70, 700, 702, 703, 704, 705, 706, 708, 709, 71, 710, 711, 713, 715, 716, 717, 718, 719, 72, 720, 721, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 748, 749, 75, 750, 752, 753, 755, 756, 757, 758, 759, 76, 761, 762, 763, 766, 767, 768, 769, 77, 774, 775, 779, 78, 780, 781, 782, 783, 784, 786, 789, 79, 790, 791, 792, 793, 796, 798, 799, 8, 80, 800, 801, 802, 803, 804, 806, 808, 809, 81, 810, 811, 812, 814, 815, 817, 819, 82, 820, 821, 822, 823, 824, 825, 826, 827, 828, 83, 831, 832, 834, 835, 836, 837, 839, 84, 840, 841, 842, 843, 845, 847, 849, 85, 850, 851, 853, 856, 857, 858, 86, 863, 864, 866, 868, 87, 870, 872, 873, 874, 875, 876, 877, 878, 879, 88, 882, 885, 886, 888, 889, 89, 890, 893, 894, 896, 897, 9, 90, 900, 901, 903, 904, 907, 909, 91, 913, 915, 916, 917, 918, 919, 92, 920, 921, 922, 923, 924, 925, 926, 93, 930, 934, 935, 938, 94, 940, 941, 942, 944, 945, 948, 95, 951, 96, 964, 967, 968, 969, 97, 970, 972, 974, 98, 980, 982, 983, 984, 985, 988, 989, 99, 990, 993, 995 THEN [0,1,2,3,4,5,6,7,8,9] ELSE [0] END, true) > 0) AND isnotnull(customer_id#6697))\n",
      "               +- *(3) ColumnarToRow\n",
      "                  +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [(size(CASE WHEN customer_id#6697 INSET 1, 10, 100, 1005, 1007, 1008, 1009, 101, 1012, 1013, 1016..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "Standard join with skew execution time: 0.6255 seconds\n",
      "Join with salting execution time: 0.7757 seconds\n",
      "\n",
      "Performance impact of salting: 0.81x improvement\n"
     ]
    }
   ],
   "source": [
    "# Techniques to handle skew in joins\n",
    "from pyspark.sql.functions import when, lit, rand, col, explode, array\n",
    "\n",
    "# 1. Using salting to distribute skewed keys\n",
    "# First, identify skewed keys\n",
    "skewed_keys = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id \n",
    "FROM (\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        COUNT(*) as cnt\n",
    "    FROM \n",
    "        transactions_table\n",
    "    GROUP BY \n",
    "        customer_id\n",
    ") t\n",
    "WHERE \n",
    "    cnt > 10\n",
    "\"\"\").collect()\n",
    "\n",
    "skewed_key_list = [row[0] for row in skewed_keys]\n",
    "print(f\"Identified {len(skewed_key_list)} skewed customer keys\")\n",
    "\n",
    "# Apply salting technique for skewed keys\n",
    "transactions_df = spark.table(\"transactions_table\")\n",
    "customers_df = spark.table(\"customers_table\")\n",
    "\n",
    "# Number of salt values\n",
    "num_salts = 10\n",
    "\n",
    "# Add salt to skewed transactions\n",
    "salted_transactions = transactions_df.withColumn(\n",
    "    \"salt\", \n",
    "    when(col(\"customer_id\").isin(skewed_key_list), \n",
    "         (rand() * num_salts).cast(\"int\"))\n",
    "    .otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# Duplicate customers for skewed keys\n",
    "from pyspark.sql.functions import explode, array\n",
    "\n",
    "# Create salt values for skewed customers\n",
    "expanded_customers = customers_df.withColumn(\n",
    "    \"salt_values\",\n",
    "    when(col(\"customer_id\").isin(skewed_key_list),\n",
    "         array([lit(i) for i in range(num_salts)]))\n",
    "    .otherwise(array(lit(0)))\n",
    ")\n",
    "\n",
    "# Explode to create multiple rows for skewed customers\n",
    "salted_customers = expanded_customers \\\n",
    "    .withColumn(\"salt\", explode(\"salt_values\")) \\\n",
    "    .drop(\"salt_values\")\n",
    "\n",
    "# Join with salted keys\n",
    "salted_join = salted_transactions.join(\n",
    "    salted_customers,\n",
    "    (salted_transactions[\"customer_id\"] == salted_customers[\"customer_id\"]) &\n",
    "    (salted_transactions[\"salt\"] == salted_customers[\"salt\"])\n",
    ")\n",
    "\n",
    "print(\"\\nPlan with salting technique:\")\n",
    "salted_join.explain()\n",
    "\n",
    "# Compare performance\n",
    "standard_time, _ = time_execution_with_metrics(skewed_join, name=\"Standard join with skew\")\n",
    "salted_time, _ = time_execution_with_metrics(salted_join, name=\"Join with salting\")\n",
    "\n",
    "print(f\"\\nPerformance impact of salting: {standard_time/salted_time:.2f}x improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Join Optimization Techniques\n",
    "\n",
    "1. **Choose the Right Join Strategy**:\n",
    "   - Broadcast Hash Join: Use for small-to-medium tables that can fit in memory\n",
    "   - Sort Merge Join: Best for large tables with evenly distributed keys\n",
    "   - Shuffle Hash Join: Can be faster than Sort Merge for medium-sized tables\n",
    "\n",
    "2. **Join Order Optimization**:\n",
    "   - Join the most filtered/smallest tables first\n",
    "   - Use the LEADING hint to control join order\n",
    "   - Filter tables before joining when possible\n",
    "\n",
    "3. **Data Skew Handling**:\n",
    "   - Use salting for skewed keys\n",
    "   - Split processing of skewed and non-skewed data\n",
    "   - Consider pre-aggregating data before joins\n",
    "\n",
    "4. **Configuration Tuning**:\n",
    "   - Adjust broadcast threshold based on your data size\n",
    "   - Set appropriate shuffle partition count\n",
    "   - Consider enabling adaptive query execution for dynamic optimization\n",
    "\n",
    "5. **Schema Optimization**:\n",
    "   - Ensure join keys have the same data type to avoid implicit conversions\n",
    "   - Consider clustering or bucketing tables on join keys\n",
    "\n",
    "6. **Join Rewriting**:\n",
    "   - Convert complex multi-way joins to simpler joins when possible\n",
    "   - Use subqueries or CTEs to break down complex joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimizing Shuffle Operations\n",
    "\n",
    "Shuffles are among the most expensive operations in Spark, involving disk I/O, serialization, network transfer, and deserialization. Understanding and optimizing shuffles is critical for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shuffle configuration:\n",
      "spark.sql.shuffle.partitions: 10\n",
      "Default parallelism: 12\n",
      "spark.sql.adaptive.enabled: false\n",
      "\n",
      "Listing all available SQL configurations:\n",
      "Total configurations: 206\n",
      "spark.sql.adaptive.advisoryPartitionSizeInBytes: <value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>\n",
      "spark.sql.adaptive.autoBroadcastJoinThreshold: <undefined>\n",
      "spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "spark.sql.adaptive.coalescePartitions.initialPartitionNum: <undefined>\n",
      "spark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the available shuffle configuration\n",
    "print(\"Current shuffle configuration:\")\n",
    "\n",
    "# Use default values for all configs to avoid errors\n",
    "print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions', 'not set')}\")\n",
    "\n",
    "# Get parallelism through SparkContext\n",
    "parallelism = spark.sparkContext.defaultParallelism\n",
    "print(f\"Default parallelism: {parallelism}\")\n",
    "\n",
    "# Check adaptive execution settings\n",
    "print(f\"spark.sql.adaptive.enabled: {spark.conf.get('spark.sql.adaptive.enabled', 'not set')}\")\n",
    "\n",
    "# Check all available configurations\n",
    "print(\"\\nListing all available SQL configurations:\")\n",
    "all_configs = spark.sql(\"SET -v\").collect()\n",
    "print(f\"Total configurations: {len(all_configs)}\")\n",
    "\n",
    "# Print a few examples\n",
    "for i, row in enumerate(all_configs[:5]):  # Just show first 5\n",
    "    print(f\"{row[0]}: {row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition Shuffle Plan:\n",
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(10), REPARTITION_BY_NUM, [plan_id=3445]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "GroupBy Shuffle Plan:\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[payment_method#6373], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(payment_method#6373, 10), ENSURE_REQUIREMENTS, [plan_id=3475]\n",
      "   +- *(1) HashAggregate(keys=[payment_method#6373], functions=[partial_count(1)])\n",
      "      +- *(1) Project [payment_method#6373]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.transactions_table[payment_method#6373,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<payment_method:string>\n",
      "\n",
      "\n",
      "\n",
      "Join Shuffle Plan:\n",
      "== Physical Plan ==\n",
      "*(5) Project [customer_id#6370, transaction_id#6369, transaction_date#6371, items#6372, payment_method#6373, status#6374, total_amount#6375, store_id#6376, metadata#6377, year#6378, month#6379, name#6698, email#6699, address#6700, phone#6701, signup_date#6702, last_activity#6703, tier#6704, preferences#6705]\n",
      "+- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "   :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#6370, 10), ENSURE_REQUIREMENTS, [plan_id=3535]\n",
      "   :     +- *(1) Filter isnotnull(customer_id#6370)\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "   +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#6697, 10), ENSURE_REQUIREMENTS, [plan_id=3544]\n",
      "         +- *(3) Filter isnotnull(customer_id#6697)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "\n",
      "Window Function Shuffle Plan:\n",
      "== Physical Plan ==\n",
      "Window [rank(total_amount#6375) windowspecdefinition(payment_method#6373, total_amount#6375 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#7481], [payment_method#6373], [total_amount#6375 ASC NULLS FIRST]\n",
      "+- *(2) Sort [payment_method#6373 ASC NULLS FIRST, total_amount#6375 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(payment_method#6373, 10), ENSURE_REQUIREMENTS, [plan_id=3585]\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create queries that trigger different types of shuffles\n",
    "\n",
    "# 1. Shuffle due to repartition\n",
    "repartition_shuffle = spark.table(\"transactions_table\").repartition(10)\n",
    "print(\"Repartition Shuffle Plan:\")\n",
    "repartition_shuffle.explain()\n",
    "\n",
    "# 2. Shuffle due to groupBy\n",
    "group_shuffle = spark.table(\"transactions_table\").groupBy(\"payment_method\").count()\n",
    "print(\"\\nGroupBy Shuffle Plan:\")\n",
    "group_shuffle.explain()\n",
    "\n",
    "# 3. Shuffle due to join (without broadcast)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "join_shuffle = spark.table(\"transactions_table\").join(\n",
    "    spark.table(\"customers_table\"),\n",
    "    \"customer_id\"\n",
    ")\n",
    "print(\"\\nJoin Shuffle Plan:\")\n",
    "join_shuffle.explain()\n",
    "\n",
    "# 4. Shuffle due to window function\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"payment_method\").orderBy(\"total_amount\")\n",
    "window_shuffle = spark.table(\"transactions_table\").withColumn(\n",
    "    \"rank\", rank().over(window_spec)\n",
    ")\n",
    "print(\"\\nWindow Function Shuffle Plan:\")\n",
    "window_shuffle.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Shuffle Partition Count\n",
    "\n",
    "The number of shuffle partitions significantly impacts performance. Too few can lead to memory pressure, while too many create small tasks with overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 5 shuffle partitions:\n",
      "== Physical Plan ==\n",
      "*(6) Sort [total_spent#7501 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_spent#7501 DESC NULLS LAST, 5), ENSURE_REQUIREMENTS, [plan_id=3686]\n",
      "   +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[count(1), sum(total_amount#6375), avg(total_amount#6375)])\n",
      "      +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[partial_count(1), partial_sum(total_amount#6375), partial_avg(total_amount#6375)])\n",
      "         +- *(5) Project [customer_id#6370, total_amount#6375, name#6698]\n",
      "            +- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "               :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(customer_id#6370, 5), ENSURE_REQUIREMENTS, [plan_id=3667]\n",
      "               :     +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "               :        +- *(1) Filter isnotnull(customer_id#6370)\n",
      "               :           +- *(1) ColumnarToRow\n",
      "               :              +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "               +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(customer_id#6697, 5), ENSURE_REQUIREMENTS, [plan_id=3676]\n",
      "                     +- *(3) Filter isnotnull(customer_id#6697)\n",
      "                        +- *(3) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n",
      "Shuffle with 5 partitions executed in 0.6657 seconds\n",
      "\n",
      "Testing with 20 shuffle partitions:\n",
      "== Physical Plan ==\n",
      "*(6) Sort [total_spent#7539 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_spent#7539 DESC NULLS LAST, 20), ENSURE_REQUIREMENTS, [plan_id=3960]\n",
      "   +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[count(1), sum(total_amount#6375), avg(total_amount#6375)])\n",
      "      +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[partial_count(1), partial_sum(total_amount#6375), partial_avg(total_amount#6375)])\n",
      "         +- *(5) Project [customer_id#6370, total_amount#6375, name#6698]\n",
      "            +- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "               :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(customer_id#6370, 20), ENSURE_REQUIREMENTS, [plan_id=3941]\n",
      "               :     +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "               :        +- *(1) Filter isnotnull(customer_id#6370)\n",
      "               :           +- *(1) ColumnarToRow\n",
      "               :              +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "               +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(customer_id#6697, 20), ENSURE_REQUIREMENTS, [plan_id=3950]\n",
      "                     +- *(3) Filter isnotnull(customer_id#6697)\n",
      "                        +- *(3) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n",
      "Shuffle with 20 partitions executed in 0.5944 seconds\n",
      "\n",
      "Testing with 100 shuffle partitions:\n",
      "== Physical Plan ==\n",
      "*(6) Sort [total_spent#7577 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_spent#7577 DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=4234]\n",
      "   +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[count(1), sum(total_amount#6375), avg(total_amount#6375)])\n",
      "      +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[partial_count(1), partial_sum(total_amount#6375), partial_avg(total_amount#6375)])\n",
      "         +- *(5) Project [customer_id#6370, total_amount#6375, name#6698]\n",
      "            +- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "               :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(customer_id#6370, 100), ENSURE_REQUIREMENTS, [plan_id=4215]\n",
      "               :     +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "               :        +- *(1) Filter isnotnull(customer_id#6370)\n",
      "               :           +- *(1) ColumnarToRow\n",
      "               :              +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "               +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(customer_id#6697, 100), ENSURE_REQUIREMENTS, [plan_id=4224]\n",
      "                     +- *(3) Filter isnotnull(customer_id#6697)\n",
      "                        +- *(3) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle with 100 partitions executed in 0.8455 seconds\n",
      "\n",
      "Testing with 200 shuffle partitions:\n",
      "== Physical Plan ==\n",
      "*(6) Sort [total_spent#7615 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_spent#7615 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=4508]\n",
      "   +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[count(1), sum(total_amount#6375), avg(total_amount#6375)])\n",
      "      +- *(5) HashAggregate(keys=[customer_id#6370, name#6698], functions=[partial_count(1), partial_sum(total_amount#6375), partial_avg(total_amount#6375)])\n",
      "         +- *(5) Project [customer_id#6370, total_amount#6375, name#6698]\n",
      "            +- *(5) SortMergeJoin [customer_id#6370], [customer_id#6697], Inner\n",
      "               :- *(2) Sort [customer_id#6370 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(customer_id#6370, 200), ENSURE_REQUIREMENTS, [plan_id=4489]\n",
      "               :     +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "               :        +- *(1) Filter isnotnull(customer_id#6370)\n",
      "               :           +- *(1) ColumnarToRow\n",
      "               :              +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "               +- *(4) Sort [customer_id#6697 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(customer_id#6697, 200), ENSURE_REQUIREMENTS, [plan_id=4498]\n",
      "                     +- *(3) Filter isnotnull(customer_id#6697)\n",
      "                        +- *(3) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:===========================>                          (12 + 12) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle with 200 partitions executed in 1.0131 seconds\n",
      "\n",
      "Shuffle Partition Performance Summary:\n",
      "Partitions: 5, Time: 0.6657 seconds\n",
      "Partitions: 20, Time: 0.5944 seconds\n",
      "Partitions: 100, Time: 0.8455 seconds\n",
      "Partitions: 200, Time: 1.0131 seconds\n",
      "\n",
      "Optimal partition count: 20 with 0.5944 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# First fix the time_execution_with_metrics function\n",
    "import time\n",
    "\n",
    "def time_execution_with_metrics(df, action=\"count\", name=\"query\"):\n",
    "    \"\"\"Time and collect metrics for a DataFrame action\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    if action == \"count\":\n",
    "        result = df.count()\n",
    "    elif action == \"collect\":\n",
    "        result = df.collect()\n",
    "    else:\n",
    "        result = df.count()  # Default to count\n",
    "        \n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    \n",
    "    print(f\"{name} executed in {duration:.4f} seconds\")\n",
    "    return duration, result\n",
    "\n",
    "# Now test different shuffle partition counts\n",
    "from builtins import min as py_min  # Import Python's built-in min\n",
    "\n",
    "partition_counts = [5, 20, 100, 200]\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "    t.customer_id,\n",
    "    c.name,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(t.total_amount) as total_spent,\n",
    "    AVG(t.total_amount) as avg_transaction\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "GROUP BY \n",
    "    t.customer_id, c.name\n",
    "ORDER BY \n",
    "    total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "results = []\n",
    "for partitions in partition_counts:\n",
    "    # Set the shuffle partitions\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", partitions)\n",
    "    \n",
    "    # Run the test query\n",
    "    test_df = spark.sql(test_query)\n",
    "    \n",
    "    # Measure performance\n",
    "    print(f\"\\nTesting with {partitions} shuffle partitions:\")\n",
    "    test_df.explain()\n",
    "    execution_time, _ = time_execution_with_metrics(test_df, name=f\"Shuffle with {partitions} partitions\")\n",
    "    \n",
    "    results.append((partitions, execution_time))\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nShuffle Partition Performance Summary:\")\n",
    "for partitions, time_value in results:\n",
    "    print(f\"Partitions: {partitions}, Time: {time_value:.4f} seconds\")\n",
    "\n",
    "# Find optimal partition count using Python's built-in min function\n",
    "optimal_result = py_min(results, key=lambda x: x[1])\n",
    "optimal_partitions, min_time = optimal_result\n",
    "\n",
    "print(f\"\\nOptimal partition count: {optimal_partitions} with {min_time:.4f} seconds\")\n",
    "\n",
    "# Reset to a reasonable value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding Unnecessary Shuffles\n",
    "\n",
    "One of the best optimizations is to avoid unnecessary shuffles entirely. Let's explore techniques for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with shuffle:\n",
      "== Physical Plan ==\n",
      "*(2) Project [customer_id#6370, transaction_id#6369, transaction_date#6371, items#6372, payment_method#6373, status#6374, total_amount#6375, store_id#6376, metadata#6377, year#6378, month#6379, name#6698, email#6699, address#6700, phone#6701, signup_date#6702, last_activity#6703, tier#6704, preferences#6705]\n",
      "+- *(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, false] as bigint)),false), [plan_id=4731]\n",
      "   :  +- *(1) Filter isnotnull(customer_id#6370)\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "   +- *(2) Filter isnotnull(customer_id#6697)\n",
      "      +- *(2) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "\n",
      "Join with broadcast (no shuffle):\n",
      "== Physical Plan ==\n",
      "*(2) Project [customer_id#6370, transaction_id#6369, transaction_date#6371, items#6372, payment_method#6373, status#6374, total_amount#6375, store_id#6376, metadata#6377, year#6378, month#6379, name#6698, email#6699, address#6700, phone#6701, signup_date#6702, last_activity#6703, tier#6704, preferences#6705]\n",
      "+- *(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildRight, false\n",
      "   :- *(2) Filter isnotnull(customer_id#6370)\n",
      "   :  +- *(2) ColumnarToRow\n",
      "   :     +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: InMemoryFileIndex(12 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table/year=2022..., PartitionFilters: [isnotnull(year#6378), (year#6378 = 2022)], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4788]\n",
      "      +- *(1) Filter isnotnull(customer_id#6697)\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,name#6698,email#6699,address#6700,phone#6701,signup_date#6702,last_activity#6703,tier#6704,preferences#6705] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,email:string,address:struct<street:string,city:string,state:st...\n",
      "\n",
      "\n",
      "Join with shuffle executed in 0.1828 seconds\n",
      "Join with broadcast executed in 0.1400 seconds\n",
      "\n",
      "Performance improvement: 1.31x faster with broadcast\n"
     ]
    }
   ],
   "source": [
    "# Technique 1: Use broadcast joins to avoid shuffles\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10m\")\n",
    "\n",
    "# Join with shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "shuffle_join = spark.table(\"transactions_table\").filter(col(\"year\") == 2022).join(\n",
    "    spark.table(\"customers_table\"),\n",
    "    \"customer_id\"\n",
    ")\n",
    "\n",
    "# Join with broadcast to avoid shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50m\")\n",
    "broadcast_join = spark.table(\"transactions_table\").filter(col(\"year\") == 2022).join(\n",
    "    broadcast(spark.table(\"customers_table\")),\n",
    "    \"customer_id\"\n",
    ")\n",
    "\n",
    "print(\"Join with shuffle:\")\n",
    "shuffle_join.explain()\n",
    "\n",
    "print(\"\\nJoin with broadcast (no shuffle):\")\n",
    "broadcast_join.explain()\n",
    "\n",
    "# Measure performance\n",
    "shuffle_time, _ = time_execution_with_metrics(shuffle_join, name=\"Join with shuffle\")\n",
    "broadcast_time, _ = time_execution_with_metrics(broadcast_join, name=\"Join with broadcast\")\n",
    "\n",
    "print(f\"\\nPerformance improvement: {shuffle_time/broadcast_time:.2f}x faster with broadcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions: 20\n",
      "spark.sql.adaptive.enabled: false\n",
      "Standard Aggregation Plan:\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[customer_id#6370], functions=[sum(total_amount#6375)])\n",
      "+- Exchange hashpartitioning(customer_id#6370, 20), ENSURE_REQUIREMENTS, [plan_id=5008]\n",
      "   +- *(1) HashAggregate(keys=[customer_id#6370], functions=[partial_sum(total_amount#6375)])\n",
      "      +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "\n",
      "\n",
      "\n",
      "Two-Phase Aggregation Plan:\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[customer_id#6370], functions=[sum(total_amount#6375)])\n",
      "+- *(2) HashAggregate(keys=[customer_id#6370], functions=[partial_sum(total_amount#6375)])\n",
      "   +- Exchange hashpartitioning(customer_id#6370, 20), REPARTITION_BY_NUM, [plan_id=5047]\n",
      "      +- *(1) Project [customer_id#6370, total_amount#6375]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:int,total_amount:double>\n",
      "\n",
      "\n",
      "Standard aggregation executed in 0.5225 seconds\n",
      "Two-phase aggregation executed in 0.5118 seconds\n",
      "\n",
      "Performance comparison: Two-phase is 1.02x vs standard\n"
     ]
    }
   ],
   "source": [
    "# Technique 2: Use map-side aggregation to reduce shuffle data volume\n",
    "\n",
    "# First check the configuration\n",
    "print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"spark.sql.adaptive.enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Standard aggregation (full shuffle)\n",
    "standard_agg = spark.table(\"transactions_table\").groupBy(\"customer_id\").sum(\"total_amount\")\n",
    "\n",
    "# Two-phase aggregation (map-side + reduce)\n",
    "# We manually repartition on the groupBy key first to colocate data\n",
    "two_phase_agg = spark.table(\"transactions_table\") \\\n",
    "    .repartition(20, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .sum(\"total_amount\")\n",
    "\n",
    "print(\"Standard Aggregation Plan:\")\n",
    "standard_agg.explain()\n",
    "\n",
    "print(\"\\nTwo-Phase Aggregation Plan:\")\n",
    "two_phase_agg.explain()\n",
    "\n",
    "# Measure performance\n",
    "standard_time, _ = time_execution_with_metrics(standard_agg, name=\"Standard aggregation\")\n",
    "two_phase_time, _ = time_execution_with_metrics(two_phase_agg, name=\"Two-phase aggregation\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: Two-phase is {standard_time/two_phase_time:.2f}x vs standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition Plan (full shuffle):\n",
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(10), REPARTITION_BY_NUM, [plan_id=5200]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "\n",
      "Coalesce Plan (partial shuffle):\n",
      "== Physical Plan ==\n",
      "Coalesce 10\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.transactions_table[transaction_id#6369,customer_id#6370,transaction_date#6371,items#6372,payment_method#6373,status#6374,total_amount#6375,store_id#6376,metadata#6377,year#6378,month#6379] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,customer_id:int,transaction_date:timestamp,items:array<struct<product_i...\n",
      "\n",
      "\n",
      "Repartition executed in 0.4703 seconds\n",
      "Coalesce executed in 0.5180 seconds\n",
      "\n",
      "Performance comparison: Coalesce is 0.91x faster than repartition\n"
     ]
    }
   ],
   "source": [
    "# Technique 3: Coalesce instead of repartition when reducing partitions\n",
    "\n",
    "# Repartition creates a full shuffle\n",
    "repartition_df = spark.table(\"transactions_table\").repartition(10)\n",
    "\n",
    "# Coalesce avoids a full shuffle when reducing partitions\n",
    "coalesce_df = spark.table(\"transactions_table\").coalesce(10)\n",
    "\n",
    "print(\"Repartition Plan (full shuffle):\")\n",
    "repartition_df.explain()\n",
    "\n",
    "print(\"\\nCoalesce Plan (partial shuffle):\")\n",
    "coalesce_df.explain()\n",
    "\n",
    "# Measure performance\n",
    "repartition_time, _ = time_execution_with_metrics(repartition_df, name=\"Repartition\")\n",
    "coalesce_time, _ = time_execution_with_metrics(coalesce_df, name=\"Coalesce\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: Coalesce is {repartition_time/coalesce_time:.2f}x faster than repartition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Query Execution for Shuffle Optimization\n",
    "\n",
    "Adaptive Query Execution (AQE) can dynamically optimize shuffles at runtime. Let's explore its impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan without Adaptive Execution:\n",
      "== Physical Plan ==\n",
      "*(4) Sort [total_sales#7904 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_sales#7904 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=5393]\n",
      "   +- *(3) HashAggregate(keys=[tier#6704, payment_method#6373], functions=[count(1), sum(total_amount#6375)])\n",
      "      +- Exchange hashpartitioning(tier#6704, payment_method#6373, 200), ENSURE_REQUIREMENTS, [plan_id=5389]\n",
      "         +- *(2) HashAggregate(keys=[tier#6704, payment_method#6373], functions=[partial_count(1), partial_sum(total_amount#6375)])\n",
      "            +- *(2) Project [payment_method#6373, total_amount#6375, tier#6704]\n",
      "               +- *(2) BroadcastHashJoin [customer_id#6370], [customer_id#6697], Inner, BuildRight, false\n",
      "                  :- *(2) Project [customer_id#6370, payment_method#6373, total_amount#6375]\n",
      "                  :  +- *(2) Filter isnotnull(customer_id#6370)\n",
      "                  :     +- *(2) ColumnarToRow\n",
      "                  :        +- FileScan parquet spark_catalog.default.transactions_table[customer_id#6370,payment_method#6373,total_amount#6375,year#6378,month#6379] Batched: true, DataFilters: [isnotnull(customer_id#6370)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/transactions_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,payment_method:string,total_amount:double>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=5383]\n",
      "                     +- *(1) Filter isnotnull(customer_id#6697)\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.customers_table[customer_id#6697,tier#6704] Batched: true, DataFilters: [isnotnull(customer_id#6697)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/spark-warehouse/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,tier:string>\n",
      "\n",
      "\n",
      "\n",
      "Plan with Adaptive Execution:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (14)\n",
      "+- Sort (13)\n",
      "   +- Exchange (12)\n",
      "      +- HashAggregate (11)\n",
      "         +- Exchange (10)\n",
      "            +- HashAggregate (9)\n",
      "               +- Project (8)\n",
      "                  +- BroadcastHashJoin Inner BuildRight (7)\n",
      "                     :- Project (3)\n",
      "                     :  +- Filter (2)\n",
      "                     :     +- Scan parquet spark_catalog.default.transactions_table (1)\n",
      "                     +- BroadcastExchange (6)\n",
      "                        +- Filter (5)\n",
      "                           +- Scan parquet spark_catalog.default.customers_table (4)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.transactions_table\n",
      "Output [5]: [customer_id#6370, payment_method#6373, total_amount#6375, year#6378, month#6379]\n",
      "Batched: true\n",
      "Location: CatalogFileIndex [file:/opt/spark/work-dir/spark-warehouse/transactions_table]\n",
      "PushedFilters: [IsNotNull(customer_id)]\n",
      "ReadSchema: struct<customer_id:int,payment_method:string,total_amount:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [5]: [customer_id#6370, payment_method#6373, total_amount#6375, year#6378, month#6379]\n",
      "Condition : isnotnull(customer_id#6370)\n",
      "\n",
      "(3) Project\n",
      "Output [3]: [customer_id#6370, payment_method#6373, total_amount#6375]\n",
      "Input [5]: [customer_id#6370, payment_method#6373, total_amount#6375, year#6378, month#6379]\n",
      "\n",
      "(4) Scan parquet spark_catalog.default.customers_table\n",
      "Output [2]: [customer_id#6697, tier#6704]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/opt/spark/work-dir/spark-warehouse/customers_table]\n",
      "PushedFilters: [IsNotNull(customer_id)]\n",
      "ReadSchema: struct<customer_id:int,tier:string>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [customer_id#6697, tier#6704]\n",
      "Condition : isnotnull(customer_id#6697)\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [2]: [customer_id#6697, tier#6704]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=5467]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [customer_id#6370]\n",
      "Right keys [1]: [customer_id#6697]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [3]: [payment_method#6373, total_amount#6375, tier#6704]\n",
      "Input [5]: [customer_id#6370, payment_method#6373, total_amount#6375, customer_id#6697, tier#6704]\n",
      "\n",
      "(9) HashAggregate\n",
      "Input [3]: [payment_method#6373, total_amount#6375, tier#6704]\n",
      "Keys [2]: [tier#6704, payment_method#6373]\n",
      "Functions [2]: [partial_count(1), partial_sum(total_amount#6375)]\n",
      "Aggregate Attributes [2]: [count#7926L, sum#7927]\n",
      "Results [4]: [tier#6704, payment_method#6373, count#7928L, sum#7929]\n",
      "\n",
      "(10) Exchange\n",
      "Input [4]: [tier#6704, payment_method#6373, count#7928L, sum#7929]\n",
      "Arguments: hashpartitioning(tier#6704, payment_method#6373, 200), ENSURE_REQUIREMENTS, [plan_id=5472]\n",
      "\n",
      "(11) HashAggregate\n",
      "Input [4]: [tier#6704, payment_method#6373, count#7928L, sum#7929]\n",
      "Keys [2]: [tier#6704, payment_method#6373]\n",
      "Functions [2]: [count(1), sum(total_amount#6375)]\n",
      "Aggregate Attributes [2]: [count(1)#7920L, sum(total_amount#6375)#7921]\n",
      "Results [4]: [tier#6704, payment_method#6373, count(1)#7920L AS transaction_count#7918L, sum(total_amount#6375)#7921 AS total_sales#7919]\n",
      "\n",
      "(12) Exchange\n",
      "Input [4]: [tier#6704, payment_method#6373, transaction_count#7918L, total_sales#7919]\n",
      "Arguments: rangepartitioning(total_sales#7919 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=5475]\n",
      "\n",
      "(13) Sort\n",
      "Input [4]: [tier#6704, payment_method#6373, transaction_count#7918L, total_sales#7919]\n",
      "Arguments: [total_sales#7919 DESC NULLS LAST], true, 0\n",
      "\n",
      "(14) AdaptiveSparkPlan\n",
      "Output [4]: [tier#6704, payment_method#6373, transaction_count#7918L, total_sales#7919]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Non-adaptive execution executed in 0.6689 seconds\n",
      "Adaptive execution executed in 0.5547 seconds\n",
      "\n",
      "Performance comparison: AQE is 1.21x faster\n"
     ]
    }
   ],
   "source": [
    "# Let's test with and without AQE\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "    c.tier,\n",
    "    t.payment_method,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(t.total_amount) as total_sales\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "GROUP BY \n",
    "    c.tier, t.payment_method\n",
    "ORDER BY \n",
    "    total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "# Set a high shuffle partition count to see AQE's impact\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "# Test without AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "non_adaptive_df = spark.sql(test_query)\n",
    "\n",
    "print(\"Plan without Adaptive Execution:\")\n",
    "non_adaptive_df.explain()\n",
    "\n",
    "# Test with AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "adaptive_df = spark.sql(test_query)\n",
    "\n",
    "print(\"\\nPlan with Adaptive Execution:\")\n",
    "adaptive_df.explain(\"formatted\")\n",
    "\n",
    "# Measure performance\n",
    "non_adaptive_time, _ = time_execution_with_metrics(non_adaptive_df, name=\"Non-adaptive execution\")\n",
    "adaptive_time, _ = time_execution_with_metrics(adaptive_df, name=\"Adaptive execution\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: AQE is {non_adaptive_time/adaptive_time:.2f}x faster\")\n",
    "\n",
    "# Reset configurations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Shuffle Optimization Techniques\n",
    "\n",
    "1. **Tune Shuffle Partitions**: \n",
    "   - For small-medium datasets: Use 2-3 the number of CPU cores\n",
    "   - For large datasets: Consider 1-2 the number of cores per GB of shuffle data\n",
    "   - Monitor task duration - target 50-200ms per task\n",
    "\n",
    "2. **Avoid Unnecessary Shuffles**: \n",
    "   - Use broadcast joins when possible\n",
    "   - Use coalesce instead of repartition when reducing partitions\n",
    "   - Reuse existing partitioning when possible\n",
    "\n",
    "3. **Optimize Data Size**:\n",
    "   - Pre-aggregate or filter data before shuffling\n",
    "   - Select only required columns before shuffling\n",
    "   - Use efficient serialization formats\n",
    "\n",
    "4. **Balance Data Distribution**:\n",
    "   - Address data skew (as covered in join optimization)\n",
    "   - Consider custom partitioners for better balance\n",
    "\n",
    "5. **Use Adaptive Execution**:\n",
    "   - Enable adaptive query execution for dynamic optimization\n",
    "   - Let Spark automatically adjust partition counts\n",
    "   - Enable skew join optimization\n",
    "\n",
    "6. **Monitor Shuffle Service**:\n",
    "   - Check for external shuffle service health\n",
    "   - Ensure sufficient disk space for shuffle files\n",
    "   - Consider configuring `spark.local.dir` for faster disks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
