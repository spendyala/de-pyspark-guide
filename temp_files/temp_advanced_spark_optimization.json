{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spark Query Plan Analysis and Optimization\n",
    "\n",
    "This notebook provides an in-depth exploration of Spark's query plans and advanced optimization techniques, building on top of basic optimizations. We'll analyze logical and physical plans in detail to identify performance bottlenecks and apply targeted optimizations.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Advanced Query Plan Analysis\n",
    "2. Setting Up the Environment\n",
    "3. Deep Dive into Logical and Physical Plans\n",
    "   - Analyzing Scan Operations\n",
    "   - Understanding Filter and Projection Pushdown\n",
    "   - Detailed Join Strategy Analysis\n",
    "   - Optimizing Shuffle Operations\n",
    "   - Partition Tuning for Performance\n",
    "   - Broadcast Operations and Memory Management\n",
    "   - Aggregation Optimization Techniques\n",
    "   - Spill to Disk Detection and Prevention\n",
    "   - Skew Detection and Handling\n",
    "   - Whole-Stage Codegen Analysis\n",
    "4. Adaptive Query Execution Deep Dive\n",
    "5. Cost-Based Optimization in Spark\n",
    "6. Advanced Performance Tuning Configurations\n",
    "7. Plan Metrics Analysis and Optimization\n",
    "8. Real-world Examples and Solutions\n",
    "\n",
    "Let's start by understanding what makes query plan analysis essential for advanced Spark optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Advanced Query Plan Analysis\n",
    "\n",
    "While basic optimization strategies can significantly improve Spark performance, advanced optimization requires deep understanding of how Spark transforms queries into executable code. By analyzing both logical and physical plans, we can identify inefficiencies that aren't obvious from the DataFrame API or SQL interface.\n",
    "\n",
    "Key components of advanced plan analysis include:\n",
    "\n",
    "- **Logical Plan Transformation Rules**: Understanding how Catalyst applies rules to optimize logical plans\n",
    "- **Physical Plan Selection Criteria**: How Spark decides which physical strategies to use\n",
    "- **Cost Model Analysis**: How statistics influence plan decisions\n",
    "- **Runtime Adaptations**: How plans are modified during execution\n",
    "- **Performance Bottleneck Identification**: Finding specific operations that limit performance\n",
    "- **Resource Usage Patterns**: Understanding memory, CPU, and I/O patterns\n",
    "\n",
    "The `explain()` method with extended modes and additional utilities in the `ExplainCommand` provide the tools needed for this deep analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "Let's set up our environment and create test datasets for our advanced analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, sum, max, min, lit, concat, broadcast, expr, \n",
    "    when, coalesce, array, explode, struct, to_json, from_json, \n",
    "    window, row_number, rank, dense_rank, ntile, lead, lag,\n",
    "    udf, pandas_udf, PandasUDFType\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, \n",
    "    DateType, TimestampType, ArrayType, MapType, BooleanType\n",
    ")\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session with detailed configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Spark Optimization\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable us to view full query plans\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")  # Initially disabled for clearer analysis\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")  # Enable whole-stage codegen\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Session initialized with detailed configuration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to time execution and get metrics\n",
    "def time_execution_with_metrics(df, action=\"count\", name=\"query\"):\n",
    "    \"\"\"Time and collect metrics for a DataFrame action\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    if action == \"count\":\n",
    "        result = df.count()\n",
    "    elif action == \"collect\":\n",
    "        result = df.collect()\n",
    "    elif action == \"show\":\n",
    "        result = df.show(n=10, truncate=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported action: {action}\")\n",
    "        \n",
    "    execution_time = time.time() - start\n",
    "    \n",
    "    print(f\"{name} execution time: {execution_time:.4f} seconds\")\n",
    "    return execution_time, result\n",
    "\n",
    "# Create a function to show both logical and physical plans with different detail levels\n",
    "def analyze_plans(df, name=\"Query\"):\n",
    "    \"\"\"Show logical and physical plans at different detail levels\"\"\"\n",
    "    print(f\"\\n{'='*20} {name} Plan Analysis {'='*20}\\n\")\n",
    "    \n",
    "    print(\"--- Logical Plan (Parsed) ---\")\n",
    "    df._jdf.queryExecution().logical().toJSON()\n",
    "    df._jdf.queryExecution().logical().toString()\n",
    "    \n",
    "    print(\"\\n--- Logical Plan (Analyzed) ---\")\n",
    "    df._jdf.queryExecution().analyzed().toString()\n",
    "    \n",
    "    print(\"\\n--- Logical Plan (Optimized) ---\")\n",
    "    df._jdf.queryExecution().optimizedPlan().toString()\n",
    "    \n",
    "    print(\"\\n--- Physical Plan ---\")\n",
    "    df.explain()\n",
    "    \n",
    "    print(\"\\n--- Detailed Physical Plan ---\")\n",
    "    df.explain(mode=\"extended\")\n",
    "    \n",
    "    print(\"\\n--- Cost Analysis ---\")\n",
    "    df.explain(mode=\"cost\")\n",
    "    \n",
    "    print(\"\\n--- Codegen Details ---\")\n",
    "    df.explain(mode=\"codegen\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a complex dataset with various data characteristics that will allow us to explore advanced optimization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger, more complex dataset with multiple tables\n",
    "# 1. Fact table (transactions) with millions of rows\n",
    "# 2. Multiple dimension tables with different sizes\n",
    "# 3. Complex data types (arrays, maps, structs)\n",
    "# 4. Skewed data distributions\n",
    "# 5. Partitioned and bucketed tables\n",
    "\n",
    "# First, let's create dimension tables\n",
    "\n",
    "# Products dimension\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"category_id\", IntegerType(), False),\n",
    "    StructField(\"subcategory_id\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), False),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"created_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Generate 5,000 products\n",
    "product_data = []\n",
    "product_names = [\"Laptop\", \"Phone\", \"Tablet\", \"TV\", \"Camera\", \"Headphones\", \"Speaker\", \"Watch\", \"Keyboard\", \"Mouse\"]\n",
    "product_prefixes = [\"Pro\", \"Elite\", \"Ultra\", \"Mini\", \"Max\", \"Lite\", \"Premium\", \"Basic\", \"Standard\", \"Advanced\"]\n",
    "product_suffixes = [\"2023\", \"Plus\", \"XL\", \"SE\", \"X\", \"S\", \"Air\", \"Book\", \"Pad\", \"Vision\"]\n",
    "\n",
    "categories = 10\n",
    "subcategories = 5  # per category\n",
    "\n",
    "# Tag options\n",
    "all_tags = [\"new\", \"bestseller\", \"sale\", \"clearance\", \"limited\", \"exclusive\", \"featured\", \"discontinued\", \"popular\", \"trending\"]\n",
    "\n",
    "# Attribute options\n",
    "attribute_keys = [\"color\", \"size\", \"weight\", \"material\", \"connectivity\", \"power\", \"warranty\", \"origin\"]\n",
    "attribute_values = {\n",
    "    \"color\": [\"black\", \"white\", \"silver\", \"gold\", \"blue\", \"red\", \"green\"],\n",
    "    \"size\": [\"small\", \"medium\", \"large\", \"XL\", \"compact\"],\n",
    "    \"weight\": [\"light\", \"medium\", \"heavy\", \"ultra-light\"],\n",
    "    \"material\": [\"plastic\", \"metal\", \"glass\", \"aluminum\", \"carbon fiber\"],\n",
    "    \"connectivity\": [\"bluetooth\", \"wifi\", \"wired\", \"usb-c\", \"lightning\"],\n",
    "    \"power\": [\"battery\", \"plug-in\", \"solar\", \"hybrid\"],\n",
    "    \"warranty\": [\"1-year\", \"2-year\", \"lifetime\", \"extended\"],\n",
    "    \"origin\": [\"USA\", \"China\", \"Japan\", \"Korea\", \"Germany\"]\n",
    "}\n",
    "\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime.now()\n",
    "date_range = (end_date - start_date).days\n",
    "\n",
    "for i in range(1, 5001):\n",
    "    # Create skewed distribution for categories (more products in lower categories)\n",
    "    category_id = min(int(random.paretovariate(1.5)), categories)\n",
    "    subcategory_id = random.randint(1, subcategories) if random.random() > 0.1 else None\n",
    "    \n",
    "    # Generate product name\n",
    "    name = f\"{random.choice(product_prefixes)} {random.choice(product_names)} {random.choice(product_suffixes)}\"\n",
    "    \n",
    "    # Generate price with some skew\n",
    "    if category_id <= 3:  # Higher priced categories\n",
    "        price = round(random.uniform(500, 2000), 2)\n",
    "    else:\n",
    "        price = round(random.uniform(10, 500), 2)\n",
    "    \n",
    "    # Generate random tags (0-5 tags)\n",
    "    num_tags = random.randint(0, 5)\n",
    "    tags = random.sample(all_tags, num_tags) if num_tags > 0 else None\n",
    "    \n",
    "    # Generate random attributes (0-5 attributes)\n",
    "    num_attrs = random.randint(0, 5)\n",
    "    if num_attrs > 0:\n",
    "        selected_keys = random.sample(attribute_keys, num_attrs)\n",
    "        attributes = {k: random.choice(attribute_values[k]) for k in selected_keys}\n",
    "    else:\n",
    "        attributes = None\n",
    "    \n",
    "    # Generate creation date\n",
    "    random_days = random.randint(0, date_range)\n",
    "    created_at = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    product_data.append((i, name, category_id, subcategory_id, price, attributes, tags, created_at))\n",
    "\n",
    "products_df = spark.createDataFrame(product_data, product_schema)\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Save as a permanent table for later use\n",
    "products_df.write.mode(\"overwrite\").saveAsTable(\"products_table\")\n",
    "\n",
    "print(f\"Created products dimension table with {products_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customers dimension table\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), False),\n",
    "        StructField(\"state\", StringType(), False),\n",
    "        StructField(\"zip\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), False)\n",
    "    ]), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), False),\n",
    "    StructField(\"last_activity\", TimestampType(), True),\n",
    "    StructField(\"tier\", StringType(), False),\n",
    "    StructField(\"preferences\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Generate 50,000 customers\n",
    "customer_data = []\n",
    "first_names = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \"William\", \"Elizabeth\"]\n",
    "last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\", \"Garcia\", \"Rodriguez\", \"Wilson\"]\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\"]\n",
    "states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\", \"PA\", \"TX\", \"CA\", \"TX\", \"CA\"]\n",
    "countries = [\"USA\" for _ in range(10)]\n",
    "tiers = [\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]\n",
    "tier_weights = [0.5, 0.3, 0.15, 0.05]  # Distribution weights\n",
    "\n",
    "preference_keys = [\"communication\", \"payment\", \"shipping\", \"recommendations\", \"notifications\"]\n",
    "preference_values = {\n",
    "    \"communication\": [\"email\", \"sms\", \"phone\", \"mail\"],\n",
    "    \"payment\": [\"credit\", \"debit\", \"paypal\", \"crypto\", \"bank_transfer\"],\n",
    "    \"shipping\": [\"standard\", \"express\", \"overnight\", \"pickup\"],\n",
    "    \"recommendations\": [\"enabled\", \"disabled\"],\n",
    "    \"notifications\": [\"high\", \"medium\", \"low\", \"none\"]\n",
    "}\n",
    "\n",
    "for i in range(1, 50001):\n",
    "    # Name and email\n",
    "    first = random.choice(first_names)\n",
    "    last = random.choice(last_names)\n",
    "    name = f\"{first} {last}\"\n",
    "    email = f\"{first.lower()}.{last.lower()}@example.com\" if random.random() > 0.1 else None\n",
    "    \n",
    "    # Address\n",
    "    if random.random() > 0.05:  # 5% have no address\n",
    "        street = f\"{random.randint(100, 9999)} Main St\" if random.random() > 0.1 else None\n",
    "        city_idx = random.randint(0, 9)\n",
    "        city = cities[city_idx]\n",
    "        state = states[city_idx]\n",
    "        zip_code = f\"{random.randint(10000, 99999)}\" if random.random() > 0.1 else None\n",
    "        country = countries[city_idx]\n",
    "        address = (street, city, state, zip_code, country)\n",
    "    else:\n",
    "        address = None\n",
    "    \n",
    "    # Phone\n",
    "    phone = f\"{random.randint(100, 999)}-{random.randint(100, 999)}-{random.randint(1000, 9999)}\" if random.random() > 0.2 else None\n",
    "    \n",
    "    # Dates\n",
    "    signup_days = random.randint(0, date_range)\n",
    "    signup_date = start_date.date() + timedelta(days=signup_days)\n",
    "    \n",
    "    if random.random() > 0.1:  # 10% have no activity\n",
    "        activity_days = random.randint(0, date_range - signup_days)  # Activity after signup\n",
    "        last_activity = start_date + timedelta(days=signup_days + activity_days)\n",
    "    else:\n",
    "        last_activity = None\n",
    "    \n",
    "    # Tier (weighted selection)\n",
    "    tier = random.choices(tiers, weights=tier_weights)[0]\n",
    "    \n",
    "    # Preferences\n",
    "    if random.random() > 0.3:  # 30% have no preferences\n",
    "        num_prefs = random.randint(1, len(preference_keys))\n",
    "        selected_prefs = random.sample(preference_keys, num_prefs)\n",
    "        preferences = {k: random.choice(preference_values[k]) for k in selected_prefs}\n",
    "    else:\n",
    "        preferences = None\n",
    "        \n",
    "    customer_data.append((i, name, email, address, phone, signup_date, last_activity, tier, preferences))\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# Save as a permanent table for later use\n",
    "customers_df.write.mode(\"overwrite\").saveAsTable(\"customers_table\")\n",
    "\n",
    "print(f\"Created customers dimension table with {customers_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fact table (transactions) with highly skewed data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"transaction_date\", TimestampType(), False),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"product_id\", IntegerType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"discount\", DoubleType(), True)\n",
    "    ])), False),\n",
    "    StructField(\"payment_method\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"total_amount\", DoubleType(), False),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Generate a smaller sample of 100,000 transactions for notebook performance\n",
    "transaction_data = []\n",
    "payment_methods = [\"credit_card\", \"debit_card\", \"paypal\", \"apple_pay\", \"bank_transfer\", \"gift_card\", \"crypto\"]\n",
    "payment_weights = [0.4, 0.3, 0.1, 0.1, 0.05, 0.04, 0.01]  # Distribution weights\n",
    "statuses = [\"completed\", \"shipped\", \"delivered\", \"cancelled\", \"refunded\", \"processing\"]\n",
    "status_weights = [0.6, 0.2, 0.1, 0.05, 0.03, 0.02]  # Distribution weights\n",
    "store_count = 50\n",
    "\n",
    "metadata_keys = [\"source\", \"device\", \"coupon_code\", \"promotion\", \"referral\"]\n",
    "metadata_values = {\n",
    "    \"source\": [\"web\", \"mobile_app\", \"store\", \"phone\", \"partner\"],\n",
    "    \"device\": [\"desktop\", \"mobile\", \"tablet\", \"kiosk\", \"pos\"],\n",
    "    \"coupon_code\": [\"SAVE10\", \"WELCOME20\", \"FLASH30\", \"SEASON25\", None],\n",
    "    \"promotion\": [\"holiday_sale\", \"clearance\", \"new_customer\", \"loyalty\", None],\n",
    "    \"referral\": [\"friend\", \"search\", \"social\", \"email\", None]\n",
    "}\n",
    "\n",
    "# Create some skewed distributions\n",
    "# 1. Few customers make many purchases (80/20 rule)\n",
    "# 2. Some products are much more popular than others\n",
    "# 3. Some days have much higher transaction volumes\n",
    "\n",
    "# Calculate weighted customer IDs (power law distribution)\n",
    "customer_weights = [1/(i**0.8) for i in range(1, 50001)]\n",
    "total_weight = sum(customer_weights)\n",
    "customer_weights = [w/total_weight for w in customer_weights]\n",
    "\n",
    "# Calculate weighted product IDs (power law distribution)\n",
    "product_weights = [1/(i**0.9) for i in range(1, 5001)]\n",
    "total_weight = sum(product_weights)\n",
    "product_weights = [w/total_weight for w in product_weights]\n",
    "\n",
    "# Generate transactions\n",
    "for i in range(1, 100001):\n",
    "    # Select customer with power law distribution\n",
    "    customer_id = np.random.choice(range(1, 50001), p=customer_weights)\n",
    "    \n",
    "    # Generate transaction date with seasonal patterns\n",
    "    # More transactions during holidays and weekends\n",
    "    random_days = random.randint(0, date_range)\n",
    "    base_date = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    # Adjust for seasonal patterns\n",
    "    month = base_date.month\n",
    "    day_of_week = base_date.weekday()\n",
    "    \n",
    "    # Boost December (holiday season)\n",
    "    if month == 12 and random.random() < 0.6:\n",
    "        # Shift to December\n",
    "        holiday_year = random.choice([2020, 2021, 2022])\n",
    "        base_date = datetime(holiday_year, 12, random.randint(1, 31))\n",
    "    \n",
    "    # Boost weekends\n",
    "    if day_of_week < 5 and random.random() < 0.4:  # Weekday\n",
    "        # Shift to a weekend\n",
    "        weekend_offset = random.choice([5, 6]) - day_of_week  # Shift to Saturday or Sunday\n",
    "        base_date = base_date + timedelta(days=weekend_offset)\n",
    "    \n",
    "    transaction_date = base_date\n",
    "    \n",
    "    # Generate 1-5 items per transaction\n",
    "    num_items = random.choices([1, 2, 3, 4, 5], weights=[0.5, 0.25, 0.15, 0.07, 0.03])[0]\n",
    "    items = []\n",
    "    \n",
    "    total_amount = 0.0\n",
    "    for _ in range(num_items):\n",
    "        # Select product with power law distribution\n",
    "        product_id = np.random.choice(range(1, 5001), p=product_weights)\n",
    "        quantity = random.randint(1, 5)\n",
    "        \n",
    "        # Price depends on product category\n",
    "        if product_id <= 1000:  # High-priced items (ids 1-1000)\n",
    "            price = round(random.uniform(500, 2000), 2)\n",
    "        else:\n",
    "            price = round(random.uniform(10, 500), 2)\n",
    "        \n",
    "        # Apply discount sometimes\n",
    "        discount = round(price * random.uniform(0.05, 0.3), 2) if random.random() < 0.3 else None\n",
    "        item_total = price * quantity\n",
    "        if discount:\n",
    "            item_total -= discount * quantity\n",
    "            \n",
    "        total_amount += item_total\n",
    "        items.append((product_id, quantity, price, discount))\n",
    "    \n",
    "    # Select payment method and status with weighted distribution\n",
    "    payment_method = random.choices(payment_methods, weights=payment_weights)[0]\n",
    "    status = random.choices(statuses, weights=status_weights)[0]\n",
    "    \n",
    "    # Store ID (NULL for online orders)\n",
    "    store_id = random.randint(1, store_count) if random.random() < 0.3 else None\n",
    "    \n",
    "    # Transaction metadata\n",
    "    if random.random() > 0.2:  # 20% have no metadata\n",
    "        num_meta = random.randint(1, 3)\n",
    "        selected_meta = random.sample(metadata_keys, num_meta)\n",
    "        metadata = {k: random.choice(metadata_values[k]) for k in selected_meta if metadata_values[k][-1] is not None or random.random() > 0.2}\n",
    "    else:\n",
    "        metadata = None\n",
    "    \n",
    "    # Add transaction to dataset\n",
    "    transaction_data.append((\n",
    "        i, customer_id, transaction_date, items, payment_method, status, \n",
    "        total_amount, store_id, metadata, transaction_date.year, transaction_date.month\n",
    "    ))\n",
    "\n",
    "transactions_df = spark.createDataFrame(transaction_data, transaction_schema)\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# Save as partitioned table by year and month for partition pruning demos\n",
    "transactions_df.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").saveAsTable(\"transactions_table\")\n",
    "\n",
    "print(f\"Created transactions fact table with {transactions_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Dive into Logical and Physical Plans\n",
    "\n",
    "Now that we have our test datasets ready, let's explore the various aspects of Spark's query planning and optimization in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Analyzing Scan Operations\n",
    "\n",
    "Scan operations determine how Spark reads data from sources and are often the first point of optimization. Let's explore different types of scans and their performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with basic scan operations and examine their plans\n",
    "# 1. Full table scan\n",
    "full_scan = spark.table(\"products_table\")\n",
    "analyze_plans(full_scan, \"Full Table Scan\")\n",
    "\n",
    "# Check execution time\n",
    "time_execution_with_metrics(full_scan, name=\"Full table scan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Column Pruning - Selecting only specific columns\n",
    "column_pruning = spark.table(\"products_table\").select(\"product_id\", \"name\", \"price\")\n",
    "analyze_plans(column_pruning, \"Column Pruning\")\n",
    "\n",
    "# Performance comparison\n",
    "pruning_time, _ = time_execution_with_metrics(column_pruning, name=\"Column pruning scan\")\n",
    "\n",
    "# Examine file format statistics\n",
    "print(\"\\nParquet file metadata:\")\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE DETAIL products_table\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Complex types handling in scans\n",
    "# Reading and projecting into complex types can impact performance\n",
    "complex_scan = spark.table(\"products_table\").select(\n",
    "    \"product_id\", \n",
    "    \"name\", \n",
    "    col(\"attributes\").getItem(\"color\").alias(\"color\"),\n",
    "    col(\"tags\")[0].alias(\"first_tag\")\n",
    ")\n",
    "analyze_plans(complex_scan, \"Complex Type Field Access\")\n",
    "\n",
    "# Performance measurement\n",
    "complex_time, _ = time_execution_with_metrics(complex_scan, name=\"Complex field access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. File format impact on scan\n",
    "# Compare Parquet read vs JSON read\n",
    "\n",
    "# First, save a sample as JSON\n",
    "sample_df = spark.table(\"products_table\").limit(1000)\n",
    "sample_df.write.json(\"/tmp/products_json\", mode=\"overwrite\")\n",
    "\n",
    "# Now read it back and compare scan plans\n",
    "json_scan = spark.read.json(\"/tmp/products_json\")\n",
    "parquet_scan = spark.read.parquet(\"/tmp/products_json\")\n",
    "\n",
    "print(\"JSON Scan Plan:\")\n",
    "json_scan.explain()\n",
    "\n",
    "print(\"\\nParquet Scan Plan:\")\n",
    "parquet_scan.explain()\n",
    "\n",
    "# Compare performance\n",
    "json_time, _ = time_execution_with_metrics(json_scan, name=\"JSON scan\")\n",
    "parquet_time, _ = time_execution_with_metrics(parquet_scan, name=\"Parquet scan\")\n",
    "\n",
    "print(f\"Performance difference: Parquet is {json_time/parquet_time:.2f}x faster than JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Scan Optimization Techniques\n",
    "\n",
    "1. **File Format Selection**: The file format significantly impacts scan performance:\n",
    "   - Parquet and ORC use columnar storage, allowing efficient column pruning and predicate pushdown\n",
    "   - JSON and CSV require parsing the entire row, even if you only need a few columns\n",
    "   - Avro provides good compression and schema evolution\n",
    "\n",
    "2. **File Size and Splitting**: \n",
    "   - Too many small files create overhead (\"small file problem\")\n",
    "   - Files that are too large might not utilize parallelism efficiently\n",
    "   - Ideal file size typically ranges from 64MB to 1GB\n",
    "\n",
    "3. **Statistics Utilization**:\n",
    "   - File formats like Parquet store statistics (min/max values) for columns\n",
    "   - These statistics enable file pruning before reading data\n",
    "   - Computing and maintaining statistics for tables improves planner decisions\n",
    "\n",
    "4. **Vectorization**:\n",
    "   - Modern file formats support vectorized reads (batch processing)\n",
    "   - Column vectors are processed more efficiently than row-by-row\n",
    "   - Parquet and ORC have built-in support for vectorized reading\n",
    "\n",
    "Let's examine file pruning using statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine statistics-based file pruning\n",
    "# First, let's analyze a partitioned table to see partition pruning\n",
    "partition_info = spark.sql(\"\"\"\n",
    "    SHOW PARTITIONS transactions_table\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Number of partitions in transactions_table: {len(partition_info)}\")\n",
    "for p in partition_info[:10]:\n",
    "    print(p[0])\n",
    "print(\"...\")\n",
    "\n",
    "# Statistics-based file skipping with Parquet\n",
    "high_price_scan = spark.table(\"products_table\").filter(col(\"price\") > 1500)\n",
    "analyze_plans(high_price_scan, \"Statistics-based Pruning\")\n",
    "\n",
    "# Force enable/disable stats collection for comparison\n",
    "current_stats = spark.conf.get(\"spark.sql.statistics.histogram.enabled\")\n",
    "print(f\"Current histogram stats setting: {current_stats}\")\n",
    "\n",
    "# Enable detailed statistics\n",
    "spark.conf.set(\"spark.sql.statistics.histogram.enabled\", \"true\")\n",
    "spark.sql(\"ANALYZE TABLE products_table COMPUTE STATISTICS FOR COLUMNS price\")\n",
    "\n",
    "# Check collected statistics\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE EXTENDED products_table price\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding Filter and Projection Pushdown\n",
    "\n",
    "Filter and projection pushdown are optimization techniques that reduce the amount of data read from disk and transferred between nodes. Let's examine these in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate filter pushdown capabilities\n",
    "simple_filter = spark.table(\"transactions_table\").filter(col(\"total_amount\") > 1000)\n",
    "print(\"Simple Filter Query Plan:\")\n",
    "simple_filter.explain()\n",
    "\n",
    "# Examine PushedFilters in the plan\n",
    "print(\"\\nDetailed Simple Filter Query Plan:\")\n",
    "simple_filter.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex expressions that may or may not be pushed down\n",
    "# 1. Simple comparison (usually pushes down)\n",
    "filter1 = spark.table(\"transactions_table\").filter(col(\"total_amount\") > 1000)\n",
    "\n",
    "# 2. Compound condition (usually pushes down)\n",
    "filter2 = spark.table(\"transactions_table\").filter(\n",
    "    (col(\"total_amount\") > 1000) & \n",
    "    (col(\"payment_method\") == \"credit_card\")\n",
    ")\n",
    "\n",
    "# 3. Functions that may not push down\n",
    "filter3 = spark.table(\"transactions_table\").filter(\n",
    "    year(col(\"transaction_date\")) == 2022\n",
    ")\n",
    "\n",
    "# 4. UDFs (typically don't push down)\n",
    "@udf(\"boolean\")\n",
    "def is_expensive(amount):\n",
    "    return amount > 1000\n",
    "\n",
    "filter4 = spark.table(\"transactions_table\").filter(\n",
    "    is_expensive(col(\"total_amount\"))\n",
    ")\n",
    "\n",
    "# Display plans to compare\n",
    "print(\"Filter 1 - Simple Comparison:\")\n",
    "filter1.explain()\n",
    "\n",
    "print(\"\\nFilter 2 - Compound Condition:\")\n",
    "filter2.explain()\n",
    "\n",
    "print(\"\\nFilter 3 - Built-in Function:\")\n",
    "filter3.explain()\n",
    "\n",
    "print(\"\\nFilter 4 - UDF:\")\n",
    "filter4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance impact of filter pushdown vs. no pushdown\n",
    "# Using built-in functions (can be pushed down) vs UDFs (cannot be pushed down)\n",
    "\n",
    "# Filter using built-in functions\n",
    "time_start = time.time()\n",
    "builtin_result = spark.table(\"transactions_table\").filter(\n",
    "    (col(\"total_amount\") > 1000) & \n",
    "    (col(\"status\") == \"completed\")\n",
    ").select(\"transaction_id\", \"customer_id\", \"total_amount\").count()\n",
    "builtin_time = time.time() - time_start\n",
    "\n",
    "# Filter using UDFs (prevents pushdown)\n",
    "@udf(\"boolean\")\n",
    "def matches_criteria(amount, status):\n",
    "    return amount > 1000 and status == \"completed\"\n",
    "\n",
    "time_start = time.time()\n",
    "udf_result = spark.table(\"transactions_table\").filter(\n",
    "    matches_criteria(col(\"total_amount\"), col(\"status\"))\n",
    ").select(\"transaction_id\", \"customer_id\", \"total_amount\").count()\n",
    "udf_time = time.time() - time_start\n",
    "\n",
    "print(f\"Built-in function filter: {builtin_time:.4f} seconds\")\n",
    "print(f\"UDF filter (no pushdown): {udf_time:.4f} seconds\")\n",
    "print(f\"Performance difference: {udf_time/builtin_time:.2f}x slower without pushdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Pushdown and Column Pruning\n",
    "\n",
    "Projection pushdown (also known as column pruning) works by reading only the columns needed for a query. This reduces I/O, network transfer, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare column pruning effectiveness with different queries\n",
    "\n",
    "# 1. Select few columns\n",
    "few_columns = spark.table(\"products_table\").select(\"product_id\", \"name\", \"price\")\n",
    "\n",
    "# 2. Select many columns\n",
    "many_columns = spark.table(\"products_table\").select(\"*\")\n",
    "\n",
    "# 3. Select with complex derived columns\n",
    "complex_columns = spark.table(\"products_table\").select(\n",
    "    \"product_id\", \n",
    "    \"name\", \n",
    "    \"price\",\n",
    "    col(\"attributes\").getItem(\"color\").alias(\"color\"),\n",
    "    col(\"attributes\").getItem(\"size\").alias(\"size\"),\n",
    "    col(\"tags\")[0].alias(\"first_tag\"),\n",
    "    (col(\"price\") * 1.1).alias(\"price_with_tax\")\n",
    ")\n",
    "\n",
    "# Compare plans and execution times\n",
    "print(\"Few Columns Query Plan:\")\n",
    "few_columns.explain()\n",
    "\n",
    "print(\"\\nMany Columns Query Plan:\")\n",
    "many_columns.explain()\n",
    "\n",
    "print(\"\\nComplex Columns Query Plan:\")\n",
    "complex_columns.explain()\n",
    "\n",
    "# Measure performance\n",
    "few_time, _ = time_execution_with_metrics(few_columns, name=\"Few columns\")\n",
    "many_time, _ = time_execution_with_metrics(many_columns, name=\"Many columns\")\n",
    "complex_time, _ = time_execution_with_metrics(complex_columns, name=\"Complex columns\")\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"Reading all columns is {many_time/few_time:.2f}x slower than selecting few columns\")\n",
    "print(f\"Complex column selection is {complex_time/few_time:.2f}x slower than simple column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Filter Optimization Techniques\n",
    "\n",
    "1. **Filter Order**: Apply the most selective filters first to reduce data volume early\n",
    "2. **Partition Pruning**: Filter on partition columns for massive performance gains\n",
    "3. **File Skipping**: Use filters that can leverage statistics for file-level skipping\n",
    "4. **Predicate Reordering**: The optimizer can reorder predicates for better performance\n",
    "5. **Avoid UDFs in Filters**: UDFs prevent pushdown, use built-in functions when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine partition pruning with different filter orderings\n",
    "\n",
    "# 1. Filter on partition columns first\n",
    "partition_first = spark.table(\"transactions_table\") \\\n",
    "    .filter(col(\"year\") == 2022) \\\n",
    "    .filter(col(\"month\") == 12) \\\n",
    "    .filter(col(\"total_amount\") > 1000)\n",
    "\n",
    "# 2. Filter on non-partition columns first\n",
    "non_partition_first = spark.table(\"transactions_table\") \\\n",
    "    .filter(col(\"total_amount\") > 1000) \\\n",
    "    .filter(col(\"year\") == 2022) \\\n",
    "    .filter(col(\"month\") == 12)\n",
    "\n",
    "# Compare plans\n",
    "print(\"Partition First Query Plan:\")\n",
    "partition_first.explain()\n",
    "\n",
    "print(\"\\nNon-Partition First Query Plan:\")\n",
    "non_partition_first.explain()\n",
    "\n",
    "# Measure performance\n",
    "partition_time, _ = time_execution_with_metrics(partition_first, name=\"Partition first\")\n",
    "non_partition_time, _ = time_execution_with_metrics(non_partition_first, name=\"Non-partition first\")\n",
    "\n",
    "print(f\"\\nNote: Both plans should have similar performance due to the query optimizer's ability to reorder filters.\")\n",
    "print(f\"Actual performance ratio: {non_partition_time/partition_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Partition Pruning\n",
    "\n",
    "Dynamic partition pruning is an optimization technique that determines which partitions to read based on filters that are only known at runtime. This often happens with join conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample partitioned table for this example\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_by_month AS\n",
    "SELECT\n",
    "    year,\n",
    "    month,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(total_amount) as total_sales\n",
    "FROM transactions_table\n",
    "GROUP BY year, month\n",
    "\"\"\")\n",
    "\n",
    "# Now let's try dynamic partition pruning with a join\n",
    "query = spark.sql(\"\"\"\n",
    "SELECT t.transaction_id, t.customer_id, t.total_amount\n",
    "FROM transactions_table t\n",
    "JOIN sales_by_month s ON t.year = s.year AND t.month = s.month\n",
    "WHERE s.total_sales > 100000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join with Dynamic Partition Pruning:\")\n",
    "query.explain()\n",
    "\n",
    "# Check execution metrics\n",
    "time_execution_with_metrics(query, name=\"Dynamic partition pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Filter and Projection Pushdown\n",
    "\n",
    "To maximize the benefits of filter and projection pushdown:\n",
    "\n",
    "1. **Use compatible file formats**: Parquet, ORC, and Delta support pushdown effectively\n",
    "2. **Select only required columns**: Avoid `select(\"*\")` when possible\n",
    "3. **Use built-in functions**: Avoid UDFs in filter conditions\n",
    "4. **Filter on partition columns**: Design your partitioning scheme for common query patterns\n",
    "5. **Compute and maintain statistics**: Enable better file skipping\n",
    "6. **Apply filters early**: Push filters as close to the data source as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Detailed Join Strategy Analysis\n",
    "\n",
    "Joins are often the most expensive operations in Spark queries. Understanding different join strategies and their performance characteristics is crucial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first check the current broadcast join threshold configuration\n",
    "broadcast_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Current broadcast join threshold: {broadcast_threshold}\")\n",
    "\n",
    "# Check table sizes to understand broadcasting decisions\n",
    "print(\"\\nTable sizes:\")\n",
    "spark.sql(\"SELECT COUNT(*) AS transactions_count FROM transactions_table\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS customers_count FROM customers_table\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS products_count FROM products_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine different join types and how Spark chooses them\n",
    "# First, create a common join query\n",
    "customer_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "# Analyze the join plan\n",
    "print(\"Default Join Strategy:\")\n",
    "customer_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's disable auto-broadcasting and force a shuffle hash join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Run the same query with broadcast disabled\n",
    "no_broadcast_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join without broadcasting:\")\n",
    "no_broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly force a broadcast join using the broadcast hint\n",
    "# Reset broadcast threshold first\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", broadcast_threshold)\n",
    "\n",
    "# Use broadcast hint\n",
    "forced_broadcast = spark.sql(\"\"\"\n",
    "SELECT /*+ BROADCAST(c) */\n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "WHERE \n",
    "    t.year = 2022 AND t.total_amount > 500\n",
    "\"\"\")\n",
    "\n",
    "print(\"Join with broadcast hint:\")\n",
    "forced_broadcast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Join Strategies\n",
    "\n",
    "Let's compare the performance of different join strategies on the same query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's systematically compare different join strategies\n",
    "# We'll use DataFrame API for easier control\n",
    "\n",
    "# Create the base tables for joins\n",
    "transactions = spark.table(\"transactions_table\").filter((col(\"year\") == 2022) & (col(\"total_amount\") > 500))\n",
    "customers = spark.table(\"customers_table\")\n",
    "\n",
    "# 1. Broadcast Hash Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50m\")  # Ensure broadcasting\n",
    "broadcast_join = transactions.join(broadcast(customers), transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"Broadcast Hash Join Plan:\")\n",
    "broadcast_join.explain()\n",
    "broadcast_time, _ = time_execution_with_metrics(broadcast_join, name=\"Broadcast Hash Join\")\n",
    "\n",
    "# 2. Sort Merge Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable broadcasting\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")  # Prefer sort merge\n",
    "sort_merge_join = transactions.join(customers, transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"\\nSort Merge Join Plan:\")\n",
    "sort_merge_join.explain()\n",
    "sort_merge_time, _ = time_execution_with_metrics(sort_merge_join, name=\"Sort Merge Join\")\n",
    "\n",
    "# 3. Shuffle Hash Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable broadcasting\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")  # Disable preference for sort merge\n",
    "shuffle_hash_join = transactions.join(customers, transactions[\"customer_id\"] == customers[\"customer_id\"])\n",
    "print(\"\\nShuffle Hash Join Plan:\")\n",
    "shuffle_hash_join.explain()\n",
    "shuffle_hash_time, _ = time_execution_with_metrics(shuffle_hash_join, name=\"Shuffle Hash Join\")\n",
    "\n",
    "# Reset configs to default\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", broadcast_threshold)\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nJoin Strategy Performance Comparison:\")\n",
    "print(f\"Broadcast Hash Join: {broadcast_time:.4f} seconds (baseline)\")\n",
    "print(f\"Sort Merge Join: {sort_merge_time:.4f} seconds ({sort_merge_time/broadcast_time:.2f}x vs broadcast)\")\n",
    "print(f\"Shuffle Hash Join: {shuffle_hash_time:.4f} seconds ({shuffle_hash_time/broadcast_time:.2f}x vs broadcast)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Order and Join Reordering\n",
    "\n",
    "Join order can significantly impact performance. The Catalyst optimizer attempts to reorder joins optimally, but sometimes it needs help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze join order optimization with a 3-way join\n",
    "\n",
    "# Create a common query with 3-way join\n",
    "complex_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    c.name as customer_name,\n",
    "    p.name as product_name,\n",
    "    i.quantity,\n",
    "    i.price\n",
    "FROM \n",
    "    transactions_table t,\n",
    "    customers_table c,\n",
    "    products_table p,\n",
    "    LATERAL EXPLODE(t.items) as i\n",
    "WHERE \n",
    "    t.customer_id = c.customer_id AND\n",
    "    i.product_id = p.product_id AND\n",
    "    t.year = 2022 AND\n",
    "    t.month = 12\n",
    "\"\"\")\n",
    "\n",
    "# Analyze the plan\n",
    "print(\"Complex Join Plan:\")\n",
    "complex_join.explain()\n",
    "\n",
    "# Now try with hints to influence join order\n",
    "hinted_join = spark.sql(\"\"\"\n",
    "SELECT /*+ LEADING(t, c, p) BROADCAST(c) BROADCAST(p) */\n",
    "    t.transaction_id,\n",
    "    c.name as customer_name,\n",
    "    p.name as product_name,\n",
    "    i.quantity,\n",
    "    i.price\n",
    "FROM \n",
    "    transactions_table t,\n",
    "    customers_table c,\n",
    "    products_table p,\n",
    "    LATERAL EXPLODE(t.items) as i\n",
    "WHERE \n",
    "    t.customer_id = c.customer_id AND\n",
    "    i.product_id = p.product_id AND\n",
    "    t.year = 2022 AND\n",
    "    t.month = 12\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nComplex Join with Hints Plan:\")\n",
    "hinted_join.explain()\n",
    "\n",
    "# Compare performance\n",
    "complex_time, _ = time_execution_with_metrics(complex_join, name=\"Standard Join Order\")\n",
    "hinted_time, _ = time_execution_with_metrics(hinted_join, name=\"Hinted Join Order\")\n",
    "\n",
    "print(f\"\\nPerformance impact of join order: {complex_time/hinted_time:.2f}x improvement with hints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Data Skew in Joins\n",
    "\n",
    "Data skew can cause severe performance problems in joins, especially with sort-merge and shuffle hash joins. Let's examine techniques to identify and handle skew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a query that will exhibit skew due to the distribution of our data\n",
    "# Remember that we created transactions with power-law distribution of customer_ids\n",
    "skewed_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    c.name as customer_name, \n",
    "    t.total_amount,\n",
    "    t.transaction_date\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Force a sort merge join to see the impact of skew\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "print(\"Join with potential skew:\")\n",
    "skewed_join.explain()\n",
    "\n",
    "# Check distribution of customer_ids in our transactions\n",
    "customer_distribution = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id, \n",
    "    COUNT(*) as transaction_count\n",
    "FROM \n",
    "    transactions_table\n",
    "GROUP BY \n",
    "    customer_id\n",
    "ORDER BY \n",
    "    transaction_count DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTop 10 customers by transaction count:\")\n",
    "customer_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Techniques to handle skew in joins\n",
    "\n",
    "# 1. Using salting to distribute skewed keys\n",
    "# First, identify skewed keys\n",
    "skewed_keys = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id \n",
    "FROM (\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        COUNT(*) as cnt\n",
    "    FROM \n",
    "        transactions_table\n",
    "    GROUP BY \n",
    "        customer_id\n",
    ") t\n",
    "WHERE \n",
    "    cnt > 10\n",
    "\"\"\").collect()\n",
    "\n",
    "skewed_key_list = [row[0] for row in skewed_keys]\n",
    "print(f\"Identified {len(skewed_key_list)} skewed customer keys\")\n",
    "\n",
    "# Apply salting technique for skewed keys\n",
    "transactions_df = spark.table(\"transactions_table\")\n",
    "customers_df = spark.table(\"customers_table\")\n",
    "\n",
    "# Number of salt values\n",
    "num_salts = 10\n",
    "\n",
    "# Add salt to skewed transactions\n",
    "salted_transactions = transactions_df.withColumn(\n",
    "    \"salt\", \n",
    "    when(col(\"customer_id\").isin(skewed_key_list), \n",
    "         (rand() * num_salts).cast(\"int\"))\n",
    "    .otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# Duplicate customers for skewed keys\n",
    "from pyspark.sql.functions import explode, array\n",
    "\n",
    "# Create salt values for skewed customers\n",
    "expanded_customers = customers_df.withColumn(\n",
    "    \"salt_values\",\n",
    "    when(col(\"customer_id\").isin(skewed_key_list),\n",
    "         array([lit(i) for i in range(num_salts)]))\n",
    "    .otherwise(array(lit(0)))\n",
    ")\n",
    "\n",
    "# Explode to create multiple rows for skewed customers\n",
    "salted_customers = expanded_customers \\\n",
    "    .withColumn(\"salt\", explode(\"salt_values\")) \\\n",
    "    .drop(\"salt_values\")\n",
    "\n",
    "# Join with salted keys\n",
    "salted_join = salted_transactions.join(\n",
    "    salted_customers,\n",
    "    (salted_transactions[\"customer_id\"] == salted_customers[\"customer_id\"]) &\n",
    "    (salted_transactions[\"salt\"] == salted_customers[\"salt\"])\n",
    ")\n",
    "\n",
    "print(\"\\nPlan with salting technique:\")\n",
    "salted_join.explain()\n",
    "\n",
    "# Compare performance\n",
    "standard_time, _ = time_execution_with_metrics(skewed_join, name=\"Standard join with skew\")\n",
    "salted_time, _ = time_execution_with_metrics(salted_join, name=\"Join with salting\")\n",
    "\n",
    "print(f\"\\nPerformance impact of salting: {standard_time/salted_time:.2f}x improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Join Optimization Techniques\n",
    "\n",
    "1. **Choose the Right Join Strategy**:\n",
    "   - Broadcast Hash Join: Use for small-to-medium tables that can fit in memory\n",
    "   - Sort Merge Join: Best for large tables with evenly distributed keys\n",
    "   - Shuffle Hash Join: Can be faster than Sort Merge for medium-sized tables\n",
    "\n",
    "2. **Join Order Optimization**:\n",
    "   - Join the most filtered/smallest tables first\n",
    "   - Use the LEADING hint to control join order\n",
    "   - Filter tables before joining when possible\n",
    "\n",
    "3. **Data Skew Handling**:\n",
    "   - Use salting for skewed keys\n",
    "   - Split processing of skewed and non-skewed data\n",
    "   - Consider pre-aggregating data before joins\n",
    "\n",
    "4. **Configuration Tuning**:\n",
    "   - Adjust broadcast threshold based on your data size\n",
    "   - Set appropriate shuffle partition count\n",
    "   - Consider enabling adaptive query execution for dynamic optimization\n",
    "\n",
    "5. **Schema Optimization**:\n",
    "   - Ensure join keys have the same data type to avoid implicit conversions\n",
    "   - Consider clustering or bucketing tables on join keys\n",
    "\n",
    "6. **Join Rewriting**:\n",
    "   - Convert complex multi-way joins to simpler joins when possible\n",
    "   - Use subqueries or CTEs to break down complex joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimizing Shuffle Operations\n",
    "\n",
    "Shuffles are among the most expensive operations in Spark, involving disk I/O, serialization, network transfer, and deserialization. Understanding and optimizing shuffles is critical for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first examine the current shuffle configuration\n",
    "print(\"Current shuffle configuration:\")\n",
    "print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"spark.default.parallelism: {spark.conf.get('spark.default.parallelism')}\")\n",
    "print(f\"spark.sql.shuffle.compress: {spark.conf.get('spark.sql.shuffle.compress')}\")\n",
    "print(f\"spark.sql.adaptive.enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"spark.sql.adaptive.coalescePartitions.enabled: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled', 'not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create queries that trigger different types of shuffles\n",
    "\n",
    "# 1. Shuffle due to repartition\n",
    "repartition_shuffle = spark.table(\"transactions_table\").repartition(10)\n",
    "print(\"Repartition Shuffle Plan:\")\n",
    "repartition_shuffle.explain()\n",
    "\n",
    "# 2. Shuffle due to groupBy\n",
    "group_shuffle = spark.table(\"transactions_table\").groupBy(\"payment_method\").count()\n",
    "print(\"\\nGroupBy Shuffle Plan:\")\n",
    "group_shuffle.explain()\n",
    "\n",
    "# 3. Shuffle due to join (without broadcast)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "join_shuffle = spark.table(\"transactions_table\").join(\n",
    "    spark.table(\"customers_table\"),\n",
    "    \"customer_id\"\n",
    ")\n",
    "print(\"\\nJoin Shuffle Plan:\")\n",
    "join_shuffle.explain()\n",
    "\n",
    "# 4. Shuffle due to window function\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"payment_method\").orderBy(\"total_amount\")\n",
    "window_shuffle = spark.table(\"transactions_table\").withColumn(\n",
    "    \"rank\", rank().over(window_spec)\n",
    ")\n",
    "print(\"\\nWindow Function Shuffle Plan:\")\n",
    "window_shuffle.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Shuffle Partition Count\n",
    "\n",
    "The number of shuffle partitions significantly impacts performance. Too few can lead to memory pressure, while too many create small tasks with overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test different shuffle partition counts\n",
    "partition_counts = [5, 20, 100, 200]\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "    t.customer_id,\n",
    "    c.name,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(t.total_amount) as total_spent,\n",
    "    AVG(t.total_amount) as avg_transaction\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "GROUP BY \n",
    "    t.customer_id, c.name\n",
    "ORDER BY \n",
    "    total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "results = []\n",
    "for partitions in partition_counts:\n",
    "    # Set the shuffle partitions\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", partitions)\n",
    "    \n",
    "    # Run the test query\n",
    "    test_df = spark.sql(test_query)\n",
    "    \n",
    "    # Measure performance\n",
    "    print(f\"\\nTesting with {partitions} shuffle partitions:\")\n",
    "    test_df.explain()\n",
    "    execution_time, _ = time_execution_with_metrics(test_df, name=f\"Shuffle with {partitions} partitions\")\n",
    "    \n",
    "    results.append((partitions, execution_time))\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nShuffle Partition Performance Summary:\")\n",
    "for partitions, time in results:\n",
    "    print(f\"Partitions: {partitions}, Time: {time:.4f} seconds\")\n",
    "\n",
    "# Find optimal partition count\n",
    "optimal_partitions, min_time = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal partition count: {optimal_partitions} with {min_time:.4f} seconds\")\n",
    "\n",
    "# Reset to a reasonable value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding Unnecessary Shuffles\n",
    "\n",
    "One of the best optimizations is to avoid unnecessary shuffles entirely. Let's explore techniques for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1: Use broadcast joins to avoid shuffles\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10m\")\n",
    "\n",
    "# Join with shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "shuffle_join = spark.table(\"transactions_table\").filter(col(\"year\") == 2022).join(\n",
    "    spark.table(\"customers_table\"),\n",
    "    \"customer_id\"\n",
    ")\n",
    "\n",
    "# Join with broadcast to avoid shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50m\")\n",
    "broadcast_join = spark.table(\"transactions_table\").filter(col(\"year\") == 2022).join(\n",
    "    broadcast(spark.table(\"customers_table\")),\n",
    "    \"customer_id\"\n",
    ")\n",
    "\n",
    "print(\"Join with shuffle:\")\n",
    "shuffle_join.explain()\n",
    "\n",
    "print(\"\\nJoin with broadcast (no shuffle):\")\n",
    "broadcast_join.explain()\n",
    "\n",
    "# Measure performance\n",
    "shuffle_time, _ = time_execution_with_metrics(shuffle_join, name=\"Join with shuffle\")\n",
    "broadcast_time, _ = time_execution_with_metrics(broadcast_join, name=\"Join with broadcast\")\n",
    "\n",
    "print(f\"\\nPerformance improvement: {shuffle_time/broadcast_time:.2f}x faster with broadcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2: Use map-side aggregation to reduce shuffle data volume\n",
    "\n",
    "# First check the configuration\n",
    "print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"spark.sql.adaptive.enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Standard aggregation (full shuffle)\n",
    "standard_agg = spark.table(\"transactions_table\").groupBy(\"customer_id\").sum(\"total_amount\")\n",
    "\n",
    "# Two-phase aggregation (map-side + reduce)\n",
    "# We manually repartition on the groupBy key first to colocate data\n",
    "two_phase_agg = spark.table(\"transactions_table\") \\\n",
    "    .repartition(20, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .sum(\"total_amount\")\n",
    "\n",
    "print(\"Standard Aggregation Plan:\")\n",
    "standard_agg.explain()\n",
    "\n",
    "print(\"\\nTwo-Phase Aggregation Plan:\")\n",
    "two_phase_agg.explain()\n",
    "\n",
    "# Measure performance\n",
    "standard_time, _ = time_execution_with_metrics(standard_agg, name=\"Standard aggregation\")\n",
    "two_phase_time, _ = time_execution_with_metrics(two_phase_agg, name=\"Two-phase aggregation\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: Two-phase is {standard_time/two_phase_time:.2f}x vs standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 3: Coalesce instead of repartition when reducing partitions\n",
    "\n",
    "# Repartition creates a full shuffle\n",
    "repartition_df = spark.table(\"transactions_table\").repartition(10)\n",
    "\n",
    "# Coalesce avoids a full shuffle when reducing partitions\n",
    "coalesce_df = spark.table(\"transactions_table\").coalesce(10)\n",
    "\n",
    "print(\"Repartition Plan (full shuffle):\")\n",
    "repartition_df.explain()\n",
    "\n",
    "print(\"\\nCoalesce Plan (partial shuffle):\")\n",
    "coalesce_df.explain()\n",
    "\n",
    "# Measure performance\n",
    "repartition_time, _ = time_execution_with_metrics(repartition_df, name=\"Repartition\")\n",
    "coalesce_time, _ = time_execution_with_metrics(coalesce_df, name=\"Coalesce\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: Coalesce is {repartition_time/coalesce_time:.2f}x faster than repartition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Query Execution for Shuffle Optimization\n",
    "\n",
    "Adaptive Query Execution (AQE) can dynamically optimize shuffles at runtime. Let's explore its impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with and without AQE\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "    c.tier,\n",
    "    t.payment_method,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(t.total_amount) as total_sales\n",
    "FROM \n",
    "    transactions_table t\n",
    "JOIN \n",
    "    customers_table c\n",
    "ON \n",
    "    t.customer_id = c.customer_id\n",
    "GROUP BY \n",
    "    c.tier, t.payment_method\n",
    "ORDER BY \n",
    "    total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "# Set a high shuffle partition count to see AQE's impact\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "# Test without AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "non_adaptive_df = spark.sql(test_query)\n",
    "\n",
    "print(\"Plan without Adaptive Execution:\")\n",
    "non_adaptive_df.explain()\n",
    "\n",
    "# Test with AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "adaptive_df = spark.sql(test_query)\n",
    "\n",
    "print(\"\\nPlan with Adaptive Execution:\")\n",
    "adaptive_df.explain(\"formatted\")\n",
    "\n",
    "# Measure performance\n",
    "non_adaptive_time, _ = time_execution_with_metrics(non_adaptive_df, name=\"Non-adaptive execution\")\n",
    "adaptive_time, _ = time_execution_with_metrics(adaptive_df, name=\"Adaptive execution\")\n",
    "\n",
    "print(f\"\\nPerformance comparison: AQE is {non_adaptive_time/adaptive_time:.2f}x faster\")\n",
    "\n",
    "# Reset configurations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", optimal_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Shuffle Optimization Techniques\n",
    "\n",
    "1. **Tune Shuffle Partitions**: \n",
    "   - For small-medium datasets: Use 2-3 the number of CPU cores\n",
    "   - For large datasets: Consider 1-2 the number of cores per GB of shuffle data\n",
    "   - Monitor task duration - target 50-200ms per task\n",
    "\n",
    "2. **Avoid Unnecessary Shuffles**: \n",
    "   - Use broadcast joins when possible\n",
    "   - Use coalesce instead of repartition when reducing partitions\n",
    "   - Reuse existing partitioning when possible\n",
    "\n",
    "3. **Optimize Data Size**:\n",
    "   - Pre-aggregate or filter data before shuffling\n",
    "   - Select only required columns before shuffling\n",
    "   - Use efficient serialization formats\n",
    "\n",
    "4. **Balance Data Distribution**:\n",
    "   - Address data skew (as covered in join optimization)\n",
    "   - Consider custom partitioners for better balance\n",
    "\n",
    "5. **Use Adaptive Execution**:\n",
    "   - Enable adaptive query execution for dynamic optimization\n",
    "   - Let Spark automatically adjust partition counts\n",
    "   - Enable skew join optimization\n",
    "\n",
    "6. **Monitor Shuffle Service**:\n",
    "   - Check for external shuffle service health\n",
    "   - Ensure sufficient disk space for shuffle files\n",
    "   - Consider configuring `spark.local.dir` for faster disks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 