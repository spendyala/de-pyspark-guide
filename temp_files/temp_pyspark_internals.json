{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Internals: Architecture, Execution, and Optimization\n",
        "\n",
        "This notebook provides an in-depth look at PySpark's internal workings, including:\n",
        "\n",
        "1. Spark Architecture Overview\n",
        "2. Executors and Resource Management\n",
        "3. Task Slots and Parallelism\n",
        "4. Lazy Execution and DAG Optimization\n",
        "5. Memory Management\n",
        "6. Data Shuffling and Partitioning\n",
        "7. Performance Tuning\n",
        "\n",
        "Throughout this tutorial, we'll combine theory with practical demonstrations to help you understand how Spark works under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session with specific configurations for demonstration\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, explode, lit, udf\n",
        "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
        "\n",
        "# Create a configuration with explicit executor settings for illustration\n",
        "conf = SparkConf() \\\n",
        "    .set(\"spark.executor.instances\", \"2\") \\\n",
        "    .set(\"spark.executor.cores\", \"2\") \\\n",
        "    .set(\"spark.executor.memory\", \"1g\") \\\n",
        "    .set(\"spark.default.parallelism\", \"4\") \\\n",
        "    .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "\n",
        "# Initialize SparkSession with our configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Internals Tutorial\") \\\n",
        "    .config(conf=conf) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# When running in local mode, some of these settings might be overridden\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(\"\\nActive Spark Configuration:\")\n",
        "for item in sorted(spark.sparkContext.getConf().getAll()):\n",
        "    print(\" - \", item[0], \":\", item[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Spark Architecture Overview\n",
        "\n",
        "Apache Spark has a distributed architecture consisting of:\n",
        "\n",
        "1. **Driver**: The central coordinator that:\n",
        "   - Runs your main program\n",
        "   - Analyzes, distributes, and schedules work across executors\n",
        "   - Maintains information about the Spark application\n",
        "   - Responds to user program or input\n",
        "   - Distributes work to executors\n",
        "\n",
        "2. **Executors**: Worker nodes that:\n",
        "   - Run the tasks assigned by the driver\n",
        "   - Store computation results in memory or disk\n",
        "   - Return results to the driver\n",
        "\n",
        "3. **Cluster Manager**: External service for acquiring resources (e.g., YARN, Mesos, Kubernetes, or Spark's standalone manager)\n",
        "\n",
        "Here's a simplified diagram of this architecture:\n",
        "\n",
        "```\n",
        "+------------------+     +----------------+\n",
        "|                  |     |                |\n",
        "|  Driver Program  |     | Cluster        |\n",
        "|  (SparkContext) |<--->| Manager        |\n",
        "|                  |     |                |\n",
        "+------------------+     +----------------+\n",
        "         |                       |\n",
        "         v                       v\n",
        "+------------------+     +------------------+\n",
        "|                  |     |                  |\n",
        "|    Executor 1    |     |    Executor N    |\n",
        "| +---+ +---+ +---+|     | +---+ +---+ +---+|\n",
        "| |T1 | |T2 | |T3 ||     | |T1 | |T2 | |T3 ||\n",
        "| +---+ +---+ +---+|     | +---+ +---+ +---+|\n",
        "|                  |     |                  |\n",
        "| (Cache)  (Cache) |     | (Cache)  (Cache) |\n",
        "+------------------+     +------------------+\n",
        "```\n",
        "\n",
        "In local mode (which we're using in this notebook), all these components run within the same JVM process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Executors and Resource Management\n",
        "\n",
        "Executors are worker processes responsible for running individual tasks for a Spark application. They're launched at the beginning of a Spark application and typically run for the entire lifetime of the application.\n",
        "\n",
        "Key executor configurations:\n",
        "\n",
        "- **spark.executor.instances**: Total number of executors requested\n",
        "- **spark.executor.cores**: Number of CPU cores allocated to each executor (also determines max tasks per executor)\n",
        "- **spark.executor.memory**: Amount of memory allocated to each executor\n",
        "- **spark.executor.memoryOverhead**: Additional memory overhead (default is 10% of executor memory)\n",
        "\n",
        "Let's look at our current configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get executor-related configurations\n",
        "executor_configs = [\n",
        "    \"spark.executor.instances\",\n",
        "    \"spark.executor.cores\",\n",
        "    \"spark.executor.memory\",\n",
        "    \"spark.executor.memoryOverhead\",\n",
        "    \"spark.driver.memory\",\n",
        "    \"spark.driver.cores\"\n",
        "]\n",
        "\n",
        "print(\"Current Executor Configuration:\")\n",
        "for config in executor_configs:\n",
        "    value = spark.conf.get(config, \"Not explicitly set\")\n",
        "    print(f\"{config}: {value}\")\n",
        "\n",
        "# In local mode, we can also see the actual cores available\n",
        "print(f\"\\nAvailable cores: {spark.sparkContext.defaultParallelism}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Executor Memory Breakdown\n",
        "\n",
        "Each executor's memory is divided into several regions:\n",
        "\n",
        "1. **Reserved Memory**: (~300MB) System reservation\n",
        "2. **User Memory**: (~40%) For user-defined data structures and Spark internal metadata\n",
        "3. **Execution Memory**: (~60%) For computation (joins, aggregations, shuffles)\n",
        "4. **Storage Memory**: Shared with execution memory, adjusts dynamically\n",
        "\n",
        "```\n",
        "+--------------------+ ------------------\n",
        "|  Reserved Memory   |     (~300MB)\n",
        "+--------------------+ ------------------\n",
        "|                    |\n",
        "|    User Memory     |     (~40%)\n",
        "|                    |\n",
        "+--------------------+ ------------------\n",
        "|                    |\n",
        "|   Execution and    |\n",
        "|  Storage Memory    |     (~60%)\n",
        "|  (Dynamic Sharing) |\n",
        "|                    |\n",
        "+--------------------+ ------------------\n",
        "```\n",
        "\n",
        "This memory management is controlled by configurations such as:\n",
        "- `spark.memory.fraction`: Fraction of heap used for execution and storage (default 0.6)\n",
        "- `spark.memory.storageFraction`: Amount of execution memory that can be claimed by storage (default 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Task Slots and Parallelism\n",
        "\n",
        "Spark divides work into **tasks** that are executed in parallel across executor cores. Let's understand how parallelism works in Spark:\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "- **Task**: The smallest unit of execution in Spark\n",
        "- **Task Slot**: A place where a task can execute (basically a CPU core in an executor)\n",
        "- **Partition**: A chunk of data processed by a single task\n",
        "\n",
        "### Determining Parallelism:\n",
        "\n",
        "The total number of parallel tasks that can run is determined by:\n",
        "```\n",
        "Total Task Slots = Number of Executors Ã— Cores per Executor\n",
        "```\n",
        "\n",
        "Let's demonstrate how parallelism affects execution by creating datasets with different numbers of partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a large-ish dataset\n",
        "def create_test_df(num_partitions):\n",
        "    # Create data with 1 million rows\n",
        "    data = spark.range(0, 1000000, 1, num_partitions)\n",
        "    return data\n",
        "\n",
        "# Function to benchmark operations with different partitioning\n",
        "def benchmark_partitions(partition_counts):\n",
        "    results = []\n",
        "    \n",
        "    for partitions in partition_counts:\n",
        "        # Create dataset with specified partitions\n",
        "        df = create_test_df(partitions)\n",
        "        \n",
        "        # Force a shuffle operation\n",
        "        start_time = time.time()\n",
        "        count = df.repartition(partitions).groupBy(df.id % 100).count().collect()\n",
        "        duration = time.time() - start_time\n",
        "        \n",
        "        results.append((partitions, duration))\n",
        "        print(f\"Partitions: {partitions}, Duration: {duration:.2f} seconds\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test with different partition counts\n",
        "partition_counts = [1, 2, 4, 8, 16]\n",
        "benchmark_results = benchmark_partitions(partition_counts)\n",
        "\n",
        "# Plot results\n",
        "partitions, durations = zip(*benchmark_results)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(partitions)), durations, tick_label=partitions)\n",
        "plt.xlabel('Number of Partitions')\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.title('Effect of Partitioning on Execution Time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Results:\n",
        "\n",
        "- With **too few partitions**, we don't fully utilize all available cores\n",
        "- With **too many partitions**, we introduce overhead from task scheduling and management\n",
        "- The **optimal number** is typically a small multiple of your total available cores\n",
        "\n",
        "For maximum parallelism, you need at least as many partitions as you have task slots (cores across executors). However, having 2-3x more partitions than cores often yields better performance due to better work distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Lazy Execution and DAG Optimization\n",
        "\n",
        "One of Spark's most powerful features is its lazy execution model. When you define transformations on DataFrames or RDDs, Spark doesn't execute them immediately. Instead, it builds a **Directed Acyclic Graph (DAG)** of operations that will only be executed when an action is triggered.\n",
        "\n",
        "### Transformations vs. Actions\n",
        "\n",
        "- **Transformations**: Operations that create a new DataFrame/RDD without executing computation (e.g., `select()`, `filter()`, `map()`)\n",
        "- **Actions**: Operations that trigger computation and return results (e.g., `count()`, `collect()`, `save()`)\n",
        "\n",
        "Let's demonstrate this concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple DataFrame\n",
        "df = spark.range(0, 1000000)\n",
        "\n",
        "# Define a sequence of transformations\n",
        "# Note: No computation happens at this point\n",
        "df_transformed = df \\\n",
        "    .filter(col(\"id\") % 2 == 0) \\\n",
        "    .select(col(\"id\"), col(\"id\") * 2) \\\n",
        "    .withColumnRenamed(\"(id * 2)\", \"doubled\")\n",
        "\n",
        "print(\"DataFrame transformations defined, but not yet executed.\")\n",
        "print(f\"Input DataFrame: {df}\")\n",
        "print(f\"Transformed DataFrame: {df_transformed}\")\n",
        "\n",
        "# Let's look at the execution plan before execution\n",
        "print(\"\\nPhysical plan (this shows what WILL happen, but hasn't happened yet):\")\n",
        "df_transformed.explain()\n",
        "\n",
        "# Now trigger an action\n",
        "start_time = time.time()\n",
        "result_count = df_transformed.count()  # This triggers execution\n",
        "duration = time.time() - start_time\n",
        "\n",
        "print(f\"\\nAction executed in {duration:.2f} seconds\")\n",
        "print(f\"Result count: {result_count}\")\n",
        "\n",
        "# Trigger another action - notice it will be computed again\n",
        "start_time = time.time()\n",
        "first_rows = df_transformed.limit(5).collect()  # Another action\n",
        "duration = time.time() - start_time\n",
        "\n",
        "print(f\"\\nSecond action executed in {duration:.2f} seconds\")\n",
        "print(\"First 5 rows:\")\n",
        "for row in first_rows:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DAG Optimization\n",
        "\n",
        "Spark's **Catalyst Optimizer** analyzes your logical plan and converts it to an optimized physical execution plan. Key optimizations include:\n",
        "\n",
        "1. **Predicate Pushdown**: Filters are pushed down to the data source level\n",
        "2. **Column Pruning**: Only required columns are read\n",
        "3. **Operation Combining**: Multiple operations are combined when possible\n",
        "4. **Constant Folding**: Expressions with constants are pre-computed\n",
        "\n",
        "Let's see a more complex example with the optimization plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a couple of DataFrames\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"John\", \"Doe\", 100),\n",
        "    (2, \"Jane\", \"Smith\", 200),\n",
        "    (3, \"Bob\", \"Jones\", 300),\n",
        "    (4, \"Alice\", \"Johnson\", 200),\n",
        "    (5, \"Charlie\", \"Brown\", 100),\n",
        "    (6, \"Eve\", \"Davis\", 400)\n",
        "], [\"id\", \"first_name\", \"last_name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (100, \"HR\", \"New York\"),\n",
        "    (200, \"Engineering\", \"San Francisco\"),\n",
        "    (300, \"Marketing\", \"Chicago\"),\n",
        "    (400, \"Sales\", \"Boston\")\n",
        "], [\"dept_id\", \"dept_name\", \"location\"])\n",
        "\n",
        "# Define a complex transformation\n",
        "result = employees \\\n",
        "    .join(departments, \"dept_id\") \\\n",
        "    .filter(col(\"location\").isin(\"San Francisco\", \"New York\")) \\\n",
        "    .filter(col(\"id\") > 1) \\\n",
        "    .select(\"id\", \"first_name\", \"last_name\", \"dept_name\")\n",
        "\n",
        "# Let's look at the logical and physical plans\n",
        "print(\"Logical Plan:\")\n",
        "result.explain(True)\n",
        "\n",
        "# Execute\n",
        "print(\"\\nResult:\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Points About Lazy Execution:\n",
        "\n",
        "1. Transformations are lazily evaluated; actions trigger execution\n",
        "2. This allows Spark to optimize the entire DAG before execution\n",
        "3. Each action triggers a new job, recomputing the transformations unless data is cached\n",
        "4. The query optimizer rewrites the logical plan to a more efficient execution plan\n",
        "\n",
        "### Stages and Tasks\n",
        "\n",
        "When you trigger an action, Spark:\n",
        "1. Converts the logical plan to a physical plan\n",
        "2. Breaks the physical plan into stages (separated by shuffle operations)\n",
        "3. Divides each stage into tasks based on data partitioning\n",
        "\n",
        "```\n",
        "Action â†’ Job â†’ Stages â†’ Tasks\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching and Persistence\n",
        "\n",
        "To avoid recomputation of the same data across multiple actions, you can cache or persist DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame with multiple transformations\n",
        "df = spark.range(0, 1000000)\n",
        "expensive_df = df \\\n",
        "    .filter(col(\"id\") % 2 == 0) \\\n",
        "    .select(col(\"id\"), col(\"id\") * 10) \\\n",
        "    .withColumnRenamed(\"(id * 10)\", \"multiplied\")\n",
        "\n",
        "# First action without caching\n",
        "start_time = time.time()\n",
        "count1 = expensive_df.count()\n",
        "duration1 = time.time() - start_time\n",
        "print(f\"First count (without cache): {count1}, Duration: {duration1:.2f} seconds\")\n",
        "\n",
        "# Another action without caching - note the similar execution time (recomputation)\n",
        "start_time = time.time()\n",
        "count2 = expensive_df.count()\n",
        "duration2 = time.time() - start_time\n",
        "print(f\"Second count (without cache): {count2}, Duration: {duration2:.2f} seconds\")\n",
        "\n",
        "# Now cache the DataFrame\n",
        "expensive_df.cache()\n",
        "\n",
        "# First count with caching - similar time as before (data is cached during this action)\n",
        "start_time = time.time()\n",
        "count3 = expensive_df.count()\n",
        "duration3 = time.time() - start_time\n",
        "print(f\"First count after cache call: {count3}, Duration: {duration3:.2f} seconds\")\n",
        "\n",
        "# Second count with caching - should be much faster\n",
        "start_time = time.time()\n",
        "count4 = expensive_df.count()\n",
        "duration4 = time.time() - start_time\n",
        "print(f\"Second count with cache: {count4}, Duration: {duration4:.2f} seconds\")\n",
        "\n",
        "# Let's see what's cached\n",
        "print(\"\\nCached DataFrames:\")\n",
        "spark.catalog.listTables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Memory Management\n",
        "\n",
        "Spark manages memory in two key areas:\n",
        "\n",
        "1. **Execution Memory**: Used for computations like joins, sorts, and aggregations\n",
        "2. **Storage Memory**: Used for caching and propagating internal data across the cluster\n",
        "\n",
        "These memory pools are unified with a soft boundary, allowing either side to borrow from the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine current memory usage and configuration\n",
        "print(\"Memory Configuration:\")\n",
        "memory_configs = [\n",
        "    \"spark.memory.fraction\",\n",
        "    \"spark.memory.storageFraction\",\n",
        "    \"spark.memory.offHeap.enabled\",\n",
        "    \"spark.memory.offHeap.size\"\n",
        "]\n",
        "\n",
        "for config in memory_configs:\n",
        "    value = spark.conf.get(config, \"Not explicitly set\")\n",
        "    print(f\"{config}: {value}\")\n",
        "\n",
        "# Storage levels for caching\n",
        "print(\"\\nAvailable Storage Levels:\")\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# Use only storage levels that are definitely available\n",
        "storage_levels = [\n",
        "    (\"MEMORY_ONLY\", StorageLevel.MEMORY_ONLY),\n",
        "    (\"MEMORY_AND_DISK\", StorageLevel.MEMORY_AND_DISK),\n",
        "    (\"DISK_ONLY\", StorageLevel.DISK_ONLY)\n",
        "]\n",
        "\n",
        "# Dynamically add other storage levels if they exist\n",
        "# This avoids AttributeError if certain levels aren't available in this version\n",
        "if hasattr(StorageLevel, \"MEMORY_ONLY_SER\"):\n",
        "    storage_levels.append((\"MEMORY_ONLY_SER\", StorageLevel.MEMORY_ONLY_SER))\n",
        "if hasattr(StorageLevel, \"MEMORY_AND_DISK_SER\"):\n",
        "    storage_levels.append((\"MEMORY_AND_DISK_SER\", StorageLevel.MEMORY_AND_DISK_SER))\n",
        "if hasattr(StorageLevel, \"OFF_HEAP\"):\n",
        "    storage_levels.append((\"OFF_HEAP\", StorageLevel.OFF_HEAP))\n",
        "\n",
        "for name, level in storage_levels:\n",
        "    print(f\"{name}: {level}\")\n",
        "\n",
        "# Print all available StorageLevel attributes for reference\n",
        "print(\"\\nAll available StorageLevel attributes:\")\n",
        "for attr in dir(StorageLevel):\n",
        "    if attr.isupper() and not attr.startswith('_'):\n",
        "        print(attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Different Persistence Levels\n",
        "\n",
        "Spark offers multiple storage levels for caching data, each with different trade-offs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a moderate-sized DataFrame to test storage levels\n",
        "test_df = spark.range(0, 1000000).selectExpr(\"id\", \"cast(id as string) as id_str\")\n",
        "\n",
        "# Test different persistence levels\n",
        "def test_persistence_level(df, storage_level, name):\n",
        "    # Unpersist if already cached\n",
        "    df.unpersist()\n",
        "    \n",
        "    # Persist with specified level\n",
        "    df.persist(storage_level)\n",
        "    \n",
        "    # Force caching with an action\n",
        "    start_time = time.time()\n",
        "    count = df.count()\n",
        "    first_duration = time.time() - start_time\n",
        "    \n",
        "    # Second action should be faster if cached properly\n",
        "    start_time = time.time()\n",
        "    df.count()\n",
        "    second_duration = time.time() - start_time\n",
        "    \n",
        "    # Report results\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  First action (caching): {first_duration:.2f} seconds\")\n",
        "    print(f\"  Second action (cached): {second_duration:.2f} seconds\")\n",
        "    print(f\"  Speed improvement: {(first_duration/second_duration):.1f}x\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Test a few storage levels\n",
        "print(\"Testing different persistence levels...\\n\")\n",
        "test_persistence_level(test_df, StorageLevel.MEMORY_ONLY, \"MEMORY_ONLY\")\n",
        "test_persistence_level(test_df, StorageLevel.MEMORY_AND_DISK, \"MEMORY_AND_DISK\")\n",
        "test_persistence_level(test_df, StorageLevel.DISK_ONLY, \"DISK_ONLY\")\n",
        "\n",
        "# Clean up\n",
        "test_df.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Management Best Practices\n",
        "\n",
        "1. **Choose the right storage level**:\n",
        "   - `MEMORY_ONLY`: Fastest but uses the most memory\n",
        "   - `MEMORY_ONLY_SER`: Better space efficiency with some CPU cost\n",
        "   - `MEMORY_AND_DISK`: Good compromise when memory is limited\n",
        "   - `DISK_ONLY`: Lowest memory impact but slower\n",
        "\n",
        "2. **Only cache when needed**:\n",
        "   - Cache DataFrames used multiple times\n",
        "   - Cache after expensive operations\n",
        "   - Unpersist when no longer needed\n",
        "\n",
        "3. **Monitor memory usage** with the Spark UI\n",
        "\n",
        "4. **Tune the fraction of memory used for storage vs. execution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Shuffling and Partitioning\n",
        "\n",
        "**Shuffling** is the process of redistributing data across partitions, often moving data between executors and machines. It's one of the most expensive operations in Spark.\n",
        "\n",
        "Operations that trigger shuffle include:\n",
        "- `repartition()`, `coalesce()`\n",
        "- `groupByKey()`, `reduceByKey()`\n",
        "- `join()`, `cogroup()`\n",
        "- `sortByKey()`, `orderBy()`\n",
        "\n",
        "Let's visualize the impact of shuffling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets for shuffle demonstration\n",
        "df1 = spark.range(0, 100000, 1, 4)  # 4 partitions\n",
        "df2 = spark.range(0, 100000, 1, 8)  # 8 partitions\n",
        "\n",
        "# Without shuffle (just a simple transformation)\n",
        "start_time = time.time()\n",
        "df1.select(col(\"id\") * 2).count()\n",
        "no_shuffle_time = time.time() - start_time\n",
        "print(f\"Operation without shuffle: {no_shuffle_time:.2f} seconds\")\n",
        "\n",
        "# With shuffle (repartitioning)\n",
        "start_time = time.time()\n",
        "df1.repartition(10).count()\n",
        "repartition_time = time.time() - start_time\n",
        "print(f\"Repartition operation: {repartition_time:.2f} seconds\")\n",
        "\n",
        "# With shuffle (join operation)\n",
        "start_time = time.time()\n",
        "df1.join(df2, \"id\").count()\n",
        "join_time = time.time() - start_time\n",
        "print(f\"Join operation: {join_time:.2f} seconds\")\n",
        "\n",
        "# With shuffle (groupBy operation)\n",
        "start_time = time.time()\n",
        "df1.groupBy(col(\"id\") % 100).count().count()\n",
        "group_time = time.time() - start_time\n",
        "print(f\"GroupBy operation: {group_time:.2f} seconds\")\n",
        "\n",
        "# Plot results\n",
        "operations = [\"No Shuffle\", \"Repartition\", \"Join\", \"GroupBy\"]\n",
        "times = [no_shuffle_time, repartition_time, join_time, group_time]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(operations, times)\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.title('Impact of Shuffle Operations on Performance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Shuffle Process\n",
        "\n",
        "During a shuffle:\n",
        "\n",
        "1. **Map phase**: Tasks in the \"map\" stage write shuffle data to local disk in shuffle files\n",
        "2. **Shuffle service**: Handles transfer of shuffle data between executors\n",
        "3. **Reduce phase**: Tasks in the \"reduce\" stage read shuffle data from multiple map outputs\n",
        "\n",
        "```\n",
        "Executor 1          Executor 2          Executor 3\n",
        "+--------+          +--------+          +--------+\n",
        "| Map 1  |          | Map 2  |          | Map 3  |\n",
        "+--------+          +--------+          +--------+\n",
        "     |                   |                   |\n",
        "     v                   v                   v\n",
        "+--------------------------SHUFFLE-------------------------+\n",
        "     |                   |                   |\n",
        "     v                   v                   v\n",
        "+--------+          +--------+          +--------+\n",
        "| Reduce1|          |Reduce2 |          |Reduce3 |\n",
        "+--------+          +--------+          +--------+\n",
        "```\n",
        "\n",
        "### Partitioning Strategies\n",
        "\n",
        "Efficient partitioning reduces shuffle overhead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate partitioning strategies\n",
        "# Create test data\n",
        "data = [(i, f\"user_{i}\", i % 10) for i in range(1000)]\n",
        "user_df = spark.createDataFrame(data, [\"id\", \"name\", \"category\"])\n",
        "\n",
        "# Check default partitioning\n",
        "print(f\"Default partitioning: {user_df.rdd.getNumPartitions()} partitions\")\n",
        "\n",
        "# Repartition by key (for better join performance later)\n",
        "partitioned_df = user_df.repartition(8, \"category\")\n",
        "print(f\"After repartitioning: {partitioned_df.rdd.getNumPartitions()} partitions\")\n",
        "\n",
        "# Examine distribution of data across partitions\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "partition_counts = partitioned_df.groupBy(spark_partition_id()).count()\n",
        "print(\"\\nDistribution of records across partitions:\")\n",
        "partition_counts.show()\n",
        "\n",
        "# Demonstrate partitioning benefit for joins\n",
        "# Create a small lookup table that shares the same key\n",
        "category_lookup = spark.createDataFrame([(i, f\"Category {i}\") for i in range(10)], [\"category\", \"description\"])\n",
        "\n",
        "# Join with partitioned data\n",
        "start_time = time.time()\n",
        "join_result = partitioned_df.join(category_lookup, \"category\")\n",
        "join_result.count()\n",
        "partitioned_join_time = time.time() - start_time\n",
        "print(f\"Join with partitioned data: {partitioned_join_time:.2f} seconds\")\n",
        "\n",
        "# Join with unpartitioned data\n",
        "start_time = time.time()\n",
        "join_result = user_df.join(category_lookup, \"category\")\n",
        "join_result.count()\n",
        "unpartitioned_join_time = time.time() - start_time\n",
        "print(f\"Join with unpartitioned data: {unpartitioned_join_time:.2f} seconds\")\n",
        "\n",
        "print(f\"Improvement factor: {unpartitioned_join_time/partitioned_join_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Minimizing Shuffle Impact\n",
        "\n",
        "Strategies to reduce shuffle overhead:\n",
        "\n",
        "1. **Filter early**: Reduce data size before shuffling\n",
        "2. **Broadcast joins**: Use broadcast joins for small tables\n",
        "3. **Partition by join key**: Pre-partition data by frequently used join keys\n",
        "4. **Control partition count**: `spark.sql.shuffle.partitions` (default 200)\n",
        "5. **Use coalesce() instead of repartition()** when reducing partitions\n",
        "\n",
        "Let's demonstrate a broadcast join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Normal join\n",
        "start_time = time.time()\n",
        "normal_join = user_df.join(category_lookup, \"category\")\n",
        "normal_join.count()\n",
        "normal_join_time = time.time() - start_time\n",
        "print(f\"Normal join: {normal_join_time:.2f} seconds\")\n",
        "\n",
        "# With broadcast hint\n",
        "start_time = time.time()\n",
        "broadcast_join = user_df.join(broadcast(category_lookup), \"category\")\n",
        "broadcast_join.count()\n",
        "broadcast_join_time = time.time() - start_time\n",
        "print(f\"Broadcast join: {broadcast_join_time:.2f} seconds\")\n",
        "\n",
        "print(f\"Improvement factor: {normal_join_time/broadcast_join_time:.2f}x\")\n",
        "\n",
        "# Look at execution plans\n",
        "print(\"\\nNormal Join Plan:\")\n",
        "normal_join.explain()\n",
        "\n",
        "print(\"\\nBroadcast Join Plan:\")\n",
        "broadcast_join.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Tuning\n",
        "\n",
        "Optimizing Spark performance involves tuning multiple aspects:\n",
        "\n",
        "### 1. Data Serialization\n",
        "\n",
        "Spark uses serialization for data transfer and storage. Options include:\n",
        "- **Java Serialization**: Default but slower\n",
        "- **Kryo Serialization**: Faster and more compact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check current serializer\n",
        "serializer_conf = spark.conf.get(\"spark.serializer\", \"Not explicitly set\")\n",
        "print(f\"Current serializer: {serializer_conf}\")\n",
        "print(\"To use Kryo serialization, configure:\")\n",
        "print(\"spark.serializer: org.apache.spark.serializer.KryoSerializer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Memory Tuning\n",
        "\n",
        "Memory-related configurations to consider:\n",
        "\n",
        "- **spark.memory.fraction**: Fraction of heap used for execution and storage (default 0.6)\n",
        "- **spark.memory.storageFraction**: Amount of execution memory that can be claimed by storage (default 0.5)\n",
        "- **spark.shuffle.file.buffer**: Size of in-memory buffer for shuffle outputs (default 32k)\n",
        "- **spark.executor.memoryOverhead**: Additional non-heap memory per executor (default is max(384M, 0.1 * spark.executor.memory))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Execution Tuning\n",
        "\n",
        "Execution-related configurations:\n",
        "\n",
        "- **spark.default.parallelism**: Default number of partitions in RDDs\n",
        "- **spark.sql.shuffle.partitions**: Number of partitions for shuffles in SQL queries (default 200)\n",
        "- **spark.sql.autoBroadcastJoinThreshold**: Maximum size for broadcasts in joins (default 10M)\n",
        "- **spark.sql.adaptive.enabled**: Enable adaptive query execution (default true in Spark 3.x)\n",
        "\n",
        "Let's check some of these settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check execution-related configurations\n",
        "execution_configs = [\n",
        "    \"spark.default.parallelism\",\n",
        "    \"spark.sql.shuffle.partitions\",\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
        "    \"spark.sql.adaptive.enabled\"\n",
        "]\n",
        "\n",
        "print(\"Execution Configuration:\")\n",
        "for config in execution_configs:\n",
        "    try:\n",
        "        value = spark.conf.get(config)\n",
        "    except Exception:\n",
        "        value = \"Not explicitly set\"\n",
        "    print(f\"{config}: {value}\")\n",
        "    \n",
        "# Alternative approach - showing all configuration values containing 'broadcast'\n",
        "print(\"\\nBroadcast-related configurations:\")\n",
        "broadcast_configs = [item for item in sorted(spark.sparkContext.getConf().getAll()) \n",
        "                    if 'broadcast' in item[0].lower()]\n",
        "for name, value in broadcast_configs:\n",
        "    print(f\"{name}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Performance Monitoring\n",
        "\n",
        "Tools for monitoring Spark performance:\n",
        "\n",
        "1. **Spark UI**: Web interface showing job details (typically on port 4040)\n",
        "2. **spark.eventLog.enabled**: Enable event logging\n",
        "3. **Spark History Server**: For viewing historical application data\n",
        "\n",
        "### 5. Key Performance Best Practices\n",
        "\n",
        "Here's a summary of key performance optimization strategies:\n",
        "\n",
        "1. **Optimize Data Input/Output**\n",
        "   - Use columnar formats like Parquet\n",
        "   - Filter early in your pipeline\n",
        "   - Partition your data sources by commonly filtered columns\n",
        "\n",
        "2. **Minimize Shuffling**\n",
        "   - Use broadcast joins for small tables\n",
        "   - Repartition by frequently joined keys\n",
        "   - Try to place transformations requiring the same partitioning together\n",
        "\n",
        "3. **Manage Resources Efficiently**\n",
        "   - Set appropriate executor numbers and sizes\n",
        "   - Tune memory allocation between storage and execution\n",
        "   - Consider the right level of parallelism for your cluster\n",
        "\n",
        "4. **Use Caching Strategically**\n",
        "   - Cache datasets used multiple times\n",
        "   - Choose the appropriate storage level\n",
        "   - Unpersist when no longer needed\n",
        "\n",
        "5. **Optimize Data Structure**\n",
        "   - Use appropriate data types (e.g., avoid strings when numbers would work)\n",
        "   - Consider using Kryo serialization for complex objects\n",
        "   - Prune unnecessary columns early in your pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has covered the internal workings of PySpark, focusing on:\n",
        "\n",
        "1. Spark's distributed architecture with drivers and executors\n",
        "2. How executors and task slots enable parallel processing\n",
        "3. Lazy execution and the optimization of execution plans\n",
        "4. Memory management across storage and execution\n",
        "5. The impact of shuffling and strategies to minimize it\n",
        "6. Approaches to tune Spark for optimal performance\n",
        "\n",
        "Understanding these internals helps you write more efficient Spark code, troubleshoot performance issues, and optimize your data processing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up and stop the SparkSession\n",
        "spark.stop()\n",
        "print(\"SparkSession stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 