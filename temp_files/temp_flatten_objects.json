{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flattening Complex Objects in PySpark with UDFs\n",
        "\n",
        "This notebook demonstrates how to use User-Defined Functions (UDFs) to flatten complex nested data structures in PySpark. We'll cover three common scenarios:\n",
        "\n",
        "1. Flattening arrays into strings\n",
        "2. Extracting data from nested structures\n",
        "3. Flattening map/dictionary structures\n",
        "\n",
        "Let's first initialize our SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, explode, array_join\n",
        "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, MapType, IntegerType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Flatten Object Examples\").getOrCreate()\n",
        "\n",
        "print(\"SparkSession initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Flattening Arrays\n",
        "\n",
        "Arrays are common data structures in Spark DataFrames. Sometimes you need to convert an array into a single string (e.g., for reporting, exporting to CSV, etc.).\n",
        "\n",
        "Let's create a DataFrame with an array column and flatten it using a UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame with array column\n",
        "array_data = [\n",
        "    (1, [\"apple\", \"banana\", \"cherry\"]),\n",
        "    (2, [\"orange\", \"grape\"]),\n",
        "    (3, []),\n",
        "    (4, None)  # Handle null case\n",
        "]\n",
        "array_df = spark.createDataFrame(array_data, [\"id\", \"fruits\"])\n",
        "\n",
        "print(\"Original DataFrame with Array:\")\n",
        "array_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UDF Approach to Flatten Array\n",
        "\n",
        "Let's define a UDF that converts an array to a comma-separated string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define UDF to flatten array to comma-separated string\n",
        "@udf(StringType())\n",
        "def flatten_array(arr):\n",
        "    if arr is None:\n",
        "        return None\n",
        "    return \", \".join(arr)\n",
        "\n",
        "# Apply the UDF\n",
        "flattened_array_df = array_df.withColumn(\"flattened_fruits\", flatten_array(col(\"fruits\")))\n",
        "\n",
        "print(\"DataFrame with Flattened Array:\")\n",
        "flattened_array_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Built-in Function Alternative\n",
        "\n",
        "While UDFs work well, Spark provides a built-in function `array_join()` that can be more efficient for this particular task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using built-in function (more efficient)\n",
        "array_df_built_in = array_df.withColumn(\n",
        "    \"flattened_fruits_built_in\", \n",
        "    array_join(col(\"fruits\"), \", \")\n",
        ")\n",
        "\n",
        "print(\"Using Built-in array_join Function:\")\n",
        "array_df_built_in.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Flattening Nested Structures\n",
        "\n",
        "Nested structures (structs) are common in semi-structured data like JSON. Let's see how to extract and flatten data from deeply nested fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema with nested structure\n",
        "nested_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"person\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"address\", StructType([\n",
        "            StructField(\"city\", StringType(), True),\n",
        "            StructField(\"zip\", StringType(), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Create data with nested structure\n",
        "nested_data = [\n",
        "    (1, {\"name\": \"John\", \"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
        "    (2, {\"name\": \"Alice\", \"address\": {\"city\": \"San Francisco\", \"zip\": \"94105\"}}),\n",
        "    (3, {\"name\": \"Bob\", \"address\": None}),\n",
        "    (4, None)  # Handle completely null record\n",
        "]\n",
        "nested_df = spark.createDataFrame(nested_data, nested_schema)\n",
        "\n",
        "print(\"Original DataFrame with Nested Structure:\")\n",
        "nested_df.printSchema()\n",
        "nested_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UDF Approach for Nested Structures\n",
        "\n",
        "Let's create a UDF that combines fields from a nested structure into a single string, with appropriate null handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define UDF to flatten the nested structure\n",
        "@udf(StringType())\n",
        "def flatten_address(person):\n",
        "    if person is None:\n",
        "        return \"No person data\"\n",
        "    if person[\"address\"] is None:\n",
        "        return f\"{person['name']} - No address\"\n",
        "    \n",
        "    address = person[\"address\"]\n",
        "    return f\"{person['name']} lives in {address['city']}, {address['zip']}\"\n",
        "\n",
        "# Apply the UDF\n",
        "flattened_nested_df = nested_df.withColumn(\n",
        "    \"person_summary\", \n",
        "    flatten_address(col(\"person\"))\n",
        ")\n",
        "\n",
        "print(\"DataFrame with Flattened Nested Structure:\")\n",
        "flattened_nested_df.select(\"id\", \"person_summary\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Direct Column Reference Alternative\n",
        "\n",
        "For accessing specific nested fields directly, Spark allows dot notation which can be more efficient than UDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extracting fields with direct column references\n",
        "direct_access_df = nested_df.select(\n",
        "    \"id\",\n",
        "    col(\"person.name\").alias(\"name\"),\n",
        "    col(\"person.address.city\").alias(\"city\"),\n",
        "    col(\"person.address.zip\").alias(\"zip_code\")\n",
        ")\n",
        "\n",
        "print(\"Using Direct Column References:\")\n",
        "direct_access_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Flattening Maps/Dictionaries\n",
        "\n",
        "Maps (key-value pairs) are often used for sparse data or attribute collections. Let's see how to extract and format map data using UDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame with a map column\n",
        "map_data = [\n",
        "    (1, {\"height\": 180, \"weight\": 75, \"age\": 30}),\n",
        "    (2, {\"height\": 165, \"weight\": 65}),  # Missing age key\n",
        "    (3, {}),  # Empty map\n",
        "    (4, None)  # Null map\n",
        "]\n",
        "map_df = spark.createDataFrame(map_data, [\"id\", \"metrics\"])\n",
        "\n",
        "print(\"Original DataFrame with Map:\")\n",
        "map_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UDF Approach for Maps\n",
        "\n",
        "Let's create a UDF that extracts specific values from the map and formats them with default handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define UDF to extract specific values with default handling\n",
        "@udf(StringType())\n",
        "def flatten_metrics(metrics):\n",
        "    if metrics is None:\n",
        "        return \"No metrics data\"\n",
        "    if len(metrics) == 0:\n",
        "        return \"Empty metrics\"\n",
        "    \n",
        "    # Extract values with defaults\n",
        "    height = metrics.get(\"height\", \"unknown\")\n",
        "    weight = metrics.get(\"weight\", \"unknown\")\n",
        "    age = metrics.get(\"age\", \"unknown\")\n",
        "    \n",
        "    return f\"Height: {height}cm, Weight: {weight}kg, Age: {age}y\"\n",
        "\n",
        "# Apply the UDF\n",
        "flattened_map_df = map_df.withColumn(\"metrics_summary\", flatten_metrics(col(\"metrics\")))\n",
        "\n",
        "print(\"DataFrame with Flattened Map:\")\n",
        "flattened_map_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explode Alternative\n",
        "\n",
        "If you need to extract all keys and values, you can use the `explode` function to convert the map to rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, map_keys, map_values\n",
        "\n",
        "# Filter out nulls and empty maps for the explode\n",
        "map_df_filtered = map_df.filter(col(\"metrics\").isNotNull() & (map_keys(col(\"metrics\")).size() > 0))\n",
        "\n",
        "# Explode the map into rows\n",
        "exploded_map_df = map_df_filtered.select(\n",
        "    \"id\",\n",
        "    explode(col(\"metrics\")).alias(\"metric_name\", \"metric_value\")\n",
        ")\n",
        "\n",
        "print(\"Using explode to Convert Map to Rows:\")\n",
        "exploded_map_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Complex Example: Combining Multiple Complex Objects\n",
        "\n",
        "In real-world scenarios, you often need to handle multiple complex types together. Let's create a more complex example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a complex schema\n",
        "complex_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"user\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"skills\", ArrayType(StringType()), True),\n",
        "        StructField(\"properties\", MapType(StringType(), StringType()), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Create complex data\n",
        "complex_data = [\n",
        "    (1, {\n",
        "        \"name\": \"John\", \n",
        "        \"skills\": [\"Python\", \"SQL\", \"Spark\"], \n",
        "        \"properties\": {\"dept\": \"Data Science\", \"level\": \"Senior\"}\n",
        "    }),\n",
        "    (2, {\n",
        "        \"name\": \"Mary\", \n",
        "        \"skills\": [\"Java\", \"Scala\"], \n",
        "        \"properties\": {\"dept\": \"Engineering\", \"location\": \"Remote\"}\n",
        "    }),\n",
        "    (3, {\n",
        "        \"name\": \"Bob\", \n",
        "        \"skills\": [], \n",
        "        \"properties\": {}\n",
        "    })\n",
        "]\n",
        "complex_df = spark.createDataFrame(complex_data, complex_schema)\n",
        "\n",
        "print(\"Complex DataFrame:\")\n",
        "complex_df.printSchema()\n",
        "complex_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive UDF for Complex Flattening\n",
        "\n",
        "Let's create a UDF that processes the entire complex structure to create a comprehensive profile string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive flattening UDF\n",
        "@udf(StringType())\n",
        "def create_user_profile(user):\n",
        "    if user is None:\n",
        "        return \"No user data\"\n",
        "    \n",
        "    # Extract basic info\n",
        "    name = user[\"name\"]\n",
        "    \n",
        "    # Process skills (array)\n",
        "    skills = user[\"skills\"]\n",
        "    if skills and len(skills) > 0:\n",
        "        skills_str = \", \".join(skills)\n",
        "    else:\n",
        "        skills_str = \"No skills listed\"\n",
        "    \n",
        "    # Process properties (map)\n",
        "    props = user[\"properties\"]\n",
        "    if props and len(props) > 0:\n",
        "        # Create a formatted string from the map\n",
        "        props_str = \", \".join([f\"{k}: {v}\" for k, v in props.items()])\n",
        "    else:\n",
        "        props_str = \"No properties listed\"\n",
        "    \n",
        "    # Combine everything into a profile\n",
        "    return f\"User: {name} | Skills: {skills_str} | Properties: {props_str}\"\n",
        "\n",
        "# Apply the UDF\n",
        "profile_df = complex_df.withColumn(\"user_profile\", create_user_profile(col(\"user\")))\n",
        "\n",
        "print(\"DataFrame with Comprehensive User Profile:\")\n",
        "profile_df.select(\"id\", \"user_profile\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Best Practices for Flattening Complex Objects\n",
        "\n",
        "1. **Choose the Right Approach:**\n",
        "   - Use built-in functions when possible (like `array_join`, dot notation, `explode`)\n",
        "   - Use UDFs when you need custom logic or handling multiple nested levels together\n",
        "\n",
        "2. **Always Handle Nulls and Edge Cases:**\n",
        "   - Check for `None` values at each level of nesting\n",
        "   - Provide meaningful defaults or error messages\n",
        "   - Handle empty collections (empty arrays, maps)\n",
        "\n",
        "3. **Consider Performance:**\n",
        "   - UDFs have serialization overhead - avoid for simple operations\n",
        "   - For large-scale data, consider restructuring data model if possible\n",
        "   - For better performance with UDFs, consider Pandas UDFs (vectorized UDFs)\n",
        "\n",
        "4. **Documentation:**\n",
        "   - Document complex UDFs clearly with examples\n",
        "   - Specify return types explicitly"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 