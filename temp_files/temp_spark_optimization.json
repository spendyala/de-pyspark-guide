{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Plan Analysis and Query Optimization\n",
    "\n",
    "This notebook provides a comprehensive guide to analyzing Spark's logical and physical execution plans and using those insights to optimize your Spark SQL queries and DataFrame operations.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Spark Execution Plans\n",
    "2. Setting Up Our Environment\n",
    "3. Key Optimization Areas in Spark Plans\n",
    "   - Scan Operations and Table Statistics\n",
    "   - Filter Pushdown\n",
    "   - Join Strategies\n",
    "   - Partition Pruning\n",
    "   - Shuffle Operations\n",
    "   - Data Skew\n",
    "   - Whole-Stage Codegen\n",
    "   - Caching\n",
    "4. End-to-End Optimization Example\n",
    "5. Best Practices\n",
    "\n",
    "Let's get started by understanding what Spark execution plans are and why they matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Spark Execution Plans\n",
    "\n",
    "Spark uses a query optimizer called Catalyst to transform your high-level DataFrame operations or SQL queries into an efficient execution plan. This process involves several phases:\n",
    "\n",
    "1. **Unresolved Logical Plan**: Initial representation of your query\n",
    "2. **Resolved Logical Plan**: Column and table references are resolved\n",
    "3. **Optimized Logical Plan**: Catalyst applies rule-based optimizations\n",
    "4. **Physical Plan**: Converts logical plan to actual execution strategy\n",
    "5. **Selected Physical Plan**: The most efficient execution plan is chosen\n",
    "6. **Executed Plan**: The final plan after adaptations during runtime\n",
    "\n",
    "Understanding these plans is key to optimizing Spark performance. The `explain()` method is our primary tool for examining these plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Environment\n",
    "\n",
    "Let's start by creating a Spark session and some sample data for our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum, max, min, lit, concat, broadcast, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Plan Analysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample employee data\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"dept_id\", IntegerType(), False),\n",
    "    StructField(\"salary\", DoubleType(), False),\n",
    "    StructField(\"hire_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Department schema\n",
    "department_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"department_name\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"budget\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Generate sample data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate departments\n",
    "dept_data = [\n",
    "    (1, \"Engineering\", \"San Francisco\", 1000000.0),\n",
    "    (2, \"Sales\", \"New York\", 800000.0),\n",
    "    (3, \"Marketing\", \"Chicago\", 600000.0),\n",
    "    (4, \"HR\", \"Seattle\", 400000.0),\n",
    "    (5, \"Finance\", \"Boston\", 750000.0)\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(dept_data, department_schema)\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Generate employees\n",
    "employee_data = []\n",
    "names = [\"John\", \"Emma\", \"Michael\", \"Sophia\", \"James\", \"Olivia\", \"William\", \"Ava\", \"Alexander\", \"Mia\"]\n",
    "surnames = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\", \"Garcia\", \"Rodriguez\", \"Wilson\"]\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "delta = (end_date - start_date).days\n",
    "\n",
    "for i in range(1, 1001):  # Generate 1000 employees\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    dept_id = random.randint(1, 5)\n",
    "    salary = round(random.uniform(50000, 150000), 2)\n",
    "    random_days = random.randint(0, delta)\n",
    "    hire_date = start_date + timedelta(days=random_days)\n",
    "    employee_data.append((i, name, dept_id, salary, hire_date))\n",
    "\n",
    "employees_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Also create a partitioned version of our employees table\n",
    "employees_df.write.mode(\"overwrite\").partitionBy(\"dept_id\").saveAsTable(\"partitioned_employees\")\n",
    "\n",
    "print(f\"Created sample datasets with {employees_df.count()} employees and {departments_df.count()} departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Optimization Areas in Spark Plans\n",
    "\n",
    "Now let's examine the key areas we need to look at in Spark plans for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Scan Operations and Table Statistics\n",
    "\n",
    "Scan operations determine how Spark reads data from sources. Inefficient scans can severely impact performance.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- Full table scans vs. filtered scans\n",
    "- Reading too many columns\n",
    "- Missing statistics\n",
    "- File format efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Analyzing a simple scan operation\n",
    "query1 = employees_df.select(\"*\")\n",
    "print(\"Full table scan with all columns:\")\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: More efficient scan with column pruning\n",
    "query2 = employees_df.select(\"id\", \"name\", \"salary\")\n",
    "print(\"Column pruning in action:\")\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Statistics\n",
    "\n",
    "Statistics help the Spark optimizer make better decisions about join strategies, broadcast size limits, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for our tables\n",
    "spark.sql(\"ANALYZE TABLE employees COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE departments COMPUTE STATISTICS\")\n",
    "\n",
    "# Check table statistics\n",
    "print(\"Table Statistics for employees:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED employees\").filter(col(\"col_name\").like(\"%Statistics%\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Scan Optimization:\n",
    "\n",
    "1. **Select only necessary columns**: Reduces I/O and memory usage\n",
    "2. **Use appropriate file formats**: Parquet/ORC > CSV/JSON for analytical workloads\n",
    "3. **Compute and maintain statistics**: For better query planning\n",
    "4. **Use partitioning**: For large tables to enable partition pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter Pushdown\n",
    "\n",
    "Filter pushdown is the ability to push filter conditions down closer to the data source, reducing the amount of data that needs to be loaded.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `PushedFilters` in the scan operation\n",
    "- Filters applied before or after reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Checking if filters are pushed down\n",
    "query3 = employees_df.filter(col(\"salary\") > 100000)\n",
    "print(\"Filter on in-memory DataFrame:\")\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Filter pushdown to Parquet files\n",
    "parquet_employees = spark.read.table(\"partitioned_employees\")\n",
    "query4 = parquet_employees.filter(col(\"salary\") > 100000)\n",
    "print(\"Filter on Parquet-backed DataFrame:\")\n",
    "query4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Filter Optimization:\n",
    "\n",
    "1. **Apply filters as early as possible**: Put filters before joins or aggregations\n",
    "2. **Use file formats that support predicate pushdown**: Parquet, ORC\n",
    "3. **Filter on partitioned columns**: For partition pruning\n",
    "4. **Use compatible filter expressions**: Some complex expressions can't be pushed down\n",
    "\n",
    "Let's compare the performance impact of filter pushdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: filter pushdown vs. no pushdown\n",
    "def time_execution(df):\n",
    "    start = time.time()\n",
    "    count = df.count()  # Force execution\n",
    "    end = time.time()\n",
    "    return end - start, count\n",
    "\n",
    "# With pushdown (filter then join)\n",
    "start = time.time()\n",
    "result1 = employees_df.filter(col(\"salary\") > 80000) \\\n",
    "    .join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time1, count1 = time_execution(result1)\n",
    "\n",
    "# Without pushdown (join then filter)\n",
    "start = time.time()\n",
    "result2 = employees_df \\\n",
    "    .join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .filter(col(\"salary\") > 80000) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time2, count2 = time_execution(result2)\n",
    "\n",
    "print(f\"Filter then Join: {time1:.4f} seconds, {count1} records\")\n",
    "print(f\"Join then Filter: {time2:.4f} seconds, {count2} records\")\n",
    "print(f\"Performance difference: {(time2/time1):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Join Strategies\n",
    "\n",
    "Spark supports several join strategies, and choosing the right one can significantly impact performance.\n",
    "\n",
    "#### Common Join Types in Spark:\n",
    "\n",
    "1. **Broadcast Hash Join**: Small table is broadcasted to all executors\n",
    "2. **Shuffle Hash Join**: Both tables are shuffled by join key\n",
    "3. **Sort Merge Join**: Both tables are sorted and then merged\n",
    "4. **Broadcast Nested Loop Join**: Used for cross joins and some non-equi joins\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- The join strategy being used\n",
    "- Broadcast hints being applied\n",
    "- Join order optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Letting Spark choose the join strategy\n",
    "auto_join = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Automatic Join Strategy Selection:\")\n",
    "auto_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Forcing a broadcast join\n",
    "broadcast_join = employees_df.join(broadcast(departments_df), employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Forced Broadcast Join:\")\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Impact of Join Strategies\n",
    "\n",
    "Let's measure the performance of different join strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join performance\n",
    "start = time.time()\n",
    "broadcast_result = employees_df.join(broadcast(departments_df), employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time_broadcast, count_broadcast = time_execution(broadcast_result)\n",
    "\n",
    "# Default join strategy performance (likely sort-merge for this data size)\n",
    "start = time.time()\n",
    "default_result = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"]) \\\n",
    "    .select(\"name\", \"department_name\", \"salary\")\n",
    "time_default, count_default = time_execution(default_result)\n",
    "\n",
    "print(f\"Broadcast Join: {time_broadcast:.4f} seconds\")\n",
    "print(f\"Default Strategy: {time_default:.4f} seconds\")\n",
    "print(f\"Performance difference: {(time_default/time_broadcast):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Strategy Selection Guidelines:\n",
    "\n",
    "1. **Broadcast Hash Join**: For small tables that can fit in memory (< 10MB by default)\n",
    "   - Controlled by `spark.sql.autoBroadcastJoinThreshold`\n",
    "   - Good for dimension tables joining with fact tables\n",
    "  \n",
    "2. **Sort Merge Join**: For large tables with well-distributed keys\n",
    "   - Becomes default when tables are too large to broadcast\n",
    "   - Good for large-to-large table joins\n",
    "  \n",
    "3. **Shuffle Hash Join**: For medium-sized tables with skewed data\n",
    "   - Controlled by `spark.sql.join.preferSortMergeJoin`\n",
    "  \n",
    "4. **Broadcast Nested Loop Join**: Last resort, typically for non-equality joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the broadcast threshold\n",
    "original_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Original broadcast threshold: {original_threshold}\")\n",
    "\n",
    "# Set a very low threshold to prevent broadcasting\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "no_broadcast_join = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"\\nJoin strategy with broadcasting disabled:\")\n",
    "no_broadcast_join.explain()\n",
    "\n",
    "# Reset to original value\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", original_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Partition Pruning\n",
    "\n",
    "Partition pruning is the ability of Spark to skip reading partitions that aren't relevant to the query, which can dramatically improve performance for large datasets.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `PartitionFilters` in the scan operation\n",
    "- Reduction in the number of files/partitions read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our partitioned table\n",
    "print(\"Partitioned table structure:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED partitioned_employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Query without partition pruning\n",
    "query_no_pruning = spark.table(\"partitioned_employees\").filter(col(\"salary\") > 100000)\n",
    "print(\"Query without partition pruning:\")\n",
    "query_no_pruning.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Query with partition pruning\n",
    "query_with_pruning = spark.table(\"partitioned_employees\").filter(col(\"dept_id\") == 2)\n",
    "print(\"Query with partition pruning:\")\n",
    "query_with_pruning.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Query with both partition pruning and additional filters\n",
    "query_combined = spark.table(\"partitioned_employees\") \\\n",
    "    .filter((col(\"dept_id\") == 2) & (col(\"salary\") > 100000))\n",
    "print(\"Query with partition pruning and additional filters:\")\n",
    "query_combined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Impact of Partition Pruning\n",
    "\n",
    "Let's measure the performance difference between queries with and without partition pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without partition pruning\n",
    "start = time.time()\n",
    "result_no_pruning = spark.table(\"partitioned_employees\") \\\n",
    "    .filter(col(\"salary\") > 100000) \\\n",
    "    .select(\"id\", \"name\", \"salary\", \"dept_id\")\n",
    "time_no_pruning, count_no_pruning = time_execution(result_no_pruning)\n",
    "\n",
    "# With partition pruning\n",
    "start = time.time()\n",
    "result_with_pruning = spark.table(\"partitioned_employees\") \\\n",
    "    .filter((col(\"dept_id\") == 2) & (col(\"salary\") > 100000)) \\\n",
    "    .select(\"id\", \"name\", \"salary\", \"dept_id\")\n",
    "time_with_pruning, count_with_pruning = time_execution(result_with_pruning)\n",
    "\n",
    "print(f\"Without Partition Pruning: {time_no_pruning:.4f} seconds, {count_no_pruning} records\")\n",
    "print(f\"With Partition Pruning: {time_with_pruning:.4f} seconds, {count_with_pruning} records\")\n",
    "\n",
    "if time_no_pruning > time_with_pruning:\n",
    "    print(f\"Performance improvement: {(time_no_pruning/time_with_pruning):.2f}x faster with pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Partition Optimization:\n",
    "\n",
    "1. **Choose appropriate partition columns**: \n",
    "   - High cardinality but not too high (e.g., date, region, category)\n",
    "   - Commonly used in filters\n",
    "  \n",
    "2. **Avoid over-partitioning**: \n",
    "   - Too many small partitions create small files and overhead\n",
    "   - Aim for partition sizes between 128MB and 1GB\n",
    "  \n",
    "3. **Include partition columns in queries**: \n",
    "   - Ensure queries filter on partition columns when possible\n",
    "  \n",
    "4. **Consider bucketing for join performance**: \n",
    "   - Complement partitioning with bucketing for frequently joined columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Shuffle Operations\n",
    "\n",
    "Shuffles redistribute data across partitions and are often the most expensive operations in Spark. They involve disk I/O, serialization, network transfer, and deserialization.\n",
    "\n",
    "#### What to Look For in the Plan:\n",
    "- `Exchange` operations in the physical plan\n",
    "- The type of exchange (e.g., HashPartitioning, RangePartitioning)\n",
    "- The number of shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Operations that trigger shuffles\n",
    "shuffled_df = employees_df.groupBy(\"dept_id\").agg(count(\"*\").alias(\"emp_count\"), avg(\"salary\").alias(\"avg_salary\"))\n",
    "print(\"GroupBy operation causing shuffle:\")\n",
    "shuffled_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Join operation causing shuffle\n",
    "joined_df = employees_df.join(departments_df, employees_df[\"dept_id\"] == departments_df[\"id\"])\n",
    "print(\"Join operation causing shuffle:\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of the Number of Shuffle Partitions\n",
    "\n",
    "Let's measure the impact of different shuffle partition settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different shuffle partition counts\n",
    "partition_counts = [2, 10, 50, 200]\n",
    "test_results = []\n",
    "\n",
    "for partitions in partition_counts:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", partitions)\n",
    "    \n",
    "    # Run a query that involves shuffling\n",
    "    start = time.time()\n",
    "    result = employees_df.groupBy(\"dept_id\").agg(\n",
    "        count(\"*\").alias(\"emp_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\")\n",
    "    )\n",
    "    execution_time, _ = time_execution(result)\n",
    "    \n",
    "    test_results.append((partitions, execution_time))\n",
    "    print(f\"Shuffle partitions: {partitions}, Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "# Reset to initial value\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Shuffle Optimization:\n",
    "\n",
    "1. **Tune shuffle partitions**: \n",
    "   - Default is 200, which is often too high for small datasets\n",
    "   - Rule of thumb: 2-3 * number of cores for small-medium datasets\n",
    "   - For larger clusters, start with cluster cores * 3-4\n",
    "  \n",
    "2. **Use appropriate partitioning**: \n",
    "   - Pre-partition data by join keys to reduce shuffling\n",
    "   - Consider repartitioning before expensive operations\n",
    "  \n",
    "3. **Minimize the number of stages**: \n",
    "   - Chain transformations that don't require shuffles\n",
    "  \n",
    "4. **Use broadcast joins**: \n",
    "   - When possible, to avoid shuffling larger tables\n",
    "  \n",
    "5. **Consider enabling Adaptive Query Execution**:\n",
    "   - Allows Spark to dynamically coalesce shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Adaptive Query Execution to automatically optimize shuffle partitions\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Run a query with many initial partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
    "adaptive_query = employees_df.groupBy(\"dept_id\").agg(\n",
    "    count(\"*\").alias(\"emp_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ")\n",
    "\n",
    "print(\"Query execution plan with Adaptive Execution enabled:\")\n",
    "adaptive_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Data Skew\n",
    "\n",
    "Data skew occurs when data is unevenly distributed across partitions, causing some tasks to run much longer than others. This can significantly impact performance in both grouping and join operations.\n",
    "\n",
    "#### What to Look For:\n",
    "- In the Spark UI: tasks in a stage taking much longer than others\n",
    "- Uneven partition sizes in the input or after a shuffle\n",
    "- High standard deviation in execution times\n",
    "\n",
    "Let's create a skewed dataset to demonstrate the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a skewed dataset\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Generate skewed data where 80% of records have dept_id = 1\n",
    "skewed_employee_data = []\n",
    "for i in range(1, 10001):  # 10,000 employees\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    # Create skew: 80% in dept_id 1\n",
    "    dept_id = 1 if random.random() < 0.8 else random.randint(2, 5)\n",
    "    salary = round(random.uniform(50000, 150000), 2)\n",
    "    random_days = random.randint(0, delta)\n",
    "    hire_date = start_date + timedelta(days=random_days)\n",
    "    skewed_employee_data.append((i, name, dept_id, salary, hire_date))\n",
    "\n",
    "skewed_employees_df = spark.createDataFrame(skewed_employee_data, employee_schema)\n",
    "skewed_employees_df.createOrReplaceTempView(\"skewed_employees\")\n",
    "\n",
    "# Check the distribution\n",
    "print(\"Distribution of employees across departments:\")\n",
    "skewed_employees_df.groupBy(\"dept_id\").count().orderBy(\"dept_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues Caused by Data Skew\n",
    "\n",
    "Let's observe the performance impact of skewed data in group by and join operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy on skewed column\n",
    "start = time.time()\n",
    "skewed_agg = skewed_employees_df.groupBy(\"dept_id\").agg(\n",
    "    count(\"*\").alias(\"emp_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ")\n",
    "time_skewed_group, _ = time_execution(skewed_agg)\n",
    "\n",
    "# Join on skewed column\n",
    "start = time.time()\n",
    "skewed_join = skewed_employees_df.join(\n",
    "    departments_df,\n",
    "    skewed_employees_df[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "time_skewed_join, _ = time_execution(skewed_join)\n",
    "\n",
    "print(f\"GroupBy on skewed data execution time: {time_skewed_group:.4f} seconds\")\n",
    "print(f\"Join on skewed data execution time: {time_skewed_join:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques to Handle Data Skew\n",
    "\n",
    "1. **Salting**: Add a random number to the skewed key to distribute it\n",
    "2. **Two-phase aggregation**: Local aggregation followed by global aggregation\n",
    "3. **Separate processing**: Handle the skewed values separately\n",
    "\n",
    "Let's implement the salting technique to handle skew in joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1: Salting for skewed joins\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat\n",
    "\n",
    "# Step 1: Identify skewed values\n",
    "skewed_keys = skewed_employees_df.groupBy(\"dept_id\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1000) \\\n",
    "    .select(\"dept_id\") \\\n",
    "    .collect()\n",
    "\n",
    "skewed_keys_list = [row[0] for row in skewed_keys]\n",
    "print(f\"Identified skewed keys: {skewed_keys_list}\")\n",
    "\n",
    "# Step 2: Add salt to skewed keys\n",
    "salt_factor = 10  # Number of salt values to use\n",
    "\n",
    "# Add a salt value to skewed records\n",
    "salted_employees = skewed_employees_df.withColumn(\n",
    "    \"salt\", \n",
    "    when(col(\"dept_id\").isin(skewed_keys_list), \n",
    "         monotonically_increasing_id() % salt_factor)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Create a salted key for joining\n",
    "salted_employees = salted_employees.withColumn(\n",
    "    \"salted_dept_id\", \n",
    "    when(col(\"salt\") > 0, \n",
    "         concat(col(\"dept_id\").cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\")))\n",
    "    .otherwise(col(\"dept_id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Step 3: Explode the department table for skewed keys\n",
    "from pyspark.sql.functions import explode, array, lit\n",
    "\n",
    "# Generate salted keys for the departments table\n",
    "exploded_depts = departments_df.withColumn(\n",
    "    \"is_skewed\", col(\"id\").isin(skewed_keys_list)\n",
    ")\n",
    "\n",
    "# For skewed keys, create multiple copies with salts\n",
    "salted_depts = exploded_depts.withColumn(\n",
    "    \"salt_values\",\n",
    "    when(col(\"is_skewed\"), \n",
    "         array([lit(i) for i in range(salt_factor)]))\n",
    "    .otherwise(array(lit(0)))\n",
    ")\n",
    "\n",
    "# Explode to create multiple rows for skewed keys\n",
    "salted_depts = salted_depts.withColumn(\"salt\", explode(\"salt_values\")).drop(\"salt_values\")\n",
    "\n",
    "# Create matching salted key\n",
    "salted_depts = salted_depts.withColumn(\n",
    "    \"salted_id\", \n",
    "    when(col(\"salt\") > 0, \n",
    "         concat(col(\"id\").cast(\"string\"), lit(\"_\"), col(\"salt\").cast(\"string\")))\n",
    "    .otherwise(col(\"id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Step 4: Join on the salted keys\n",
    "salted_join = salted_employees.join(\n",
    "    salted_depts,\n",
    "    salted_employees[\"salted_dept_id\"] == salted_depts[\"salted_id\"]\n",
    ")\n",
    "\n",
    "# Measure performance\n",
    "time_salted_join, count_salted = time_execution(salted_join)\n",
    "\n",
    "print(f\"Regular join on skewed data: {time_skewed_join:.4f} seconds\")\n",
    "print(f\"Salted join: {time_salted_join:.4f} seconds\")\n",
    "if time_skewed_join > time_salted_join:\n",
    "    print(f\"Performance improvement: {(time_skewed_join/time_salted_join):.2f}x faster with salting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Approach: Handle Skewed Values Separately\n",
    "\n",
    "Another strategy is to process skewed values separately from the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2: Process skewed values separately\n",
    "\n",
    "# Split the dataset\n",
    "skewed_records = skewed_employees_df.filter(col(\"dept_id\").isin(skewed_keys_list))\n",
    "normal_records = skewed_employees_df.filter(~col(\"dept_id\").isin(skewed_keys_list))\n",
    "\n",
    "print(f\"Skewed records: {skewed_records.count()}, Normal records: {normal_records.count()}\")\n",
    "\n",
    "# Process the skewed records with broadcast join\n",
    "start = time.time()\n",
    "skewed_result = skewed_records.join(\n",
    "    broadcast(departments_df),\n",
    "    skewed_records[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "\n",
    "# Process normal records with regular join\n",
    "normal_result = normal_records.join(\n",
    "    departments_df,\n",
    "    normal_records[\"dept_id\"] == departments_df[\"id\"]\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "combined_result = skewed_result.union(normal_result)\n",
    "time_split_process, count_split = time_execution(combined_result)\n",
    "\n",
    "print(f\"Regular join on skewed data: {time_skewed_join:.4f} seconds\")\n",
    "print(f\"Split processing: {time_split_process:.4f} seconds\")\n",
    "if time_skewed_join > time_split_process:\n",
    "    print(f\"Performance improvement: {(time_skewed_join/time_split_process):.2f}x faster with split processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. End-to-End Optimization Example\n",
    "\n",
    "Let's bring everything together with an end-to-end example. We'll start with a suboptimal query and improve it step by step using the techniques we've discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset configuration to defaults\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Default value\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # 10MB\n",
    "\n",
    "# Create larger datasets for this example\n",
    "from pyspark.sql.functions import current_date, datediff, rand\n",
    "\n",
    "# Create a large fact table (orders)\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", DateType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"price\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Generate 100,000 orders\n",
    "order_data = []\n",
    "for i in range(1, 100001):\n",
    "    customer_id = random.randint(1, 1000)\n",
    "    product_id = random.randint(1, 100)\n",
    "    random_days = random.randint(0, 365*3)  # Last 3 years\n",
    "    order_date = datetime.now() - timedelta(days=random_days)\n",
    "    quantity = random.randint(1, 10)\n",
    "    price = round(random.uniform(10, 1000), 2)\n",
    "    order_data.append((i, customer_id, product_id, order_date, quantity, price))\n",
    "\n",
    "orders_df = spark.createDataFrame(order_data, orders_schema)\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# Create dimension tables\n",
    "# Customers\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"signup_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Generate 1,000 customers\n",
    "customer_data = []\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\"]\n",
    "states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\", \"PA\", \"TX\", \"CA\"]\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    name = f\"{random.choice(names)} {random.choice(surnames)}\"\n",
    "    email = f\"{name.lower().replace(' ', '.')}@example.com\"\n",
    "    city_idx = random.randint(0, len(cities)-1)\n",
    "    city = cities[city_idx]\n",
    "    state = states[city_idx]\n",
    "    random_days = random.randint(365, 365*5)  # 1-5 years ago\n",
    "    signup_date = datetime.now() - timedelta(days=random_days)\n",
    "    customer_data.append((i, name, email, city, state, signup_date))\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# Products\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"base_price\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Generate 100 products\n",
    "product_data = []\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Sports\"]\n",
    "product_names = [\"Laptop\", \"Phone\", \"Tablet\", \"TV\", \"Camera\", \"Shirt\", \"Pants\", \"Shoes\", \"Jacket\", \"Sofa\", \n",
    "                 \"Chair\", \"Table\", \"Bed\", \"Novel\", \"Textbook\", \"Cookbook\", \"Basketball\", \"Tennis Racket\", \"Bicycle\"]\n",
    "\n",
    "for i in range(1, 101):\n",
    "    name = random.choice(product_names)\n",
    "    category = random.choice(categories)\n",
    "    base_price = round(random.uniform(10, 500), 2)\n",
    "    product_data.append((i, name, category, base_price))\n",
    "\n",
    "products_df = spark.createDataFrame(product_data, product_schema)\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(f\"Created datasets with {orders_df.count()} orders, {customers_df.count()} customers, and {products_df.count()} products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Suboptimal Query\n",
    "\n",
    "Let's start with a suboptimal query that computes total sales by state and category for the last year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original inefficient query\n",
    "def run_original_query():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.state, \n",
    "        p.category, \n",
    "        COUNT(DISTINCT o.order_id) as num_orders,\n",
    "        SUM(o.quantity * o.price) as total_sales\n",
    "    FROM \n",
    "        orders o\n",
    "    JOIN \n",
    "        customers c ON o.customer_id = c.customer_id\n",
    "    JOIN \n",
    "        products p ON o.product_id = p.product_id\n",
    "    WHERE \n",
    "        o.order_date >= date_sub(current_date(), 365)\n",
    "    GROUP BY \n",
    "        c.state, p.category\n",
    "    ORDER BY \n",
    "        total_sales DESC\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Original Query Plan:\")\n",
    "original_query = run_original_query()\n",
    "original_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "original_query.collect()\n",
    "original_time = time.time() - start\n",
    "print(f\"\\nOriginal query execution time: {original_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 1: Filter Pushdown and Early Projections\n",
    "\n",
    "Our first optimization will be to apply filters early and use only necessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter pushdown and early projections\n",
    "def run_optimized_query_step1():\n",
    "    # First filter the orders\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    # Use only necessary columns from dimension tables\n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    # Join and aggregate\n",
    "    result = filtered_orders \\\n",
    "        .join(customers_slim, \"customer_id\") \\\n",
    "        .join(products_slim, \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 1 Query Plan (Filter Pushdown and Column Pruning):\")\n",
    "step1_query = run_optimized_query_step1()\n",
    "step1_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step1_query.collect()\n",
    "step1_time = time.time() - start\n",
    "print(f\"\\nStep 1 query execution time: {step1_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step1_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 2: Broadcast Small Tables\n",
    "\n",
    "Next, let's use broadcast joins for the dimension tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add broadcast joins\n",
    "def run_optimized_query_step2():\n",
    "    # First filter the orders\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    # Use only necessary columns from dimension tables\n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    # Broadcast the dimension tables for more efficient joins\n",
    "    result = filtered_orders \\\n",
    "        .join(broadcast(customers_slim), \"customer_id\") \\\n",
    "        .join(broadcast(products_slim), \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 2 Query Plan (With Broadcast Joins):\")\n",
    "step2_query = run_optimized_query_step2()\n",
    "step2_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step2_query.collect()\n",
    "step2_time = time.time() - start\n",
    "print(f\"\\nStep 2 query execution time: {step2_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step2_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 1: {(step1_time/step2_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 3: Tune Shuffle Partitions\n",
    "\n",
    "Now, let's adjust the number of shuffle partitions to better match our dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tune shuffle partitions\n",
    "# Default is 200, which is likely too high for our relatively small dataset\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)  # Adjusted value\n",
    "\n",
    "def run_optimized_query_step3():\n",
    "    # Same query as step 2, but with adjusted shuffle partitions\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    result = filtered_orders \\\n",
    "        .join(broadcast(customers_slim), \"customer_id\") \\\n",
    "        .join(broadcast(products_slim), \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 3 Query Plan (With Adjusted Shuffle Partitions):\")\n",
    "step3_query = run_optimized_query_step3()\n",
    "step3_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step3_query.collect()\n",
    "step3_time = time.time() - start\n",
    "print(f\"\\nStep 3 query execution time: {step3_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step3_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 2: {(step2_time/step3_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Step 4: Enable Adaptive Query Execution\n",
    "\n",
    "Finally, let's enable Adaptive Query Execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Enable Adaptive Query Execution\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "def run_optimized_query_step4():\n",
    "    # Same query as step 3, but with AQE enabled\n",
    "    filtered_orders = orders_df \\\n",
    "        .filter(col(\"order_date\") >= expr(\"date_sub(current_date(), 365)\")) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\")\n",
    "    \n",
    "    customers_slim = customers_df.select(\"customer_id\", \"state\")\n",
    "    products_slim = products_df.select(\"product_id\", \"category\")\n",
    "    \n",
    "    result = filtered_orders \\\n",
    "        .join(customers_slim, \"customer_id\") \\\n",
    "        .join(products_slim, \"product_id\") \\\n",
    "        .groupBy(\"state\", \"category\") \\\n",
    "        .agg( \\\n",
    "            count(\"order_id\").alias(\"num_orders\"), \\\n",
    "            sum(col(\"quantity\") * col(\"price\")).alias(\"total_sales\") \\\n",
    "        ) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get the execution plan\n",
    "print(\"Step 4 Query Plan (With Adaptive Query Execution):\")\n",
    "step4_query = run_optimized_query_step4()\n",
    "step4_query.explain()\n",
    "\n",
    "# Measure performance\n",
    "start = time.time()\n",
    "step4_query.collect()\n",
    "step4_time = time.time() - start\n",
    "print(f\"\\nStep 4 query execution time: {step4_time:.4f} seconds\")\n",
    "print(f\"Improvement from original: {(original_time/step4_time):.2f}x faster\")\n",
    "print(f\"Improvement from step 3: {(step3_time/step4_time):.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing All Optimizations\n",
    "\n",
    "Let's summarize the improvements from each optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of performance improvements\n",
    "print(\"Performance Summary:\")\n",
    "print(f\"Original query: {original_time:.4f} seconds\")\n",
    "print(f\"Step 1 (Filter Pushdown & Column Pruning): {step1_time:.4f} seconds, {(original_time/step1_time):.2f}x faster\")\n",
    "print(f\"Step 2 (Broadcast Joins): {step2_time:.4f} seconds, {(original_time/step2_time):.2f}x faster\")\n",
    "print(f\"Step 3 (Tuned Shuffle Partitions): {step3_time:.4f} seconds, {(original_time/step3_time):.2f}x faster\")\n",
    "print(f\"Step 4 (Adaptive Query Execution): {step4_time:.4f} seconds, {(original_time/step4_time):.2f}x faster\")\n",
    "\n",
    "# Reset configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Default value\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices and Guidelines\n",
    "\n",
    "Let's summarize the key best practices for optimizing Spark performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization by Area\n",
    "\n",
    "#### 1. Data Reading and Filtering\n",
    "- **Select only necessary columns**: Reduces I/O and memory usage\n",
    "- **Apply filters as early as possible**: Reduces data volume for downstream operations\n",
    "- **Use appropriate file formats**: Parquet/ORC > CSV/JSON for analytical workloads\n",
    "- **Leverage partition pruning**: Choose good partition columns for large tables\n",
    "- **Compute and maintain statistics**: For better query planning\n",
    "\n",
    "#### 2. Joins\n",
    "- **Optimize join order**: Join the most filtered/smallest tables first\n",
    "- **Use broadcast joins for small tables**: Avoid shuffling large tables\n",
    "- **Handle skew in joins**: Use salting or separate processing for skewed keys\n",
    "- **Pre-partition data on join keys**: To reduce shuffling\n",
    "- **Filter before joining**: Reduces the size of the join operation\n",
    "\n",
    "#### 3. Aggregations\n",
    "- **Tune shuffle partitions**: Based on data size and cluster capacity\n",
    "- **Use window functions efficiently**: For complex analytical queries\n",
    "- **Pre-aggregate data when possible**: Reduces data volume for global aggregations\n",
    "- **Handle skew in groupBy**: Similar to join skew handling\n",
    "\n",
    "#### 4. General Optimizations\n",
    "- **Use built-in functions over UDFs**: Better performance with codegen\n",
    "- **Enable Adaptive Query Execution**: For dynamic optimizations\n",
    "- **Cache judiciously**: Only for frequently accessed datasets\n",
    "- **Monitor and analyze query plans**: Identify bottlenecks\n",
    "- **Set appropriate configurations**: Memory, cores, shuffle partitions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "\n",
    "Here are some important configuration parameters to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current configuration values\n",
    "print(\"Current configuration:\")\n",
    "config_params = [\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.sql.files.maxPartitionBytes\",\n",
    "    \"spark.default.parallelism\"\n",
    "]\n",
    "\n",
    "for param in config_params:\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"{param}: {value}\")\n",
    "    except:\n",
    "        print(f\"{param}: Not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Configuration Tuning\n",
    "\n",
    "1. **spark.sql.shuffle.partitions**:\n",
    "   - Default: 200\n",
    "   - Start with 2-3 times the number of cores for small-medium datasets\n",
    "   - For large datasets (TB+), increase based on data size\n",
    "\n",
    "2. **spark.sql.autoBroadcastJoinThreshold**:\n",
    "   - Default: 10MB\n",
    "   - Increase for larger driver memory (e.g., 50-100MB)\n",
    "   - Set to -1 to disable automatic broadcasting\n",
    "\n",
    "3. **spark.sql.adaptive.enabled**:\n",
    "   - Default: true (in Spark 3.x)\n",
    "   - Recommended: true for most workloads\n",
    "\n",
    "4. **spark.driver.memory** and **spark.executor.memory**:\n",
    "   - Depends on your cluster resources\n",
    "   - Avoid OOM errors with appropriate sizing\n",
    "\n",
    "5. **spark.sql.files.maxPartitionBytes**:\n",
    "   - Default: 128MB\n",
    "   - Adjust based on file size and memory available\n",
    "\n",
    "### When to Use Each Optimization Technique\n",
    "\n",
    "| Technique               | When to Use                                 | Example Scenario                             |\n",
    "|-------------------------|---------------------------------------------|----------------------------------------------|\n",
    "| Filter Pushdown         | Large datasets with filtering conditions    | Querying subset of time-series data          |\n",
    "| Broadcast Joins         | Joining with small dimension tables         | Fact table with dimension lookups            |\n",
    "| Repartitioning          | Before joins on non-uniform data            | Pre-shuffle data on join key                 |\n",
    "| Caching                 | Repeatedly accessed intermediate results    | Iterative algorithms, multiple queries       |\n",
    "| Salting                 | Highly skewed join or groupBy keys          | User activity data with popular items        |\n",
    "| Shuffle Partition Tuning| Performance tuning for specific data size   | Adjust based on cluster and dataset size     |\n",
    "| Adaptive Execution      | Complex queries with unpredictable stats    | Ad-hoc analytical queries                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Tips\n",
    "\n",
    "1. **Understand your data**: Distribution, size, access patterns\n",
    "2. **Analyze query plans**: Identify bottlenecks early\n",
    "3. **Test incrementally**: Apply one optimization at a time\n",
    "4. **Monitor Spark UI**: For execution details and skew detection\n",
    "5. **Prefer simple optimizations first**: Often the biggest gains come from basic techniques\n",
    "\n",
    "By carefully analyzing execution plans and applying these optimization techniques, you can significantly improve the performance of your Spark queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.catalog.clearCache()\n",
    "print(\"Resources cleaned up. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 