{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Best Practices\n",
        "\n",
        "This notebook covers key best practices and optimization techniques for working with PySpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. SparkSession Configuration\n",
        "\n",
        "Properly configuring your SparkSession is the first step toward optimized performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a well-configured SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Best Practices\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 200) \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.default.parallelism\", 8) \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Get the current configuration\n",
        "print(spark.sparkContext.getConf().getAll())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Configuration Parameters\n",
        "\n",
        "- **spark.sql.shuffle.partitions**: Controls the number of partitions during shuffles (default: 200)\n",
        "- **spark.executor.memory**: Memory per executor\n",
        "- **spark.driver.memory**: Memory for driver process\n",
        "- **spark.default.parallelism**: Default number of partitions for RDDs\n",
        "- **spark.sql.adaptive.enabled**: Enables adaptive query execution\n",
        "\n",
        "ðŸ’¡ **Best Practice**: Tune these parameters based on your cluster size and workload characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Schema Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Define schema explicitly\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"salary\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Create sample data\n",
        "data = [\n",
        "    (1, \"Alice\", 30, 50000.0),\n",
        "    (2, \"Bob\", 32, 60000.0),\n",
        "    (3, \"Charlie\", 28, 55000.0)\n",
        "]\n",
        "\n",
        "# Create DataFrame with explicit schema\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Practices for Data Loading\n",
        "\n",
        "1. **Always Define Schema Explicitly**\n",
        "   - Avoids costly schema inference\n",
        "   - Ensures correct data types\n",
        "   - Improves performance on large datasets\n",
        "\n",
        "2. **Use Appropriate File Formats**\n",
        "   - Parquet is usually the best choice (columnar, compressed, schema-preserved)\n",
        "   - ORC is good for Hive compatibility\n",
        "   - Avoid text/CSV for large datasets when possible\n",
        "\n",
        "3. **Partition Data Appropriately**\n",
        "   - Choose partition columns that distribute data evenly\n",
        "   - Avoid too many small partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of reading/writing with best practices\n",
        "\n",
        "# Write data using Parquet format with partitioning\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"age\") \\\n",
        "    .parquet(\"/tmp/example-data\")\n",
        "\n",
        "# Read data with explicit schema\n",
        "df_read = spark.read \\\n",
        "    .schema(schema) \\\n",
        "    .parquet(\"/tmp/example-data\")\n",
        "\n",
        "df_read.explain()  # Show execution plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DataFrame Operations - Transformations and Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when, expr, avg, sum, max, min, count\n",
        "\n",
        "# Column operations - Good practice\n",
        "df_transformed = df \\\n",
        "    .select(\n",
        "        col(\"id\"),\n",
        "        col(\"name\"),\n",
        "        col(\"age\"),\n",
        "        (col(\"salary\") * 1.1).alias(\"adjusted_salary\")\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"salary_category\",\n",
        "        when(col(\"salary\") < 55000, \"Entry\")\n",
        "        .when(col(\"salary\") < 65000, \"Mid\")\n",
        "        .otherwise(\"Senior\")\n",
        "    ) \\\n",
        "    .filter(col(\"age\") > 25)\n",
        "\n",
        "df_transformed.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Practices for Transformations\n",
        "\n",
        "1. **Chain Operations Efficiently**\n",
        "   - Chain multiple transformations before actions\n",
        "   - Use method chaining for readability\n",
        "\n",
        "2. **Use Column Expressions**\n",
        "   - Prefer `col()` and expressions over UDFs when possible\n",
        "   - Use SQL functions from `pyspark.sql.functions`\n",
        "\n",
        "3. **Limit Shuffling Operations**\n",
        "   - Operations like `groupBy`, `join`, `repartition` cause shuffling\n",
        "   - Try to minimize these operations\n",
        "\n",
        "4. **Filter Early**\n",
        "   - Apply filters as early as possible to reduce data volume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Optimization Techniques - Caching and Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Cache data that will be reused\n",
        "df_cached = df.cache()  # or df.persist()\n",
        "df_cached.count()  # Materialize the cache\n",
        "\n",
        "# More control with specific storage level\n",
        "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "df_persisted.count()  # Materialize the persistence\n",
        "\n",
        "# Clean up when done\n",
        "df_cached.unpersist()\n",
        "df_persisted.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching Best Practices\n",
        "\n",
        "1. **When to Cache**\n",
        "   - Cache DataFrames used multiple times\n",
        "   - Cache after expensive transformations\n",
        "   - Cache after filtering down large datasets\n",
        "\n",
        "2. **Storage Levels**\n",
        "   - `MEMORY_ONLY`: Default, fastest but can cause OOM errors\n",
        "   - `MEMORY_AND_DISK`: Safer option, spills to disk if needed\n",
        "   - `DISK_ONLY`: When memory is limited\n",
        "\n",
        "3. **Clean Up Cache**\n",
        "   - Call `unpersist()` when done to free up resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Joins and Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a department DataFrame\n",
        "dept_data = [(1, \"Engineering\"), (2, \"HR\"), (3, \"Marketing\")]\n",
        "dept_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"department\", StringType(), True)\n",
        "])\n",
        "dept_df = spark.createDataFrame(dept_data, schema=dept_schema)\n",
        "\n",
        "# Broadcast join (efficient for small + large table joins)\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Broadcast the smaller DataFrame\n",
        "joined_df = df.join(broadcast(dept_df), df.id == dept_df.id)\n",
        "joined_df.explain()\n",
        "joined_df.show()\n",
        "\n",
        "# Efficient aggregations\n",
        "agg_df = df.groupBy(\"age\").agg(\n",
        "    count(\"id\").alias(\"count\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\"),\n",
        "    max(\"salary\").alias(\"max_salary\")\n",
        ")\n",
        "\n",
        "agg_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Practices for Joins and Aggregations\n",
        "\n",
        "1. **Join Strategies**\n",
        "   - Use broadcast joins when one DataFrame is small (<10MB)\n",
        "   - Join on columns with high cardinality\n",
        "   - Prefer using `join()` with explicit conditions over SQL-style joins\n",
        "\n",
        "2. **Join Types**\n",
        "   - Inner joins are fastest\n",
        "   - Left/right outer joins preserve one side\n",
        "   - Full outer joins are most expensive\n",
        "\n",
        "3. **Efficient Aggregations**\n",
        "   - Combine multiple aggregations in a single call\n",
        "   - Filter before grouping when possible\n",
        "   - Consider `approx_count_distinct()` for approximate counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Partitioning and Bucketing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repartitioning to control parallelism\n",
        "df_repartitioned = df.repartition(8)\n",
        "print(f\"Number of partitions: {df_repartitioned.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Repartition by specific column (good for joins)\n",
        "df_repart_by_col = df.repartition(\"age\")\n",
        "\n",
        "# Coalesce to reduce partitions (no shuffle)\n",
        "df_coalesced = df_repartitioned.coalesce(2)\n",
        "print(f\"Number of partitions after coalesce: {df_coalesced.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Write with bucketing (good for repeated joins)\n",
        "df.write \\\n",
        "    .bucketBy(4, \"id\") \\\n",
        "    .sortBy(\"id\") \\\n",
        "    .saveAsTable(\"bucketed_table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partitioning Best Practices\n",
        "\n",
        "1. **Choosing Number of Partitions**\n",
        "   - Rule of thumb: 2-3 Ã— number of CPU cores\n",
        "   - Too few: underutilization, potential OOM\n",
        "   - Too many: task scheduling overhead\n",
        "\n",
        "2. **When to Repartition**\n",
        "   - Before wide operations (joins, groupBy)\n",
        "   - When partition sizes are skewed\n",
        "   - When number of partitions is too low/high\n",
        "\n",
        "3. **Coalesce vs. Repartition**\n",
        "   - Use `coalesce()` to reduce partitions (no shuffle)\n",
        "   - Use `repartition()` to increase partitions (full shuffle)\n",
        "\n",
        "4. **Bucketing**\n",
        "   - Good for repeated joins on same column\n",
        "   - Pre-organizes data to avoid shuffles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. UDFs and Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Avoid this approach for simple operations\n",
        "@udf(returnType=IntegerType())\n",
        "def slow_add_one(x):\n",
        "    if x is not None:\n",
        "        return x + 1\n",
        "    return None\n",
        "\n",
        "# Prefer built-in functions (much faster)\n",
        "df_slow = df.withColumn(\"age_plus_one\", slow_add_one(col(\"age\")))\n",
        "df_fast = df.withColumn(\"age_plus_one\", col(\"age\") + 1)\n",
        "\n",
        "# Compare execution plans\n",
        "print(\"With UDF:\")\n",
        "df_slow.explain()\n",
        "print(\"\\nWith built-in function:\")\n",
        "df_fast.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UDF Best Practices\n",
        "\n",
        "1. **Avoid UDFs When Possible**\n",
        "   - Use built-in functions from `pyspark.sql.functions`\n",
        "   - Use SQL expressions with `expr()`\n",
        "   - UDFs require serialization/deserialization overhead\n",
        "\n",
        "2. **When to Use UDFs**\n",
        "   - Complex logic not available in built-in functions\n",
        "   - Operations requiring external libraries\n",
        "\n",
        "3. **Pandas UDFs**\n",
        "   - Use Pandas UDFs (vectorized UDFs) for better performance\n",
        "   - Can be 10-100x faster than regular UDFs\n",
        "   - Requires Arrow serialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pandas UDF example (much faster than regular UDF)\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(IntegerType())\n",
        "def pandas_add_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "df_pandas_udf = df.withColumn(\"age_plus_one\", pandas_add_one(col(\"age\")))\n",
        "df_pandas_udf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank, lead, lag, sum\n",
        "\n",
        "# Create window spec\n",
        "window_spec = Window.partitionBy(\"age\").orderBy(\"salary\")\n",
        "\n",
        "# Apply window functions\n",
        "df_window = df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
        "              .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
        "              .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "              .withColumn(\"next_salary\", lead(\"salary\", 1).over(window_spec)) \\\n",
        "              .withColumn(\"prev_salary\", lag(\"salary\", 1).over(window_spec))\n",
        "\n",
        "# Window for running totals\n",
        "sum_window = Window.partitionBy(\"age\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "df_window = df_window.withColumn(\"running_total\", sum(\"salary\").over(sum_window))\n",
        "\n",
        "df_window.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window Function Best Practices\n",
        "\n",
        "1. **Reuse Window Specifications**\n",
        "   - Define window specs once and reuse\n",
        "   - Improves readability and performance\n",
        "\n",
        "2. **Window Function Types**\n",
        "   - Ranking functions: `rank()`, `dense_rank()`, `row_number()`\n",
        "   - Analytic functions: `lead()`, `lag()`\n",
        "   - Aggregate functions: `sum()`, `avg()`, `min()`, `max()`\n",
        "\n",
        "3. **Bounded vs. Unbounded Windows**\n",
        "   - Bounded windows (e.g., `rowsBetween(-2, 2)`) are faster\n",
        "   - Unbounded windows need more resources\n",
        "\n",
        "4. **Partitioning Considerations**\n",
        "   - Partition by columns with reasonable cardinality\n",
        "   - Too many partitions can degrade performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get execution plan\n",
        "df_complex = df.join(dept_df, df.id == dept_df.id) \\\n",
        "               .groupBy(\"department\") \\\n",
        "               .agg(avg(\"salary\").alias(\"avg_salary\")) \\\n",
        "               .filter(col(\"avg_salary\") > 50000)\n",
        "\n",
        "# Logical and physical plans\n",
        "print(\"Logical Plan:\")\n",
        "df_complex.explain()\n",
        "\n",
        "print(\"\\nDetailed Physical Plan:\")\n",
        "df_complex.explain(\"formatted\")\n",
        "\n",
        "# Count with explanation\n",
        "from time import time\n",
        "start = time()\n",
        "count = df_complex.count()\n",
        "end = time()\n",
        "print(f\"Count: {count}, Time: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Monitoring Best Practices\n",
        "\n",
        "1. **Use explain() to Understand Plans**\n",
        "   - Check physical plan for expensive operations\n",
        "   - Look for broadcast joins, exchange (shuffle) operations\n",
        "\n",
        "2. **Monitor Spark UI**\n",
        "   - Available at http://localhost:4040 by default\n",
        "   - Check stage durations, executor utilization\n",
        "   - Identify skew in task durations\n",
        "\n",
        "3. **Performance Metrics to Watch**\n",
        "   - Shuffle read/write size\n",
        "   - Spill memory to disk\n",
        "   - Task durations\n",
        "   - Cache hit ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Common Optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Broadcast variables for lookup tables\n",
        "lookup_dict = {1: \"A\", 2: \"B\", 3: \"C\"}\n",
        "lookup_broadcast = spark.sparkContext.broadcast(lookup_dict)\n",
        "\n",
        "@udf(StringType())\n",
        "def lookup_value(key):\n",
        "    return lookup_broadcast.value.get(key, \"Unknown\")\n",
        "\n",
        "df.withColumn(\"lookup_result\", lookup_value(col(\"id\"))).show()\n",
        "\n",
        "# 2. Avoid collect() on large DataFrames\n",
        "# Good: Take just what you need\n",
        "sample_rows = df.limit(10).collect()\n",
        "\n",
        "# 3. Use SQL when it's more intuitive\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "dept_df.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "sql_result = spark.sql(\"\"\"\n",
        "    SELECT d.department, AVG(e.salary) as avg_salary\n",
        "    FROM employees e JOIN departments d ON e.id = d.id\n",
        "    GROUP BY d.department\n",
        "    HAVING AVG(e.salary) > 50000\n",
        "\"\"\")\n",
        "\n",
        "sql_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of PySpark Best Practices\n",
        "\n",
        "1. **Data Management**\n",
        "   - Use Parquet format for efficient storage\n",
        "   - Define schemas explicitly\n",
        "   - Partition data wisely\n",
        "\n",
        "2. **Performance Optimization**\n",
        "   - Cache/persist reused DataFrames\n",
        "   - Use broadcast joins for small tables\n",
        "   - Minimize shuffling operations\n",
        "   - Filter early to reduce data volume\n",
        "\n",
        "3. **Code Practices**\n",
        "   - Prefer built-in functions over UDFs\n",
        "   - Use Pandas UDFs for better performance\n",
        "   - Chain transformations efficiently\n",
        "   - Monitor the Spark UI and physical plans\n",
        "\n",
        "4. **Resource Management**\n",
        "   - Tune partitioning based on cluster size\n",
        "   - Configure memory settings appropriately\n",
        "   - Clean up cached data when no longer needed"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 