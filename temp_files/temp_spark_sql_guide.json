{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spark SQL: Comprehensive Guide with Best Practices\n",
        "\n",
        "This notebook covers the essentials of Spark SQL, including:\n",
        "\n",
        "1. **SQL Basics**: Creating tables/views and running queries\n",
        "2. **User-Defined Functions (UDFs)** in SQL context\n",
        "3. **Advanced SQL Features**: Window functions, complex types, and SQL optimization\n",
        "4. **Performance Best Practices**: Tips for efficient Spark SQL usage\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr, udf, lit, when, avg, sum, max, min, count, desc\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType, BooleanType\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark SQL Tutorial\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(\"Spark Session initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Creating Sample Datasets\n",
        "\n",
        "Before diving into SQL, let's create some sample datasets to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data for employees\n",
        "employee_data = [\n",
        "    (1, \"John\", \"Doe\", \"Engineering\", 80000, 5),\n",
        "    (2, \"Jane\", \"Smith\", \"Engineering\", 95000, 7),\n",
        "    (3, \"Alice\", \"Johnson\", \"Sales\", 75000, 3),\n",
        "    (4, \"Bob\", \"Brown\", \"Sales\", 68000, 2),\n",
        "    (5, \"Charlie\", \"Miller\", \"Marketing\", 72000, 4),\n",
        "    (6, \"Dave\", \"Wilson\", \"Engineering\", 105000, 9),\n",
        "    (7, \"Eve\", \"Davis\", \"HR\", 65000, 5),\n",
        "    (8, \"Frank\", \"Jones\", \"Marketing\", 78000, 6),\n",
        "    (9, \"Grace\", \"Taylor\", \"Engineering\", 92000, 6),\n",
        "    (10, \"Helen\", \"Moore\", \"Sales\", 81000, 4)\n",
        "]\n",
        "\n",
        "employee_schema = StructType([\n",
        "    StructField(\"emp_id\", IntegerType(), False),\n",
        "    StructField(\"first_name\", StringType(), False),\n",
        "    StructField(\"last_name\", StringType(), False),\n",
        "    StructField(\"department\", StringType(), False),\n",
        "    StructField(\"salary\", IntegerType(), True),\n",
        "    StructField(\"years_exp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "employees_df = spark.createDataFrame(employee_data, employee_schema)\n",
        "employees_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data for departments\n",
        "department_data = [\n",
        "    (\"Engineering\", \"San Francisco\", \"John Smith\", 35),\n",
        "    (\"Sales\", \"New York\", \"Mary Johnson\", 28),\n",
        "    (\"Marketing\", \"Chicago\", \"James Brown\", 22),\n",
        "    (\"HR\", \"Boston\", \"Patricia Davis\", 15),\n",
        "    (\"Finance\", \"San Jose\", \"Robert Wilson\", 18)\n",
        "]\n",
        "\n",
        "department_schema = StructType([\n",
        "    StructField(\"dept_name\", StringType(), False),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"manager\", StringType(), True),\n",
        "    StructField(\"employee_count\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "departments_df = spark.createDataFrame(department_data, department_schema)\n",
        "departments_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more complex dataset with arrays and maps\n",
        "projects_data = [\n",
        "    (1, \"Mobile App\", [1, 2, 6, 9], {\"budget\": 250000, \"status\": \"active\", \"priority\": \"high\"}),\n",
        "    (2, \"Website Redesign\", [3, 5, 8], {\"budget\": 175000, \"status\": \"active\", \"priority\": \"medium\"}),\n",
        "    (3, \"Database Migration\", [2, 6], {\"budget\": 300000, \"status\": \"planning\", \"priority\": \"high\"}),\n",
        "    (4, \"API Integration\", [1, 4, 7], {\"budget\": 120000, \"status\": \"completed\", \"priority\": \"low\"}),\n",
        "    (5, \"Data Analytics\", [5, 8, 9, 10], {\"budget\": 200000, \"status\": \"active\", \"priority\": \"medium\"})\n",
        "]\n",
        "\n",
        "projects_schema = StructType([\n",
        "    StructField(\"project_id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), False),\n",
        "    StructField(\"team_members\", ArrayType(IntegerType()), True),\n",
        "    StructField(\"details\", MapType(StringType(), StringType()), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "projects_df = spark.createDataFrame(projects_data, projects_schema)\n",
        "projects_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Spark SQL Basics\n",
        "\n",
        "Now that we have our data, let's explore different ways to work with Spark SQL. Spark SQL provides a SQL interface to interact with structured data in Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Creating Temporary Views\n",
        "\n",
        "To query data using SQL, we first need to create temporary views from our DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create temporary views from DataFrames\n",
        "employees_df.createOrReplaceTempView(\"employees\")\n",
        "departments_df.createOrReplaceTempView(\"departments\")\n",
        "projects_df.createOrReplaceTempView(\"projects\")\n",
        "\n",
        "# List all tables in the current session\n",
        "print(\"Available tables:\")\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Basic SQL Queries\n",
        "\n",
        "Let's start with some basic SQL queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple SELECT query\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    department, \n",
        "    salary\n",
        "FROM \n",
        "    employees\n",
        "WHERE \n",
        "    salary > 80000\n",
        "ORDER BY \n",
        "    salary DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregation query\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    department, \n",
        "    COUNT(*) as employee_count, \n",
        "    AVG(salary) as avg_salary, \n",
        "    MAX(salary) as max_salary,\n",
        "    MIN(salary) as min_salary,\n",
        "    SUM(salary) as total_salary\n",
        "FROM \n",
        "    employees\n",
        "GROUP BY \n",
        "    department\n",
        "ORDER BY \n",
        "    avg_salary DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JOIN query\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    e.emp_id, \n",
        "    e.first_name, \n",
        "    e.last_name, \n",
        "    e.department, \n",
        "    d.location,\n",
        "    d.manager as dept_manager\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "ORDER BY \n",
        "    e.emp_id\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Working with Complex Types\n",
        "\n",
        "Spark SQL can handle complex data types like arrays and maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query with array operations\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    project_id, \n",
        "    name, \n",
        "    size(team_members) as team_size,\n",
        "    team_members as member_ids,\n",
        "    array_contains(team_members, 1) as has_employee_1\n",
        "FROM \n",
        "    projects\n",
        "ORDER BY \n",
        "    team_size DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query with map operations\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    project_id, \n",
        "    name, \n",
        "    details['budget'] as budget,\n",
        "    details['status'] as status,\n",
        "    details['priority'] as priority\n",
        "FROM \n",
        "    projects\n",
        "WHERE \n",
        "    details['status'] = 'active'\n",
        "ORDER BY \n",
        "    details['budget'] DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Subqueries and Common Table Expressions (CTEs)\n",
        "\n",
        "Spark SQL supports advanced SQL features like subqueries and CTEs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subquery example\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    department, \n",
        "    salary\n",
        "FROM \n",
        "    employees\n",
        "WHERE \n",
        "    salary > (\n",
        "        SELECT AVG(salary) \n",
        "        FROM employees\n",
        "    )\n",
        "ORDER BY \n",
        "    salary DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CTE (Common Table Expression) example\n",
        "query = \"\"\"\n",
        "WITH dept_stats AS (\n",
        "    SELECT \n",
        "        department, \n",
        "        AVG(salary) as avg_dept_salary,\n",
        "        MAX(years_exp) as max_experience\n",
        "    FROM \n",
        "        employees\n",
        "    GROUP BY \n",
        "        department\n",
        "),\n",
        "high_paid_departments AS (\n",
        "    SELECT \n",
        "        department\n",
        "    FROM \n",
        "        dept_stats\n",
        "    WHERE \n",
        "        avg_dept_salary > 80000\n",
        ")\n",
        "SELECT \n",
        "    e.first_name,\n",
        "    e.last_name,\n",
        "    e.department,\n",
        "    e.salary,\n",
        "    s.avg_dept_salary,\n",
        "    s.max_experience\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    dept_stats s\n",
        "ON \n",
        "    e.department = s.department\n",
        "WHERE \n",
        "    e.department IN (SELECT department FROM high_paid_departments)\n",
        "ORDER BY \n",
        "    e.salary DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Window Functions\n",
        "\n",
        "Window functions perform calculations across related rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Window function example\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id,\n",
        "    first_name,\n",
        "    last_name,\n",
        "    department,\n",
        "    salary,\n",
        "    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank,\n",
        "    DENSE_RANK() OVER (ORDER BY salary DESC) as overall_salary_rank,\n",
        "    salary - AVG(salary) OVER (PARTITION BY department) as diff_from_dept_avg,\n",
        "    salary / SUM(salary) OVER (PARTITION BY department) * 100 as pct_of_dept_total\n",
        "FROM \n",
        "    employees\n",
        "ORDER BY \n",
        "    department, \n",
        "    dept_salary_rank\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. User-Defined Functions (UDFs) in Spark SQL\n",
        "\n",
        "User-Defined Functions allow you to extend SQL with custom logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Creating and Using SQL UDFs\n",
        "\n",
        "Let's create some UDFs and use them in SQL queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Register a UDF using SQL syntax\n",
        "spark.sql(\"\"\"\n",
        "CREATE TEMPORARY FUNCTION calculate_bonus\n",
        "AS (salary, years, rate) -> \n",
        "    CASE\n",
        "      WHEN years > 5 THEN salary * rate * 1.5\n",
        "      ELSE salary * rate\n",
        "    END\n",
        "\"\"\")\n",
        "\n",
        "# Use the UDF in a SQL query\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    salary, \n",
        "    years_exp,\n",
        "    calculate_bonus(salary, years_exp, 0.1) as bonus\n",
        "FROM \n",
        "    employees\n",
        "ORDER BY \n",
        "    bonus DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Register a Python function as a UDF\n",
        "# Define Python function\n",
        "def full_name(first_name, last_name, add_title=False):\n",
        "    if add_title:\n",
        "        return f\"Mr./Ms. {first_name} {last_name}\"\n",
        "    else:\n",
        "        return f\"{first_name} {last_name}\"\n",
        "\n",
        "# Register as UDF\n",
        "spark.udf.register(\"full_name\", full_name, StringType())\n",
        "\n",
        "# Use in SQL\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    full_name(first_name, last_name) as name,\n",
        "    full_name(first_name, last_name, true) as formal_name\n",
        "FROM \n",
        "    employees\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Complex UDFs with Struct Returns\n",
        "\n",
        "UDFs can return complex data types like structs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Define return schema for our UDF\n",
        "compensation_schema = StructType([\n",
        "    StructField(\"base\", IntegerType(), True),\n",
        "    StructField(\"bonus\", DoubleType(), True),\n",
        "    StructField(\"total\", DoubleType(), True),\n",
        "    StructField(\"rating\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Define Python function\n",
        "def calculate_compensation(salary, years):\n",
        "    # Calculate bonus\n",
        "    bonus_rate = 0.05 + (years * 0.01)  # 5% + 1% per year\n",
        "    bonus = salary * bonus_rate\n",
        "    total = salary + bonus\n",
        "    \n",
        "    # Determine rating\n",
        "    if total > 100000:\n",
        "        rating = \"Excellent\"\n",
        "    elif total > 80000:\n",
        "        rating = \"Good\"\n",
        "    else:\n",
        "        rating = \"Average\"\n",
        "        \n",
        "    return (int(salary), float(bonus), float(total), rating)\n",
        "\n",
        "# Register UDF\n",
        "spark.udf.register(\"calculate_compensation\", calculate_compensation, compensation_schema)\n",
        "\n",
        "# Use in SQL\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    department,\n",
        "    calculate_compensation(salary, years_exp) as compensation,\n",
        "    calculate_compensation(salary, years_exp).total as total_comp,\n",
        "    calculate_compensation(salary, years_exp).rating as performance\n",
        "FROM \n",
        "    employees\n",
        "ORDER BY \n",
        "    total_comp DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Pandas UDFs (Vectorized UDFs)\n",
        "\n",
        "Pandas UDFs provide much better performance than regular UDFs by leveraging vectorized operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "# Register a pandas UDF\n",
        "@pandas_udf(DoubleType())\n",
        "def calculate_tax(salary_series: pd.Series) -> pd.Series:\n",
        "    # Apply tax brackets\n",
        "    # - 10% up to 50K\n",
        "    # - 20% from 50K to 100K\n",
        "    # - 30% above 100K\n",
        "    def tax_for_salary(salary):\n",
        "        if salary <= 50000:\n",
        "            return salary * 0.10\n",
        "        elif salary <= 100000:\n",
        "            return 5000 + (salary - 50000) * 0.20\n",
        "        else:\n",
        "            return 5000 + 10000 + (salary - 100000) * 0.30\n",
        "    \n",
        "    return salary_series.apply(tax_for_salary)\n",
        "\n",
        "# Register for SQL use\n",
        "spark.udf.register(\"calculate_tax\", calculate_tax)\n",
        "\n",
        "# Use in SQL\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    emp_id, \n",
        "    first_name, \n",
        "    last_name, \n",
        "    department,\n",
        "    salary,\n",
        "    calculate_tax(salary) as tax,\n",
        "    salary - calculate_tax(salary) as after_tax\n",
        "FROM \n",
        "    employees\n",
        "ORDER BY \n",
        "    tax DESC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Advanced Features and Best Practices\n",
        "\n",
        "This section covers advanced features and best practices for working with Spark SQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Creating and Using Tables\n",
        "\n",
        "Beyond temporary views, you can create more persistent tables in Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save data as a Spark table (in the metastore)\n",
        "employees_df.write.saveAsTable(\"global_employees\")\n",
        "\n",
        "# List all tables\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Describe table schema\n",
        "spark.sql(\"DESCRIBE TABLE global_employees\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Optimizing SQL Queries\n",
        "\n",
        "Let's look at how to optimize SQL queries in Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine query plans to understand optimization\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    e.department, \n",
        "    AVG(e.salary) as avg_salary,\n",
        "    d.location\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "WHERE \n",
        "    e.salary > 70000\n",
        "GROUP BY \n",
        "    e.department, d.location\n",
        "HAVING \n",
        "    AVG(e.salary) > 75000\n",
        "ORDER BY \n",
        "    avg_salary DESC\n",
        "\"\"\"\n",
        "\n",
        "# Get query plan\n",
        "print(\"Logical and Physical Plans:\")\n",
        "spark.sql(query).explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Broadcast join for small tables\n",
        "# Enable automatic broadcast joins for small tables\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # 10MB\n",
        "\n",
        "# Force broadcast with a hint\n",
        "query = \"\"\"\n",
        "SELECT /*+ BROADCAST(d) */ \n",
        "    e.emp_id, \n",
        "    e.first_name, \n",
        "    e.last_name, \n",
        "    e.department, \n",
        "    d.location\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "\"\"\"\n",
        "\n",
        "# Check if broadcast was applied\n",
        "print(\"Physical Plan (should include BroadcastHashJoin):\")\n",
        "spark.sql(query).explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Performance Comparison: DataFrame vs SQL\n",
        "\n",
        "Let's compare the performance of equivalent operations using DataFrame API vs SQL syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a larger dataset for performance testing\n",
        "large_df = spark.range(0, 1000000) \\\n",
        "    .withColumn(\"random_value\", (col(\"id\") * 12.345) % 100) \\\n",
        "    .withColumn(\"group\", (col(\"id\") % 10).cast(\"integer\")) \\\n",
        "    .withColumn(\"subgroup\", (col(\"id\") % 100).cast(\"integer\"))\n",
        "\n",
        "large_df.createOrReplaceTempView(\"large_table\")\n",
        "\n",
        "# Warm up the JVM\n",
        "large_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with SQL\n",
        "def test_sql_performance():\n",
        "    start_time = time.time()\n",
        "    result = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            group, \n",
        "            subgroup, \n",
        "            COUNT(*) as count, \n",
        "            AVG(random_value) as avg_val,\n",
        "            MAX(random_value) as max_val\n",
        "        FROM \n",
        "            large_table\n",
        "        WHERE \n",
        "            random_value > 50\n",
        "        GROUP BY \n",
        "            group, subgroup\n",
        "        HAVING \n",
        "            COUNT(*) > 5\n",
        "        ORDER BY \n",
        "            group, subgroup\n",
        "    \"\"\")\n",
        "    result.collect()  # Force execution\n",
        "    return time.time() - start_time\n",
        "\n",
        "# Test with DataFrame API\n",
        "def test_df_performance():\n",
        "    start_time = time.time()\n",
        "    result = large_df \\\n",
        "        .filter(col(\"random_value\") > 50) \\\n",
        "        .groupBy(\"group\", \"subgroup\") \\\n",
        "        .agg( \\\n",
        "            count(\"*\").alias(\"count\"), \\\n",
        "            avg(\"random_value\").alias(\"avg_val\"), \\\n",
        "            max(\"random_value\").alias(\"max_val\") \\\n",
        "        ) \\\n",
        "        .filter(col(\"count\") > 5) \\\n",
        "        .orderBy(\"group\", \"subgroup\")\n",
        "    result.collect()  # Force execution\n",
        "    return time.time() - start_time\n",
        "\n",
        "# Run multiple times for more accurate comparison\n",
        "sql_times = []\n",
        "df_times = []\n",
        "\n",
        "for i in range(3):\n",
        "    sql_time = test_sql_performance()\n",
        "    df_time = test_df_performance()\n",
        "    sql_times.append(sql_time)\n",
        "    df_times.append(df_time)\n",
        "    print(f\"Run {i+1}: SQL: {sql_time:.3f}s, DataFrame: {df_time:.3f}s\")\n",
        "\n",
        "print(f\"\\nAverage: SQL: {sum(sql_times)/len(sql_times):.3f}s, DataFrame: {sum(df_times)/len(df_times):.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 SQL Best Practices\n",
        "\n",
        "Here are some best practices for using Spark SQL effectively:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Filter Early\n",
        "\n",
        "Apply filters as early as possible to reduce data size before expensive operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Good: Filter before join\n",
        "query_good = \"\"\"\n",
        "SELECT e.first_name, e.last_name, d.location\n",
        "FROM \n",
        "    (SELECT * FROM employees WHERE salary > 80000) e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "\"\"\"\n",
        "\n",
        "# Bad: Filter after join\n",
        "query_bad = \"\"\"\n",
        "SELECT e.first_name, e.last_name, d.location\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "WHERE \n",
        "    e.salary > 80000\n",
        "\"\"\"\n",
        "\n",
        "# Compare execution plans\n",
        "print(\"Good query plan:\")\n",
        "spark.sql(query_good).explain()\n",
        "\n",
        "print(\"\\nBad query plan:\")\n",
        "spark.sql(query_bad).explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Use Appropriate Join Strategies\n",
        "\n",
        "- Use broadcast joins for small tables\n",
        "- Be mindful of join types (inner, left, right, full)\n",
        "- Avoid cartesian products (cross joins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use broadcast hint for small tables\n",
        "query = \"\"\"\n",
        "SELECT /*+ BROADCAST(d) */ \n",
        "    e.emp_id, e.first_name, e.last_name, d.location\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Favor Standard SQL Functions Over UDFs When Possible\n",
        "\n",
        "Built-in functions are optimized and faster than custom UDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using built-in functions\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    first_name, \n",
        "    last_name,\n",
        "    CONCAT(first_name, ' ', last_name) AS full_name,\n",
        "    CASE \n",
        "        WHEN salary > 90000 THEN 'High'\n",
        "        WHEN salary > 70000 THEN 'Medium'\n",
        "        ELSE 'Low'\n",
        "    END AS salary_tier\n",
        "FROM \n",
        "    employees\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Use Persistent Tables for Frequently Accessed Data\n",
        "\n",
        "Save frequently used DataFrames as tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a table with useful department statistics\n",
        "query = \"\"\"\n",
        "CREATE OR REPLACE TABLE dept_statistics AS\n",
        "SELECT \n",
        "    e.department, \n",
        "    d.location,\n",
        "    COUNT(*) AS emp_count,\n",
        "    AVG(salary) AS avg_salary,\n",
        "    MAX(salary) AS max_salary,\n",
        "    MIN(salary) AS min_salary,\n",
        "    SUM(salary) AS total_salary,\n",
        "    AVG(years_exp) AS avg_experience\n",
        "FROM \n",
        "    employees e\n",
        "JOIN \n",
        "    departments d\n",
        "ON \n",
        "    e.department = d.dept_name\n",
        "GROUP BY \n",
        "    e.department, d.location\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query)\n",
        "\n",
        "# Now use the persistent table\n",
        "spark.sql(\"SELECT * FROM dept_statistics\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Leverage Spark SQL Configurations\n",
        "\n",
        "Tune Spark SQL with appropriate configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show current SQL configurations\n",
        "print(\"Current SQL Configurations:\")\n",
        "configs = [\n",
        "    \"spark.sql.shuffle.partitions\",\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
        "    \"spark.sql.adaptive.enabled\",\n",
        "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
        "    \"spark.sql.optimizer.maxIterations\"\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    try:\n",
        "        value = spark.conf.get(config)\n",
        "        print(f\"{config}: {value}\")\n",
        "    except:\n",
        "        print(f\"{config}: Not set\")\n",
        "\n",
        "# Set some optimized values\n",
        "print(\"\\nSetting optimized values...\")\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "\n",
        "# Verify changes\n",
        "print(\"\\nVerify new configurations:\")\n",
        "for config in configs:\n",
        "    try:\n",
        "        value = spark.conf.get(config)\n",
        "        print(f\"{config}: {value}\")\n",
        "    except:\n",
        "        print(f\"{config}: Not set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "In this notebook, we've covered a wide range of Spark SQL features and best practices:\n",
        "\n",
        "1. **Basic SQL Operations**: Queries, joins, aggregations, and complex data types\n",
        "2. **Advanced SQL Features**: Subqueries, CTEs, and window functions\n",
        "3. **User-Defined Functions**: Different types of UDFs and when to use them\n",
        "4. **Performance Optimization**: Tips and tricks for efficient Spark SQL usage\n",
        "\n",
        "Remember these key takeaways:\n",
        "\n",
        "- Use Spark SQL when working with structured data and when you need SQL-like operations\n",
        "- Prefer built-in functions over UDFs when possible for better performance\n",
        "- Apply filters early and use appropriate join strategies\n",
        "- Leverage Spark's SQL optimizer by understanding query plans\n",
        "- Choose the right persistence strategy for your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up resources\n",
        "spark.sql(\"DROP TABLE IF EXISTS global_employees\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS dept_statistics\")\n",
        "spark.catalog.clearCache()\n",
        "\n",
        "print(\"Cleanup complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 