{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complex UDFs with Dictionary Return Types in PySpark\n",
        "\n",
        "This notebook demonstrates how to create and use PySpark UDFs that read multiple input columns and return Python dictionaries containing various data types including:\n",
        "- Integers\n",
        "- Strings\n",
        "- Nested objects/dictionaries\n",
        "\n",
        "We'll cover:\n",
        "1. Creating UDFs that return dictionaries\n",
        "2. Converting Python dictionaries to structured data in Spark\n",
        "3. Handling nested structures efficiently\n",
        "4. Performance considerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, struct, to_json, from_json\n",
        "from pyspark.sql.types import (\n",
        "    StringType, IntegerType, DoubleType, BooleanType, \n",
        "    StructType, StructField, MapType, ArrayType\n",
        ")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Dictionary UDF Examples\").getOrCreate()\n",
        "\n",
        "print(\"SparkSession initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create Sample Data\n",
        "\n",
        "Let's create a sample DataFrame with various data types to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample DataFrame\n",
        "data = [\n",
        "    (1, \"John\", 32, \"New York\", 75000.0),\n",
        "    (2, \"Alice\", 28, \"San Francisco\", 95000.0),\n",
        "    (3, \"Bob\", 45, \"Chicago\", 85000.0),\n",
        "    (4, \"Emma\", 36, \"Seattle\", 92000.0),\n",
        "    (5, \"Michael\", 52, \"Boston\", 120000.0)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"name\", \"age\", \"city\", \"salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Sample DataFrame:\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. UDF that Returns a Simple Dictionary\n",
        "\n",
        "First, let's create a UDF that processes multiple columns and returns a simple dictionary containing string and integer values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a UDF that returns a simple dictionary\n",
        "def create_person_dict(name, age, city):\n",
        "    \"\"\"\n",
        "    Create a dictionary with person information\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"full_name\": name,\n",
        "        \"age_in_years\": age,\n",
        "        \"residence\": city,\n",
        "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
        "    }\n",
        "\n",
        "# Define the schema for the struct we want to return\n",
        "person_struct_schema = StructType([\n",
        "    StructField(\"full_name\", StringType(), True),\n",
        "    StructField(\"age_in_years\", IntegerType(), True),\n",
        "    StructField(\"residence\", StringType(), True),\n",
        "    StructField(\"age_group\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Register UDF with struct return type that can be converted to JSON\n",
        "person_struct_udf = udf(create_person_dict, person_struct_schema)\n",
        "\n",
        "# Apply the UDF and convert resulting struct to JSON\n",
        "df_with_dict = df.withColumn(\n",
        "    \"person_info\", \n",
        "    to_json(person_struct_udf(col(\"name\"), col(\"age\"), col(\"city\")))\n",
        ")\n",
        "\n",
        "print(\"DataFrame with Dictionary UDF result:\")\n",
        "df_with_dict.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative Approach: Direct JSON String\n",
        "\n",
        "Another approach is to return a JSON string directly from the UDF instead of using the `to_json` function. This is often simpler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Better approach: Return a JSON string directly from the UDF\n",
        "import json\n",
        "\n",
        "def create_person_json(name, age, city):\n",
        "    \"\"\"\n",
        "    Create a dictionary and convert it to a JSON string\n",
        "    \"\"\"\n",
        "    person_dict = {\n",
        "        \"full_name\": name,\n",
        "        \"age_in_years\": age,\n",
        "        \"residence\": city,\n",
        "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
        "    }\n",
        "    return json.dumps(person_dict)\n",
        "\n",
        "# Register UDF with return type as string\n",
        "person_json_udf = udf(create_person_json, StringType())\n",
        "\n",
        "# Apply the UDF\n",
        "df_with_json = df.withColumn(\n",
        "    \"person_info\", \n",
        "    person_json_udf(col(\"name\"), col(\"age\"), col(\"city\"))\n",
        ")\n",
        "\n",
        "print(\"DataFrame with JSON UDF result:\")\n",
        "df_with_json.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parsing the JSON Back to a Structured Format\n",
        "\n",
        "Now, let's parse the JSON string back to a structured column using `from_json`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for the person_info JSON\n",
        "person_schema = StructType([\n",
        "    StructField(\"full_name\", StringType(), True),\n",
        "    StructField(\"age_in_years\", IntegerType(), True),\n",
        "    StructField(\"residence\", StringType(), True),\n",
        "    StructField(\"age_group\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Parse the JSON string back to a struct\n",
        "df_parsed = df_with_json.withColumn(\n",
        "    \"person_struct\",\n",
        "    from_json(col(\"person_info\"), person_schema)\n",
        ")\n",
        "\n",
        "# Access fields from the parsed struct\n",
        "df_parsed_fields = df_parsed.select(\n",
        "    \"id\",\n",
        "    \"salary\",\n",
        "    \"person_struct.full_name\",\n",
        "    \"person_struct.age_in_years\",\n",
        "    \"person_struct.residence\",\n",
        "    \"person_struct.age_group\"\n",
        ")\n",
        "\n",
        "print(\"DataFrame with parsed JSON fields:\")\n",
        "df_parsed_fields.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Complex UDF with Nested Dictionaries\n",
        "\n",
        "Let's create a more complex UDF that returns nested dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_complex_profile(id, name, age, city, salary):\n",
        "    \"\"\"\n",
        "    Create a complex dictionary with nested structures\n",
        "    \"\"\"\n",
        "    # Calculate some derived values\n",
        "    tax_rate = 0.20 if salary < 85000 else 0.30\n",
        "    take_home = salary * (1 - tax_rate)\n",
        "    retirement_years = 65 - age\n",
        "    \n",
        "    # Create nested dictionary\n",
        "    profile = {\n",
        "        \"personal\": {\n",
        "            \"id\": id,\n",
        "            \"name\": name,\n",
        "            \"age\": age,\n",
        "            \"location\": {\n",
        "                \"city\": city,\n",
        "                \"country\": \"USA\",  # Assuming USA for all\n",
        "                \"is_metro\": city in [\"New York\", \"San Francisco\", \"Chicago\", \"Boston\"]\n",
        "            }\n",
        "        },\n",
        "        \"financial\": {\n",
        "            \"income\": {\n",
        "                \"gross_salary\": salary,\n",
        "                \"tax_rate\": tax_rate,\n",
        "                \"take_home\": take_home\n",
        "            },\n",
        "            \"retirement\": {\n",
        "                \"years_to_retirement\": retirement_years,\n",
        "                \"retirement_age\": 65\n",
        "            }\n",
        "        },\n",
        "        \"summary\": f\"{name} from {city}, age {age}, earning ${salary:.2f}\"\n",
        "    }\n",
        "    \n",
        "    return json.dumps(profile)\n",
        "\n",
        "# Register UDF\n",
        "complex_profile_udf = udf(create_complex_profile, StringType())\n",
        "\n",
        "# Apply the UDF\n",
        "df_complex = df.withColumn(\n",
        "    \"profile\",\n",
        "    complex_profile_udf(\n",
        "        col(\"id\"), col(\"name\"), col(\"age\"), col(\"city\"), col(\"salary\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"DataFrame with complex nested dictionary:\")\n",
        "df_complex.select(\"id\", \"profile\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Parse Complex Nested Structure\n",
        "\n",
        "Now, let's define a schema for our complex structure and parse it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for the complex profile\n",
        "location_schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"is_metro\", BooleanType(), True)\n",
        "])\n",
        "\n",
        "personal_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"location\", location_schema, True)\n",
        "])\n",
        "\n",
        "income_schema = StructType([\n",
        "    StructField(\"gross_salary\", DoubleType(), True),\n",
        "    StructField(\"tax_rate\", DoubleType(), True),\n",
        "    StructField(\"take_home\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "retirement_schema = StructType([\n",
        "    StructField(\"years_to_retirement\", IntegerType(), True),\n",
        "    StructField(\"retirement_age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "financial_schema = StructType([\n",
        "    StructField(\"income\", income_schema, True),\n",
        "    StructField(\"retirement\", retirement_schema, True)\n",
        "])\n",
        "\n",
        "profile_schema = StructType([\n",
        "    StructField(\"personal\", personal_schema, True),\n",
        "    StructField(\"financial\", financial_schema, True),\n",
        "    StructField(\"summary\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Parse the complex JSON\n",
        "df_complex_parsed = df_complex.withColumn(\n",
        "    \"parsed_profile\",\n",
        "    from_json(col(\"profile\"), profile_schema)\n",
        ")\n",
        "\n",
        "# Show schema of the parsed profile\n",
        "print(\"Schema of parsed profile:\")\n",
        "df_complex_parsed.select(\"parsed_profile\").printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extract Specific Fields from Nested Structure\n",
        "\n",
        "Now let's extract and work with specific fields from our complex nested structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific fields\n",
        "df_extracted = df_complex_parsed.select(\n",
        "    \"parsed_profile.personal.name\",\n",
        "    \"parsed_profile.personal.location.city\",\n",
        "    \"parsed_profile.personal.location.is_metro\",\n",
        "    \"parsed_profile.financial.income.gross_salary\",\n",
        "    \"parsed_profile.financial.income.take_home\",\n",
        "    \"parsed_profile.financial.retirement.years_to_retirement\",\n",
        "    \"parsed_profile.summary\"\n",
        ")\n",
        "\n",
        "print(\"Extracted fields from nested structure:\")\n",
        "df_extracted.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Creating a MapType UDF\n",
        "\n",
        "Another approach is to use a MapType return type instead of converting to/from JSON. However, this has limitations on the types of values that can be stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a UDF that returns a map (key-value pairs)\n",
        "def create_person_map(name, age):\n",
        "    \"\"\"\n",
        "    Create a map of person attributes\n",
        "    Note: All values must be of the same type when using MapType\n",
        "    \"\"\"\n",
        "    # Convert all values to strings for consistency\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"age\": str(age),\n",
        "        \"age_group\": \"young\" if age < 30 else \"middle\" if age < 50 else \"senior\"\n",
        "    }\n",
        "\n",
        "# Register UDF with MapType return type\n",
        "# Note: Both keys and values must be of uniform types\n",
        "person_map_udf = udf(create_person_map, MapType(StringType(), StringType()))\n",
        "\n",
        "# Apply the UDF\n",
        "df_with_map = df.withColumn(\n",
        "    \"person_map\",\n",
        "    person_map_udf(col(\"name\"), col(\"age\"))\n",
        ")\n",
        "\n",
        "print(\"DataFrame with MapType result:\")\n",
        "df_with_map.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Access individual values from the map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access a specific key from the map\n",
        "df_map_access = df_with_map.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    col(\"person_map\")[\"name\"].alias(\"map_name\"),\n",
        "    col(\"person_map\")[\"age_group\"].alias(\"map_age_group\")\n",
        ")\n",
        "\n",
        "print(\"Accessing specific keys from map:\")\n",
        "df_map_access.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Performance Comparison\n",
        "\n",
        "Let's look at the performance implications of these different approaches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a larger DataFrame for performance testing\n",
        "from pyspark.sql.functions import lit, rand\n",
        "import time\n",
        "\n",
        "# Multiply our DataFrame to create more rows\n",
        "df_large = df.union(df).union(df).union(df)  # 20 rows\n",
        "for i in range(3):  # Multiple more times to get ~160 rows\n",
        "    df_large = df_large.union(df_large)\n",
        "\n",
        "# Add some random variation\n",
        "df_large = df_large.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
        "df_large = df_large.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "print(f\"Created larger DataFrame with {df_large.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Fix error with previous cell\n",
        "df_large = df.union(df).union(df).union(df)  # 20 rows\n",
        "for i in range(3):  # Multiple more times to get ~160 rows\n",
        "    df_large = df_large.union(df_large)\n",
        "\n",
        "# Add some random variation\n",
        "df_large = df_large.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
        "df_large = df_large.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "print(f\"Created larger DataFrame with {df_large.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test JSON string approach\n",
        "start_time = time.time()\n",
        "df_large_json = df_large.withColumn(\n",
        "    \"profile\",\n",
        "    complex_profile_udf(\n",
        "        col(\"id\"), col(\"name\"), col(\"age\"), col(\"city\"), col(\"salary\")\n",
        "    )\n",
        ")\n",
        "df_large_json.select(\"id\", \"profile\").count()  # Force execution\n",
        "json_time = time.time() - start_time\n",
        "\n",
        "# Test MapType approach\n",
        "start_time = time.time()\n",
        "df_large_map = df_large.withColumn(\n",
        "    \"person_map\",\n",
        "    person_map_udf(col(\"name\"), col(\"age\"))\n",
        ")\n",
        "df_large_map.select(\"id\", \"person_map\").count()  # Force execution\n",
        "map_time = time.time() - start_time\n",
        "\n",
        "print(f\"Time for JSON string approach: {json_time:.2f} seconds\")\n",
        "print(f\"Time for MapType approach: {map_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Best Practices\n",
        "\n",
        "Based on our exploration, here are some best practices for working with dictionary-based UDFs in PySpark:\n",
        "\n",
        "1. **For simple dictionaries with homogeneous value types:**\n",
        "   - Use `MapType` for better performance and direct access\n",
        "   - Ensure all values are of the same type\n",
        "\n",
        "2. **For complex nested dictionaries with heterogeneous types:**\n",
        "   - Convert to JSON strings in the UDF\n",
        "   - Use `from_json` with explicit schema to parse\n",
        "   - Leverage dot notation to access nested fields\n",
        "\n",
        "3. **Performance considerations:**\n",
        "   - JSON serialization/deserialization adds overhead\n",
        "   - Consider using Pandas UDFs for better performance with large datasets\n",
        "   - If possible, restructure your data model to avoid complex nested structures\n",
        "\n",
        "4. **Schema Management:**\n",
        "   - Always define explicit schemas for JSON parsing\n",
        "   - Use appropriate data types to avoid later conversions\n",
        "   - Document your schema structure for future reference"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 