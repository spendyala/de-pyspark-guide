{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark File Format Guide: Reading and Writing Data\n",
        "\n",
        "This notebook demonstrates how to efficiently read and write data in various file formats using PySpark:\n",
        "\n",
        "1. CSV\n",
        "2. JSON\n",
        "3. Parquet\n",
        "4. Avro\n",
        "\n",
        "For each format, we'll cover:\n",
        "- Basic reading and writing operations\n",
        "- Common options and parameters\n",
        "- Performance considerations\n",
        "- Best practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, rand, monotonically_increasing_id\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, \n",
        "    DoubleType, BooleanType, DateType, TimestampType\n",
        ")\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"File Format Guide\") \\\n",
        "    .config(\"spark.sql.avro.compression.codec\", \"snappy\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Sample Data\n",
        "\n",
        "First, let's create a sample dataset to work with throughout this guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple dataset with different data types\n",
        "data = [\n",
        "    (1, \"John Doe\", 35, \"New York\", 72000.50, True, \"2020-01-15\"),\n",
        "    (2, \"Jane Smith\", 28, \"San Francisco\", 86000.00, False, \"2019-06-22\"),\n",
        "    (3, \"Robert Brown\", 42, \"Chicago\", 92000.75, True, \"2021-03-08\"),\n",
        "    (4, \"Maria Garcia\", 31, \"Los Angeles\", 67500.25, True, \"2018-11-30\"),\n",
        "    (5, \"James Wilson\", 45, \"Seattle\", 115000.00, False, \"2022-02-12\")\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"salary\", DoubleType(), True),\n",
        "    StructField(\"is_manager\", BooleanType(), True),\n",
        "    StructField(\"hire_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs(\"/tmp/spark_data\", exist_ok=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "print(\"Sample DataFrame:\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CSV Files\n",
        "\n",
        "CSV (Comma-Separated Values) is a widely-used format for tabular data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic CSV write\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .csv(\"/tmp/spark_data/basic.csv\")\n",
        "\n",
        "# CSV write with options\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"delimiter\", \",\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
        "    .option(\"nullValue\", \"NULL\") \\\n",
        "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
        "\n",
        "# CSV with partition (data is stored in subdirectories by city)\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"city\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"/tmp/spark_data/partitioned.csv\")\n",
        "\n",
        "print(\"CSV files written successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic CSV read\n",
        "df_csv_basic = spark.read.csv(\"/tmp/spark_data/basic.csv\")\n",
        "\n",
        "print(\"Basic CSV Read (note how column names are generic and types are strings):\")\n",
        "df_csv_basic.printSchema()\n",
        "df_csv_basic.show(3)\n",
        "\n",
        "# Read with options and schema inference\n",
        "df_csv_with_header = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \",\") \\\n",
        "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
        "\n",
        "print(\"\\nCSV Read with Header and Schema Inference:\")\n",
        "df_csv_with_header.printSchema()\n",
        "df_csv_with_header.show(3)\n",
        "\n",
        "# Read with explicit schema\n",
        "df_csv_with_schema = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .csv(\"/tmp/spark_data/formatted.csv\")\n",
        "\n",
        "print(\"\\nCSV Read with Explicit Schema:\")\n",
        "df_csv_with_schema.printSchema()\n",
        "df_csv_with_schema.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CSV Best Practices\n",
        "\n",
        "1. **Always specify a schema for production workloads** - Schema inference is convenient but slow and may guess the wrong types.\n",
        "2. **Use header=true when possible** - Makes your data self-documenting.\n",
        "3. **Set appropriate nullValue option** - Define how NULL values are represented in your CSV.\n",
        "4. **Be explicit with date formats** - Set dateFormat to ensure correct parsing.\n",
        "5. **For large datasets, consider**:\n",
        "   - Setting compression (compression='gzip')\n",
        "   - Proper partitioning (partitionBy)\n",
        "   - Specifying escape characters for special data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. JSON Files\n",
        "\n",
        "JSON (JavaScript Object Notation) is excellent for semi-structured data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing JSON Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic JSON write\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .json(\"/tmp/spark_data/basic.json\")\n",
        "\n",
        "# JSON write with options\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"gzip\") \\\n",
        "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
        "    .json(\"/tmp/spark_data/compressed.json\")\n",
        "\n",
        "# JSON with pretty printing (one record per line, properly formatted)\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"pretty\", \"true\") \\\n",
        "    .json(\"/tmp/spark_data/pretty.json\")\n",
        "\n",
        "print(\"JSON files written successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading JSON Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic JSON read\n",
        "df_json_basic = spark.read.json(\"/tmp/spark_data/basic.json\")\n",
        "\n",
        "print(\"Basic JSON Read (with schema inference):\")\n",
        "df_json_basic.printSchema()\n",
        "df_json_basic.show(3)\n",
        "\n",
        "# Read with explicit schema\n",
        "df_json_with_schema = spark.read \\\n",
        "    .schema(schema) \\\n",
        "    .json(\"/tmp/spark_data/basic.json\")\n",
        "\n",
        "print(\"\\nJSON Read with Explicit Schema:\")\n",
        "df_json_with_schema.printSchema()\n",
        "df_json_with_schema.show(3)\n",
        "\n",
        "# Reading multi-line JSON (where each JSON object may span multiple lines)\n",
        "multi_line_json = \"\"\"[\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"name\": \"John Doe\",\n",
        "        \"age\": 35,\n",
        "        \"city\": \"New York\",\n",
        "        \"salary\": 72000.50,\n",
        "        \"is_manager\": true,\n",
        "        \"hire_date\": \"2020-01-15\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"name\": \"Jane Smith\",\n",
        "        \"age\": 28,\n",
        "        \"city\": \"San Francisco\",\n",
        "        \"salary\": 86000.00,\n",
        "        \"is_manager\": false,\n",
        "        \"hire_date\": \"2019-06-22\"\n",
        "    }\n",
        "]\"\"\"\n",
        "\n",
        "# Write multi-line JSON to a file\n",
        "with open(\"/tmp/spark_data/multiline.json\", \"w\") as f:\n",
        "    f.write(multi_line_json)\n",
        "\n",
        "# Read multi-line JSON\n",
        "df_multiline_json = spark.read \\\n",
        "    .option(\"multiline\", \"true\") \\\n",
        "    .json(\"/tmp/spark_data/multiline.json\")\n",
        "\n",
        "print(\"\\nMulti-line JSON Read:\")\n",
        "df_multiline_json.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JSON Best Practices\n",
        "\n",
        "1. **Use multiline=true when needed** - For JSON files where objects span multiple lines.\n",
        "2. **Prefer one record per line** - For performance and parallelism.\n",
        "3. **Use explicit schemas in production** - For consistent type handling.\n",
        "4. **Consider compression** - JSON is verbose, so compression helps with storage.\n",
        "5. **Be careful with complex nested structures** - These can be processed but impact performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parquet Files\n",
        "\n",
        "Parquet is a columnar format optimized for analytics workloads, offering efficient storage and querying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing Parquet Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Parquet write\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/tmp/spark_data/basic.parquet\")\n",
        "\n",
        "# Parquet with compression options\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(\"/tmp/spark_data/compressed.parquet\")\n",
        "\n",
        "# Parquet with partitioning\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"city\", \"is_manager\") \\\n",
        "    .parquet(\"/tmp/spark_data/partitioned.parquet\")\n",
        "\n",
        "print(\"Parquet files written successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading Parquet Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Parquet read\n",
        "df_parquet_basic = spark.read.parquet(\"/tmp/spark_data/basic.parquet\")\n",
        "\n",
        "print(\"Basic Parquet Read (schemas are preserved):\")\n",
        "df_parquet_basic.printSchema()\n",
        "df_parquet_basic.show(3)\n",
        "\n",
        "# Read with column projection (reading only specific columns)\n",
        "df_parquet_select = spark.read.parquet(\"/tmp/spark_data/basic.parquet\").select(\"id\", \"name\", \"city\")\n",
        "\n",
        "print(\"\\nParquet Read with Column Projection:\")\n",
        "df_parquet_select.show(3)\n",
        "\n",
        "# Read with partition discovery and filtering\n",
        "df_parquet_filtered = spark.read.parquet(\"/tmp/spark_data/partitioned.parquet\") \\\n",
        "    .filter(col(\"city\") == \"New York\")\n",
        "\n",
        "print(\"\\nParquet Read with Partition Filtering:\")\n",
        "df_parquet_filtered.explain()  # Look for PushedFilters and PartitionFilters in the plan\n",
        "df_parquet_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parquet Best Practices\n",
        "\n",
        "1. **Use Parquet for analytics workloads** - It's optimized for query performance.\n",
        "2. **Choose appropriate partitioning** - Partition on columns used for filtering.\n",
        "3. **Enable predicate pushdown** - Filtering happens at file read time, not after loading data.\n",
        "4. **Choose Snappy compression** - Good balance of compression ratio and speed.\n",
        "5. **Consider file size** - Aim for parquet files in the 256MB-1GB range for best performance.\n",
        "6. **Design for column projection** - Parquet shines when you only need to read a subset of columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Avro Files\n",
        "\n",
        "Avro is a row-based format that's good for record-oriented data and is particularly well-suited for evolving schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure we have the spark-avro module\n",
        "# If running in an environment where you can't add packages to Spark session, you may need to install it separately\n",
        "try:\n",
        "    df_avro_test = spark.read.format(\"avro\").load(\"/tmp/spark_data/dummy.avro\")\n",
        "    print(\"Avro format is available\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "    print(\"The Avro format might not be available in this Spark build.\")\n",
        "    print(\"You may need to add the spark-avro package when creating your SparkSession:\")\n",
        "    print(\"\"\"\n",
        "    SparkSession.builder \\\n",
        "        .appName(\"Your App\") \\\n",
        "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.x.x\") \\\n",
        "        .getOrCreate()\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing Avro Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Avro write\n",
        "df.write \\\n",
        "    .format(\"avro\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/tmp/spark_data/basic.avro\")\n",
        "\n",
        "# Avro with compression\n",
        "df.write \\\n",
        "    .format(\"avro\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .save(\"/tmp/spark_data/compressed.avro\")\n",
        "\n",
        "# Avro with partitioning\n",
        "df.write \\\n",
        "    .format(\"avro\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"city\") \\\n",
        "    .save(\"/tmp/spark_data/partitioned.avro\")\n",
        "\n",
        "print(\"Avro files written successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading Avro Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Avro read\n",
        "df_avro_basic = spark.read.format(\"avro\").load(\"/tmp/spark_data/basic.avro\")\n",
        "\n",
        "print(\"Basic Avro Read:\")\n",
        "df_avro_basic.printSchema()\n",
        "df_avro_basic.show(3)\n",
        "\n",
        "# Reading with column projection\n",
        "df_avro_select = spark.read \\\n",
        "    .format(\"avro\") \\\n",
        "    .load(\"/tmp/spark_data/basic.avro\") \\\n",
        "    .select(\"id\", \"name\", \"salary\")\n",
        "\n",
        "print(\"\\nAvro Read with Column Selection:\")\n",
        "df_avro_select.show(3)\n",
        "\n",
        "# Reading with partition discovery\n",
        "df_avro_partitioned = spark.read \\\n",
        "    .format(\"avro\") \\\n",
        "    .load(\"/tmp/spark_data/partitioned.avro\")\n",
        "    \n",
        "print(\"\\nAvro Read with Partition Discovery:\")\n",
        "df_avro_partitioned.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Avro Best Practices\n",
        "\n",
        "1. **Use Avro for schema evolution** - It handles schema changes well.\n",
        "2. **Consider for streaming data** - Works well with Kafka and other streaming systems.\n",
        "3. **Enable compression** - Snappy is a good default choice.\n",
        "4. **Be mindful of complex types** - Avro handles complex types well but with some performance impact.\n",
        "5. **Don't rely on predicate pushdown** - It's not as efficient as Parquet for filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Comparison\n",
        "\n",
        "Let's create a larger dataset and compare the formats for both writing and reading performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a larger dataset\n",
        "large_df = df\n",
        "for i in range(4):  # Will give us about 80 rows (5 * 2^4)\n",
        "    large_df = large_df.union(large_df)\n",
        "\n",
        "# Add some randomness\n",
        "large_df = large_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "large_df = large_df.withColumn(\"salary\", col(\"salary\") * (rand() + 0.5))\n",
        "\n",
        "print(f\"Created performance test dataset with {large_df.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to measure write performance\n",
        "def measure_write_performance(df, format_name, options={}):\n",
        "    path = f\"/tmp/spark_data/perf_{format_name}\"\n",
        "    writer = df.write.mode(\"overwrite\")\n",
        "    \n",
        "    # Add options\n",
        "    for k, v in options.items():\n",
        "        writer = writer.option(k, v)\n",
        "    \n",
        "    # Measure write time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if format_name == \"avro\":\n",
        "        writer.format(\"avro\").save(path)\n",
        "    else:\n",
        "        getattr(writer, format_name)(path)\n",
        "    \n",
        "    write_time = time.time() - start_time\n",
        "    \n",
        "    return path, write_time\n",
        "\n",
        "# Function to measure read performance\n",
        "def measure_read_performance(format_name, path, options={}):\n",
        "    reader = spark.read\n",
        "    \n",
        "    # Add options\n",
        "    for k, v in options.items():\n",
        "        reader = reader.option(k, v)\n",
        "    \n",
        "    # Measure read time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if format_name == \"avro\":\n",
        "        df_read = reader.format(\"avro\").load(path)\n",
        "    else:\n",
        "        df_read = getattr(reader, format_name)(path)\n",
        "    \n",
        "    count = df_read.count()  # Force execution\n",
        "    read_time = time.time() - start_time\n",
        "    \n",
        "    return read_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run performance comparison\n",
        "results = []\n",
        "\n",
        "# Test CSV\n",
        "csv_path, csv_write_time = measure_write_performance(\n",
        "    large_df, \"csv\", {\"header\": \"true\"}\n",
        ")\n",
        "csv_read_time = measure_read_performance(\n",
        "    \"csv\", csv_path, {\"header\": \"true\", \"inferSchema\": \"true\"}\n",
        ")\n",
        "results.append((\"CSV\", csv_write_time, csv_read_time))\n",
        "\n",
        "# Test JSON\n",
        "json_path, json_write_time = measure_write_performance(large_df, \"json\")\n",
        "json_read_time = measure_read_performance(\"json\", json_path)\n",
        "results.append((\"JSON\", json_write_time, json_read_time))\n",
        "\n",
        "# Test Parquet\n",
        "parquet_path, parquet_write_time = measure_write_performance(\n",
        "    large_df, \"parquet\", {\"compression\": \"snappy\"}\n",
        ")\n",
        "parquet_read_time = measure_read_performance(\"parquet\", parquet_path)\n",
        "results.append((\"Parquet\", parquet_write_time, parquet_read_time))\n",
        "\n",
        "# Test Avro\n",
        "try:\n",
        "    avro_path, avro_write_time = measure_write_performance(\n",
        "        large_df, \"avro\", {\"compression\": \"snappy\"}\n",
        "    )\n",
        "    avro_read_time = measure_read_performance(\"avro\", avro_path)\n",
        "    results.append((\"Avro\", avro_write_time, avro_read_time))\n",
        "except Exception as e:\n",
        "    print(f\"Skipping Avro performance test: {e}\")\n",
        "\n",
        "# Display results\n",
        "print(\"Performance Comparison:\")\n",
        "print(\"Format\\tWrite Time (s)\\tRead Time (s)\")\n",
        "print(\"------\\t--------------\\t-------------\")\n",
        "for format_name, write_time, read_time in results:\n",
        "    print(f\"{format_name}\\t{write_time:.2f}\\t\\t{read_time:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. File Size Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_directory_size(path):\n",
        "    # A helper function to calculate directory size\n",
        "    # This is a simplified version and might not work in all environments\n",
        "    import subprocess\n",
        "    try:\n",
        "        output = subprocess.check_output(['du', '-sh', path]).decode('utf-8')\n",
        "        size = output.split()[0]\n",
        "        return size\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Measure directory sizes\n",
        "print(\"File Size Comparison:\")\n",
        "print(\"Format\\tSize\")\n",
        "print(\"------\\t----\")\n",
        "print(f\"CSV\\t{get_directory_size(csv_path)}\")\n",
        "print(f\"JSON\\t{get_directory_size(json_path)}\")\n",
        "print(f\"Parquet\\t{get_directory_size(parquet_path)}\")\n",
        "try:\n",
        "    print(f\"Avro\\t{get_directory_size(avro_path)}\")\n",
        "except NameError:\n",
        "    print(\"Avro\\tNot tested\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Format Selection Guide\n",
        "\n",
        "### When to use each format:\n",
        "\n",
        "**CSV**\n",
        "- ✅ Human-readable, universal compatibility\n",
        "- ✅ Easy to edit manually or with spreadsheet software\n",
        "- ❌ Poor performance for large data\n",
        "- ❌ Inefficient storage (no compression by default)\n",
        "- ❌ No schema preservation\n",
        "- **Best for**: Small datasets, data exchange with non-Spark systems, human-editable files\n",
        "\n",
        "**JSON**\n",
        "- ✅ Supports nested structures\n",
        "- ✅ Good compatibility with web services\n",
        "- ✅ Human-readable\n",
        "- ❌ Verbose format, larger files\n",
        "- ❌ Slower than binary formats\n",
        "- **Best for**: API integrations, semi-structured data, moderate-sized datasets\n",
        "\n",
        "**Parquet**\n",
        "- ✅ Best query performance\n",
        "- ✅ Column pruning and predicate pushdown\n",
        "- ✅ Efficient compression\n",
        "- ✅ Schema preservation\n",
        "- ❌ Not human-readable\n",
        "- ❌ Less universal than CSV/JSON\n",
        "- **Best for**: Analytics workloads, large datasets, frequent querying, column-oriented access patterns\n",
        "\n",
        "**Avro**\n",
        "- ✅ Schema evolution support\n",
        "- ✅ Good for record/row-based access\n",
        "- ✅ Rich data type support\n",
        "- ✅ Compact binary format\n",
        "- ❌ Not as efficient as Parquet for analytics\n",
        "- ❌ Not human-readable\n",
        "- **Best for**: Data with evolving schemas, streaming data integration, row-oriented access patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary of Key Options and Parameters\n",
        "\n",
        "### Common Parameters for All Formats\n",
        "- `mode`: `overwrite`, `append`, `ignore`, `error` (default)\n",
        "- `partitionBy`: Saves data in partitioned directory structure\n",
        "\n",
        "### CSV Options\n",
        "- `header`: `true` to include column names\n",
        "- `delimiter`: Character separator (default `,`)\n",
        "- `quote`: Character for quoting (default `\"`)\n",
        "- `escape`: Character for escaping (default `\\`)\n",
        "- `nullValue`: String representation for null values\n",
        "- `inferSchema`: Automatically detect column types\n",
        "- `dateFormat`: Format string for date parsing\n",
        "\n",
        "### JSON Options\n",
        "- `multiLine`: `true` for multi-line JSON records\n",
        "- `dateFormat`: Format string for date parsing\n",
        "- `compression`: Compression codec (e.g., `gzip`, `snappy`)\n",
        "- `primitivesAsString`: Convert primitives to strings\n",
        "- `allowUnquotedFieldNames`: Allow unquoted field names\n",
        "\n",
        "### Parquet Options\n",
        "- `compression`: Compression codec (e.g., `snappy`, `gzip`, `none`)\n",
        "- `mergeSchema`: Reconcile schemas when reading multiple files\n",
        "- `partitionOverwriteMode`: `static` or `dynamic` partition overwrite\n",
        "\n",
        "### Avro Options\n",
        "- `avroSchema`: User-provided schema\n",
        "- `compression`: Compression codec (e.g., `snappy`, `deflate`)\n",
        "- `recordName`: Record name in the Avro schema\n",
        "- `recordNamespace`: Namespace in the Avro schema"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 